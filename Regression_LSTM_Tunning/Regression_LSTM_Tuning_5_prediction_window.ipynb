{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression LSTM with best parameters\n",
    "    find the best prediction window to apply w/ lr = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\maria\\anaconda3\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from pandas) (1.18.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from python-dateutil>=2.6.1->pandas) (1.14.0)\n",
      "Requirement already satisfied: sklearn in c:\\users\\maria\\anaconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\maria\\anaconda3\\lib\\site-packages (from sklearn) (0.22.1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.18.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (0.14.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.4.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\maria\\anaconda3\\lib\\site-packages (3.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from matplotlib) (2.4.6)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.11 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from matplotlib) (1.18.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from python-dateutil>=2.1->matplotlib) (1.14.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\maria\\anaconda3\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib) (45.2.0.post20200210)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install sklearn\n",
    "!pip install matplotlib\n",
    "\n",
    "import pandas as pd;\n",
    "import numpy as np;\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "date =  [2018010000, \n",
    "         2018030000, \n",
    "         2018050000,\n",
    "         2018070000, \n",
    "         2018090000, \n",
    "         2018110000]\n",
    "\n",
    "# parameters\n",
    "steps = 96\n",
    "n_hidden = 2\n",
    "units = 180\n",
    "batch_size = 336\n",
    "epochs = 180\n",
    "features_num = 14\n",
    "\n",
    "# lists to append results\n",
    "mae_gen = []\n",
    "mae_nor = []\n",
    "mae_spi = []\n",
    "rmse_gen = []\n",
    "rmse_nor = []\n",
    "rmse_spi = []\n",
    "y_pred_list = []\n",
    "time_count = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import keras libraries, packages and data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LeakyReLU\n",
    "from keras import initializers\n",
    "from keras import optimizers\n",
    "\n",
    "# import data\n",
    "data_full = pd.read_csv('Data_set_1_smaller_(1).csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create loop for different dates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maria\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3997: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n",
      "C:\\Users\\maria\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/180\n",
      "6/6 [==============================] - 7s 1s/step - loss: 2.4735 - mse: 2.4735 - mae: 1.0016\n",
      "Epoch 2/180\n",
      "6/6 [==============================] - 7s 1s/step - loss: 0.1582 - mse: 0.1582 - mae: 0.3157\n",
      "Epoch 3/180\n",
      "6/6 [==============================] - 8s 1s/step - loss: 0.1669 - mse: 0.1669 - mae: 0.3315\n",
      "Epoch 4/180\n",
      "6/6 [==============================] - 11s 2s/step - loss: 0.2363 - mse: 0.2363 - mae: 0.3933\n",
      "Epoch 5/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.1848 - mse: 0.1848 - mae: 0.3436\n",
      "Epoch 6/180\n",
      "6/6 [==============================] - 11s 2s/step - loss: 0.1875 - mse: 0.1875 - mae: 0.3500\n",
      "Epoch 7/180\n",
      "6/6 [==============================] - 11s 2s/step - loss: 0.2222 - mse: 0.2222 - mae: 0.3843\n",
      "Epoch 8/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.1525 - mse: 0.1525 - mae: 0.3156\n",
      "Epoch 9/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.1917 - mse: 0.1917 - mae: 0.3595\n",
      "Epoch 10/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.1092 - mse: 0.1092 - mae: 0.2643\n",
      "Epoch 11/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.1990 - mse: 0.1990 - mae: 0.3687\n",
      "Epoch 12/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.1059 - mse: 0.1059 - mae: 0.2599\n",
      "Epoch 13/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.1008 - mse: 0.1008 - mae: 0.2545\n",
      "Epoch 14/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.1433 - mse: 0.1433 - mae: 0.3135\n",
      "Epoch 15/180\n",
      "6/6 [==============================] - 9s 2s/step - loss: 0.0781 - mse: 0.0781 - mae: 0.2237\n",
      "Epoch 16/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.1067 - mse: 0.1067 - mae: 0.2678\n",
      "Epoch 17/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0861 - mse: 0.0861 - mae: 0.2386\n",
      "Epoch 18/180\n",
      "6/6 [==============================] - 8s 1s/step - loss: 0.0659 - mse: 0.0659 - mae: 0.2051\n",
      "Epoch 19/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0831 - mse: 0.0831 - mae: 0.2356\n",
      "Epoch 20/180\n",
      "6/6 [==============================] - 17s 3s/step - loss: 0.0617 - mse: 0.0617 - mae: 0.1983\n",
      "Epoch 21/180\n",
      "6/6 [==============================] - 16s 3s/step - loss: 0.0688 - mse: 0.0688 - mae: 0.2138\n",
      "Epoch 22/180\n",
      "6/6 [==============================] - 13s 2s/step - loss: 0.0436 - mse: 0.0436 - mae: 0.1647\n",
      "Epoch 23/180\n",
      "6/6 [==============================] - 13s 2s/step - loss: 0.0608 - mse: 0.0608 - mae: 0.2018\n",
      "Epoch 24/180\n",
      "6/6 [==============================] - 13s 2s/step - loss: 0.0433 - mse: 0.0433 - mae: 0.1670\n",
      "Epoch 25/180\n",
      "6/6 [==============================] - 11s 2s/step - loss: 0.0416 - mse: 0.0416 - mae: 0.1626\n",
      "Epoch 26/180\n",
      "6/6 [==============================] - 13s 2s/step - loss: 0.0459 - mse: 0.0459 - mae: 0.1730\n",
      "Epoch 27/180\n",
      "6/6 [==============================] - 12s 2s/step - loss: 0.0360 - mse: 0.0360 - mae: 0.1522\n",
      "Epoch 28/180\n",
      "6/6 [==============================] - 35s 6s/step - loss: 0.0355 - mse: 0.0355 - mae: 0.1525\n",
      "Epoch 29/180\n",
      "6/6 [==============================] - 16s 3s/step - loss: 0.0310 - mse: 0.0310 - mae: 0.1421\n",
      "Epoch 30/180\n",
      "6/6 [==============================] - 12s 2s/step - loss: 0.0279 - mse: 0.0279 - mae: 0.1354\n",
      "Epoch 31/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0337 - mse: 0.0337 - mae: 0.1490\n",
      "Epoch 32/180\n",
      "6/6 [==============================] - 12s 2s/step - loss: 0.0173 - mse: 0.0173 - mae: 0.1036\n",
      "Epoch 33/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0223 - mse: 0.0223 - mae: 0.1186\n",
      "Epoch 34/180\n",
      "6/6 [==============================] - 9s 2s/step - loss: 0.0270 - mse: 0.0270 - mae: 0.1325\n",
      "Epoch 35/180\n",
      "6/6 [==============================] - 9s 2s/step - loss: 0.0140 - mse: 0.0140 - mae: 0.0943\n",
      "Epoch 36/180\n",
      "6/6 [==============================] - 9s 2s/step - loss: 0.0249 - mse: 0.0249 - mae: 0.1290\n",
      "Epoch 37/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0123 - mse: 0.0123 - mae: 0.0867\n",
      "Epoch 38/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0189 - mse: 0.0189 - mae: 0.1113\n",
      "Epoch 39/180\n",
      "6/6 [==============================] - 9s 2s/step - loss: 0.0117 - mse: 0.0117 - mae: 0.0859\n",
      "Epoch 40/180\n",
      "6/6 [==============================] - 9s 1s/step - loss: 0.0122 - mse: 0.0122 - mae: 0.0866\n",
      "Epoch 41/180\n",
      "6/6 [==============================] - 9s 1s/step - loss: 0.0186 - mse: 0.0186 - mae: 0.1125\n",
      "Epoch 42/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0097 - mse: 0.0097 - mae: 0.0768\n",
      "Epoch 43/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0102 - mse: 0.0102 - mae: 0.0796\n",
      "Epoch 44/180\n",
      "6/6 [==============================] - 9s 2s/step - loss: 0.0108 - mse: 0.0108 - mae: 0.0827\n",
      "Epoch 45/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0085 - mse: 0.0085 - mae: 0.0728\n",
      "Epoch 46/180\n",
      "6/6 [==============================] - 11s 2s/step - loss: 0.0089 - mse: 0.0089 - mae: 0.0752\n",
      "Epoch 47/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0078 - mse: 0.0078 - mae: 0.0692\n",
      "Epoch 48/180\n",
      "6/6 [==============================] - 11s 2s/step - loss: 0.0069 - mse: 0.0069 - mae: 0.0650\n",
      "Epoch 49/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0080 - mse: 0.0080 - mae: 0.0711\n",
      "Epoch 50/180\n",
      "6/6 [==============================] - 11s 2s/step - loss: 0.0052 - mse: 0.0052 - mae: 0.0550\n",
      "Epoch 51/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0638\n",
      "Epoch 52/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0050 - mse: 0.0050 - mae: 0.0542\n",
      "Epoch 53/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0049 - mse: 0.0049 - mae: 0.0531\n",
      "Epoch 54/180\n",
      "6/6 [==============================] - 9s 2s/step - loss: 0.0053 - mse: 0.0053 - mae: 0.0563\n",
      "Epoch 55/180\n",
      "6/6 [==============================] - 9s 2s/step - loss: 0.0040 - mse: 0.0040 - mae: 0.0483\n",
      "Epoch 56/180\n",
      "6/6 [==============================] - 9s 1s/step - loss: 0.0046 - mse: 0.0046 - mae: 0.0518\n",
      "Epoch 57/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0037 - mse: 0.0037 - mae: 0.0446\n",
      "Epoch 58/180\n",
      "6/6 [==============================] - 11s 2s/step - loss: 0.0039 - mse: 0.0039 - mae: 0.0466\n",
      "Epoch 59/180\n",
      "6/6 [==============================] - 11s 2s/step - loss: 0.0038 - mse: 0.0038 - mae: 0.0463\n",
      "Epoch 60/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0032 - mse: 0.0032 - mae: 0.0409\n",
      "Epoch 61/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0033 - mse: 0.0033 - mae: 0.0419\n",
      "Epoch 62/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0034 - mse: 0.0034 - mae: 0.0431\n",
      "Epoch 63/180\n",
      "6/6 [==============================] - 9s 2s/step - loss: 0.0025 - mse: 0.0025 - mae: 0.0355\n",
      "Epoch 64/180\n",
      "6/6 [==============================] - 9s 1s/step - loss: 0.0031 - mse: 0.0031 - mae: 0.0408\n",
      "Epoch 65/180\n",
      "6/6 [==============================] - 9s 2s/step - loss: 0.0026 - mse: 0.0026 - mae: 0.0359\n",
      "Epoch 66/180\n",
      "6/6 [==============================] - 9s 2s/step - loss: 0.0027 - mse: 0.0027 - mae: 0.0365\n",
      "Epoch 67/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0024 - mse: 0.0024 - mae: 0.0344\n",
      "Epoch 68/180\n",
      "6/6 [==============================] - 11s 2s/step - loss: 0.0024 - mse: 0.0024 - mae: 0.0334\n",
      "Epoch 69/180\n",
      "6/6 [==============================] - 9s 2s/step - loss: 0.0023 - mse: 0.0023 - mae: 0.0330\n",
      "Epoch 70/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0020 - mse: 0.0020 - mae: 0.0303\n",
      "Epoch 71/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0022 - mse: 0.0022 - mae: 0.0320\n",
      "Epoch 72/180\n",
      "6/6 [==============================] - 11s 2s/step - loss: 0.0022 - mse: 0.0022 - mae: 0.0311\n",
      "Epoch 73/180\n",
      "6/6 [==============================] - 11s 2s/step - loss: 0.0019 - mse: 0.0019 - mae: 0.0289\n",
      "Epoch 74/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0019 - mse: 0.0019 - mae: 0.0284\n",
      "Epoch 75/180\n",
      "6/6 [==============================] - 11s 2s/step - loss: 0.0020 - mse: 0.0020 - mae: 0.0289\n",
      "Epoch 76/180\n",
      "6/6 [==============================] - 11s 2s/step - loss: 0.0018 - mse: 0.0018 - mae: 0.0274\n",
      "Epoch 77/180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 2s/step - loss: 0.0019 - mse: 0.0019 - mae: 0.0280\n",
      "Epoch 78/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0018 - mse: 0.0018 - mae: 0.0273\n",
      "Epoch 79/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0017 - mse: 0.0017 - mae: 0.0263\n",
      "Epoch 80/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0018 - mse: 0.0018 - mae: 0.0270\n",
      "Epoch 81/180\n",
      "6/6 [==============================] - 9s 2s/step - loss: 0.0017 - mse: 0.0017 - mae: 0.0253\n",
      "Epoch 82/180\n",
      "6/6 [==============================] - 9s 1s/step - loss: 0.0017 - mse: 0.0017 - mae: 0.0253\n",
      "Epoch 83/180\n",
      "6/6 [==============================] - 9s 2s/step - loss: 0.0017 - mse: 0.0017 - mae: 0.0250\n",
      "Epoch 84/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0017 - mse: 0.0017 - mae: 0.0246\n",
      "Epoch 85/180\n",
      "6/6 [==============================] - 9s 2s/step - loss: 0.0016 - mse: 0.0016 - mae: 0.0243\n",
      "Epoch 86/180\n",
      "6/6 [==============================] - 9s 1s/step - loss: 0.0016 - mse: 0.0016 - mae: 0.0235\n",
      "Epoch 87/180\n",
      "6/6 [==============================] - 9s 1s/step - loss: 0.0015 - mse: 0.0015 - mae: 0.0234\n",
      "Epoch 88/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0016 - mse: 0.0016 - mae: 0.0237\n",
      "Epoch 89/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0015 - mse: 0.0015 - mae: 0.0227\n",
      "Epoch 90/180\n",
      "6/6 [==============================] - 11s 2s/step - loss: 0.0015 - mse: 0.0015 - mae: 0.0229\n",
      "Epoch 91/180\n",
      "6/6 [==============================] - 9s 2s/step - loss: 0.0015 - mse: 0.0015 - mae: 0.0225\n",
      "Epoch 92/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0015 - mse: 0.0015 - mae: 0.0226\n",
      "Epoch 93/180\n",
      "6/6 [==============================] - 9s 2s/step - loss: 0.0015 - mse: 0.0015 - mae: 0.0220\n",
      "Epoch 94/180\n",
      "6/6 [==============================] - 9s 1s/step - loss: 0.0015 - mse: 0.0015 - mae: 0.0220\n",
      "Epoch 95/180\n",
      "6/6 [==============================] - 11s 2s/step - loss: 0.0014 - mse: 0.0014 - mae: 0.0216\n",
      "Epoch 96/180\n",
      "6/6 [==============================] - 12s 2s/step - loss: 0.0014 - mse: 0.0014 - mae: 0.0217\n",
      "Epoch 97/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0014 - mse: 0.0014 - mae: 0.0211\n",
      "Epoch 98/180\n",
      "6/6 [==============================] - 11s 2s/step - loss: 0.0015 - mse: 0.0015 - mae: 0.0216\n",
      "Epoch 99/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0014 - mse: 0.0014 - mae: 0.0212\n",
      "Epoch 100/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0014 - mse: 0.0014 - mae: 0.0216\n",
      "Epoch 101/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0014 - mse: 0.0014 - mae: 0.0207\n",
      "Epoch 102/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0014 - mse: 0.0014 - mae: 0.0202\n",
      "Epoch 103/180\n",
      "6/6 [==============================] - 9s 2s/step - loss: 0.0014 - mse: 0.0014 - mae: 0.0206\n",
      "Epoch 104/180\n",
      "6/6 [==============================] - 9s 2s/step - loss: 0.0014 - mse: 0.0014 - mae: 0.0209\n",
      "Epoch 105/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0014 - mse: 0.0014 - mae: 0.0203\n",
      "Epoch 106/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0013 - mse: 0.0013 - mae: 0.0197\n",
      "Epoch 107/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0013 - mse: 0.0013 - mae: 0.0199\n",
      "Epoch 108/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0014 - mse: 0.0014 - mae: 0.0203\n",
      "Epoch 109/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0013 - mse: 0.0013 - mae: 0.0195\n",
      "Epoch 110/180\n",
      "6/6 [==============================] - 9s 2s/step - loss: 0.0013 - mse: 0.0013 - mae: 0.0197\n",
      "Epoch 111/180\n",
      "6/6 [==============================] - 9s 2s/step - loss: 0.0013 - mse: 0.0013 - mae: 0.0199\n",
      "Epoch 112/180\n",
      "6/6 [==============================] - 9s 2s/step - loss: 0.0013 - mse: 0.0013 - mae: 0.0195\n",
      "Epoch 113/180\n",
      "6/6 [==============================] - 9s 1s/step - loss: 0.0013 - mse: 0.0013 - mae: 0.0195\n",
      "Epoch 114/180\n",
      "6/6 [==============================] - 9s 2s/step - loss: 0.0013 - mse: 0.0013 - mae: 0.0191\n",
      "Epoch 115/180\n",
      "6/6 [==============================] - 9s 1s/step - loss: 0.0013 - mse: 0.0013 - mae: 0.0194\n",
      "Epoch 116/180\n",
      "6/6 [==============================] - 9s 1s/step - loss: 0.0013 - mse: 0.0013 - mae: 0.0190\n",
      "Epoch 117/180\n",
      "6/6 [==============================] - 11s 2s/step - loss: 0.0013 - mse: 0.0013 - mae: 0.0194\n",
      "Epoch 118/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0013 - mse: 0.0013 - mae: 0.0197\n",
      "Epoch 119/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0013 - mse: 0.0013 - mae: 0.0186\n",
      "Epoch 120/180\n",
      "6/6 [==============================] - 8s 1s/step - loss: 0.0013 - mse: 0.0013 - mae: 0.0189\n",
      "Epoch 121/180\n",
      "6/6 [==============================] - 9s 1s/step - loss: 0.0013 - mse: 0.0013 - mae: 0.0191\n",
      "Epoch 122/180\n",
      "6/6 [==============================] - 9s 1s/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0181\n",
      "Epoch 123/180\n",
      "6/6 [==============================] - 9s 1s/step - loss: 0.0013 - mse: 0.0013 - mae: 0.0188\n",
      "Epoch 124/180\n",
      "6/6 [==============================] - 9s 2s/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0186\n",
      "Epoch 125/180\n",
      "6/6 [==============================] - 9s 2s/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0183\n",
      "Epoch 126/180\n",
      "6/6 [==============================] - 9s 1s/step - loss: 0.0013 - mse: 0.0013 - mae: 0.0188\n",
      "Epoch 127/180\n",
      "6/6 [==============================] - 9s 1s/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0182\n",
      "Epoch 128/180\n",
      "6/6 [==============================] - 9s 2s/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0182\n",
      "Epoch 129/180\n",
      "6/6 [==============================] - 9s 2s/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0182\n",
      "Epoch 130/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0184\n",
      "Epoch 131/180\n",
      "6/6 [==============================] - 9s 2s/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0179\n",
      "Epoch 132/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0184\n",
      "Epoch 133/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0181\n",
      "Epoch 134/180\n",
      "6/6 [==============================] - 11s 2s/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0179\n",
      "Epoch 135/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0013 - mse: 0.0013 - mae: 0.0184\n",
      "Epoch 136/180\n",
      "6/6 [==============================] - 9s 1s/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0183\n",
      "Epoch 137/180\n",
      "6/6 [==============================] - 9s 2s/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0179\n",
      "Epoch 138/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0180\n",
      "Epoch 139/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0180\n",
      "Epoch 140/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0180\n",
      "Epoch 141/180\n",
      "6/6 [==============================] - 11s 2s/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0177\n",
      "Epoch 142/180\n",
      "6/6 [==============================] - 11s 2s/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0179\n",
      "Epoch 143/180\n",
      "6/6 [==============================] - 9s 2s/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0174\n",
      "Epoch 144/180\n",
      "6/6 [==============================] - 9s 1s/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0182\n",
      "Epoch 145/180\n",
      "6/6 [==============================] - 9s 2s/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0171\n",
      "Epoch 146/180\n",
      "6/6 [==============================] - 9s 2s/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0173\n",
      "Epoch 147/180\n",
      "6/6 [==============================] - 9s 1s/step - loss: 0.0013 - mse: 0.0013 - mae: 0.0185\n",
      "Epoch 148/180\n",
      "6/6 [==============================] - 9s 2s/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0174\n",
      "Epoch 149/180\n",
      "6/6 [==============================] - 11s 2s/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0174\n",
      "Epoch 150/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0174\n",
      "Epoch 151/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0181\n",
      "Epoch 152/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0171\n",
      "Epoch 153/180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 2s/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0168\n",
      "Epoch 154/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0179\n",
      "Epoch 155/180\n",
      "6/6 [==============================] - 11s 2s/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0167\n",
      "Epoch 156/180\n",
      "6/6 [==============================] - 9s 2s/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0179\n",
      "Epoch 157/180\n",
      "6/6 [==============================] - 9s 1s/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0168\n",
      "Epoch 158/180\n",
      "6/6 [==============================] - 9s 1s/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0176\n",
      "Epoch 159/180\n",
      "6/6 [==============================] - 9s 1s/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0172\n",
      "Epoch 160/180\n",
      "6/6 [==============================] - 9s 2s/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0171\n",
      "Epoch 161/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0174\n",
      "Epoch 162/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0159\n",
      "Epoch 163/180\n",
      "6/6 [==============================] - 9s 2s/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0169\n",
      "Epoch 164/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0177\n",
      "Epoch 165/180\n",
      "6/6 [==============================] - 9s 2s/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0168\n",
      "Epoch 166/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0168\n",
      "Epoch 167/180\n",
      "6/6 [==============================] - 11s 2s/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0174\n",
      "Epoch 168/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0165\n",
      "Epoch 169/180\n",
      "6/6 [==============================] - 11s 2s/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0159\n",
      "Epoch 170/180\n",
      "6/6 [==============================] - 11s 2s/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0172\n",
      "Epoch 171/180\n",
      "6/6 [==============================] - 12s 2s/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0175\n",
      "Epoch 172/180\n",
      "6/6 [==============================] - 11s 2s/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0171\n",
      "Epoch 173/180\n",
      "6/6 [==============================] - 11s 2s/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0182\n",
      "Epoch 174/180\n",
      "6/6 [==============================] - 12s 2s/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0175\n",
      "Epoch 175/180\n",
      "6/6 [==============================] - 11s 2s/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0181\n",
      "Epoch 176/180\n",
      "6/6 [==============================] - 11s 2s/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0164\n",
      "Epoch 177/180\n",
      "6/6 [==============================] - 9s 1s/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0179\n",
      "Epoch 178/180\n",
      "6/6 [==============================] - 9s 1s/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0167\n",
      "Epoch 179/180\n",
      "6/6 [==============================] - 9s 1s/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0165\n",
      "Epoch 180/180\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0168\n"
     ]
    }
   ],
   "source": [
    "# function to split data into correct shape for RNN\n",
    "def split_data(X, y, steps):\n",
    "    X_, y_ = list(), list()\n",
    "    for i in range(steps, len(y)):\n",
    "        X_.append(X[i - steps : i, :])\n",
    "        y_.append(y[i]) \n",
    "    return np.array(X_), np.array(y_)\n",
    "\n",
    "# function to cut data set so it can be divisible by the batch_size\n",
    "def cut_data(data, batch_size):\n",
    "     # see if it is divisivel\n",
    "    condition = data.shape[0] % batch_size\n",
    "    if condition == 0:\n",
    "        return data\n",
    "    else:\n",
    "        return data[: -condition]\n",
    "\n",
    "# design the LSTM\n",
    "def regressor_tunning(kernel_initializer = 'he_uniform',\n",
    "                      bias_initializer = initializers.Ones()):\n",
    "    model = Sequential()\n",
    "    if n_hidden == 1:\n",
    "        model.add(LSTM(units = units,                    \n",
    "                       batch_input_shape = (batch_size, steps, features_num), \n",
    "                       stateful = True,\n",
    "                       kernel_initializer = kernel_initializer,\n",
    "                       bias_initializer = bias_initializer))\n",
    "        model.add(LeakyReLU(alpha = 0.2))\n",
    "        model.add(Dropout(0.2))\n",
    "    else:\n",
    "        model.add(LSTM(units = units,                    \n",
    "                       batch_input_shape = (batch_size, steps, features_num), \n",
    "                       stateful = True,\n",
    "                       return_sequences = True,\n",
    "                       kernel_initializer = kernel_initializer,\n",
    "                       bias_initializer = bias_initializer))\n",
    "        model.add(LeakyReLU(alpha = 0.2))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(units = units, \n",
    "                       batch_input_shape = (batch_size, steps, features_num), \n",
    "                       stateful = True,\n",
    "                       kernel_initializer = kernel_initializer,\n",
    "                       bias_initializer = bias_initializer))\n",
    "        model.add(LeakyReLU(alpha = 0.2))\n",
    "        model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    optimizer = optimizers.RMSprop()\n",
    "    model.compile(loss = 'mse', metrics = ['mse', 'mae'], optimizer = optimizer)\n",
    "    return model\n",
    "\n",
    "# for final one\n",
    "date = [2018110000]\n",
    "  \n",
    "# LOOP STARTS\n",
    "for i in date:\n",
    "    start_time = time.time()\n",
    "    # data\n",
    "    data = data_full.loc[data_full.index > i, :]\n",
    "\n",
    "    # reset index\n",
    "    data.reset_index(inplace = True)\n",
    "    data.drop('index', axis = 1, inplace = True)\n",
    "\n",
    "    # fill nan values in the whole data set\n",
    "    data.fillna(data.mean(), inplace = True)\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # divide data into train and test \n",
    "    data_train, data_test = train_test_split(\n",
    "             data, test_size = 0.15, shuffle=False)  \n",
    "    \n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    # data scaling  (including offer (y))\n",
    "    sc_X = MinMaxScaler()\n",
    "    data_train = sc_X.fit_transform(data_train)\n",
    "    data_test = sc_X.transform(data_test)\n",
    "    \n",
    "    # divide features and labels\n",
    "    X_train = data_train[:, 0:14] \n",
    "    y_train = data_train[:, -1]\n",
    "    X_test = data_test[:, 0:14] \n",
    "    y_test = data_test[:, -1] \n",
    "\n",
    "    # divide data into train and test \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "             X_train, y_train, test_size = 0.10, shuffle=False)\n",
    "\n",
    "    # put data into correct shape\n",
    "    X_train, y_train = split_data(X_train, y_train, steps)\n",
    "    X_test, y_test = split_data(X_test, y_test, steps)\n",
    "    X_val, y_val = split_data(X_val, y_val, steps)\n",
    "\n",
    "    X_train = cut_data(X_train, batch_size)\n",
    "    y_train = cut_data(y_train, batch_size)\n",
    "    X_test = cut_data(X_test, batch_size)\n",
    "    y_test = cut_data(y_test, batch_size)\n",
    "    X_val = cut_data(X_val, batch_size)\n",
    "    y_val = cut_data(y_val, batch_size)\n",
    "\n",
    "    model = regressor_tunning()\n",
    "    \n",
    "    # fitting the LSTM to the training set\n",
    "    history = model.fit(X_train,\n",
    "                        y_train, \n",
    "                        batch_size = batch_size, \n",
    "                        epochs = epochs,\n",
    "                        shuffle = False)\n",
    "                        #validation_data = (X_val, y_val))\n",
    "    \n",
    "    model.reset_states()\n",
    "    \n",
    "    # make new predicitons with test set\n",
    "    y_pred = model.predict(X_test, batch_size = batch_size)\n",
    "    \n",
    "    # prices col = 15 (inverso should not be used as scalling was made with the whole data set)\n",
    "    y_pred = (y_pred * sc_X.data_range_[14]) + (sc_X.data_min_[14])\n",
    "    y_test = (y_test * sc_X.data_range_[14]) + (sc_X.data_min_[14])\n",
    "\n",
    "    # smal adjustment\n",
    "    y_test = pd.Series(y_test)\n",
    "    y_test.replace(0, 0.0001,inplace = True)\n",
    "    \n",
    "    y_pred_list.append(y_pred)\n",
    "    \n",
    "    from sklearn.metrics import mean_squared_error as mse\n",
    "    from sklearn.metrics import mean_absolute_error as mae\n",
    "\n",
    "    rmse_error = mse(y_test, y_pred, squared = False)\n",
    "    mae_error = mae(y_test, y_pred)\n",
    "    \n",
    "    rmse_gen.append(rmse_error)\n",
    "    mae_gen.append(mae_error)\n",
    "    \n",
    "    # =============================================================================\n",
    "    # Metrics evaluation on spike regions\n",
    "    # =============================================================================\n",
    "    \n",
    "    # Need to process data with spike occurences the same way as features\n",
    "    data = pd.read_csv('Spike_binary_1std.csv', index_col = 0)\n",
    "\n",
    "    # set predictive window according with tuning best results\n",
    "    data = data.loc[data.index > i, :]\n",
    "\n",
    "    # make sure shaded area will correspond to values outputed by LSTM\n",
    "    data.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    # fill_nan is already made - so lets split data into test and train\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # divide data into train and test \n",
    "    shade_train, shade_test = train_test_split(\n",
    "             data, test_size = 0.15, shuffle = False)\n",
    "\n",
    "    # reset index of testing data\n",
    "    shade_test.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    # function to split data into correct shape for RNN\n",
    "    def split_data_shade(shade_test, steps):\n",
    "        y_spike_occ = list()\n",
    "        upper_lim = list()\n",
    "        lower_lim = list()\n",
    "        for i in range(steps, len(shade_test.index)):\n",
    "            y_spike_occ.append(shade_test['spike_occurance'][i])\n",
    "            upper_lim.append(shade_test['spike_upperlim'][i])\n",
    "            lower_lim.append(shade_test['spike_lowerlim'][i])\n",
    "        return np.array(y_spike_occ), np.array(upper_lim), np.array(lower_lim)\n",
    "\n",
    "    # function to cut data set so it can be divisible by the batch_size\n",
    "    def cut_data_shade(data, batch_size):\n",
    "         # see if it is divisivel\n",
    "        condition = data.shape[0] % batch_size\n",
    "        if condition == 0:\n",
    "            return data\n",
    "        else:\n",
    "            return data[: -condition]\n",
    "    \n",
    "    # shape y_spike_occ for the right size to compare results in normal and spike regions\n",
    "    y_spike_occ, spike_upperlim, spike_lowerlim = split_data_shade(shade_test, steps)\n",
    "    y_spike_occ = cut_data_shade(y_spike_occ, batch_size)\n",
    "\n",
    "    # continue\n",
    "    \n",
    "    # select y_pred and y_test only for regions with spikes\n",
    "    y_test_spike = (y_test.T * y_spike_occ).T\n",
    "    y_pred_spike = (y_pred.T * y_spike_occ).T\n",
    "    y_test_spike = y_test_spike[y_test_spike != 0]\n",
    "    y_pred_spike = y_pred_spike[y_pred_spike != 0]\n",
    "    \n",
    "    # calculate metric\n",
    "    rmse_spike = mse(y_test_spike, y_pred_spike, squared = False)\n",
    "    mae_spike = mae(y_test_spike, y_pred_spike)\n",
    "    \n",
    "    rmse_spi.append(rmse_spike)\n",
    "    mae_spi.append(mae_spike)\n",
    "    \n",
    "    # =============================================================================\n",
    "    # Metric evaluation on normal regions\n",
    "    # =============================================================================\n",
    "    \n",
    "    # inverse y_spike_occ so the only normal occurences are chosen\n",
    "    y_normal_occ = (y_spike_occ - 1) * (-1)\n",
    "    \n",
    "    # sanity check\n",
    "    y_normal_occ.sum() + y_spike_occ.sum() # gives the correct total \n",
    "    \n",
    "    # select y_pred and y_test only for normal regions\n",
    "    y_test_normal = (y_test.T * y_normal_occ).T\n",
    "    y_pred_normal = (y_pred.T * y_normal_occ).T\n",
    "    y_test_normal = y_test_normal[y_test_normal != 0.00]\n",
    "    y_pred_normal = y_pred_normal[y_pred_normal != 0.00]\n",
    "    \n",
    "    # calculate metric\n",
    "    rmse_normal = mse(y_test_normal, y_pred_normal, squared = False)\n",
    "    mae_normal = mae(y_test_normal, y_pred_normal)\n",
    "    \n",
    "    rmse_nor.append(rmse_normal)\n",
    "    mae_nor.append(mae_normal)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    time_count.append(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({                        \n",
    "                        'rmse_general': rmse_gen, \n",
    "                 \n",
    "                        'mae_general': mae_gen,\n",
    "                        \n",
    "                        'rmse_spike': rmse_spi,\n",
    "                 \n",
    "                        'mae_spike': mae_spi,\n",
    "                        \n",
    "                        'rmse_normal': rmse_nor,\n",
    "                    \n",
    "                        'mae_normal': mae_nor,\n",
    "    \n",
    "                        'time': time_count})\n",
    "\n",
    "#y_pred = pd.DataFrame({'dates': date,\n",
    "#                       'Predicitons': y_pred_list})\n",
    "\n",
    "#y_pred.to_csv('Pedictions_LSTM_5_prediction_window.csv')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_2cdbc2a8_dcb6_11ea_be15_7cb27da2bf47row2_col0 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_2cdbc2a8_dcb6_11ea_be15_7cb27da2bf47row2_col1 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_2cdbc2a8_dcb6_11ea_be15_7cb27da2bf47row2_col4 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_2cdbc2a8_dcb6_11ea_be15_7cb27da2bf47row2_col5 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_2cdbc2a8_dcb6_11ea_be15_7cb27da2bf47row3_col2 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_2cdbc2a8_dcb6_11ea_be15_7cb27da2bf47row3_col3 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_2cdbc2a8_dcb6_11ea_be15_7cb27da2bf47row4_col6 {\n",
       "            background-color:  yellow;\n",
       "        }</style><table id=\"T_2cdbc2a8_dcb6_11ea_be15_7cb27da2bf47\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >rmse_general</th>        <th class=\"col_heading level0 col1\" >mae_general</th>        <th class=\"col_heading level0 col2\" >rmse_spike</th>        <th class=\"col_heading level0 col3\" >mae_spike</th>        <th class=\"col_heading level0 col4\" >rmse_normal</th>        <th class=\"col_heading level0 col5\" >mae_normal</th>        <th class=\"col_heading level0 col6\" >time</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_2cdbc2a8_dcb6_11ea_be15_7cb27da2bf47level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_2cdbc2a8_dcb6_11ea_be15_7cb27da2bf47row0_col0\" class=\"data row0 col0\" >36.753872</td>\n",
       "                        <td id=\"T_2cdbc2a8_dcb6_11ea_be15_7cb27da2bf47row0_col1\" class=\"data row0 col1\" >24.360023</td>\n",
       "                        <td id=\"T_2cdbc2a8_dcb6_11ea_be15_7cb27da2bf47row0_col2\" class=\"data row0 col2\" >49.547439</td>\n",
       "                        <td id=\"T_2cdbc2a8_dcb6_11ea_be15_7cb27da2bf47row0_col3\" class=\"data row0 col3\" >31.386293</td>\n",
       "                        <td id=\"T_2cdbc2a8_dcb6_11ea_be15_7cb27da2bf47row0_col4\" class=\"data row0 col4\" >33.725673</td>\n",
       "                        <td id=\"T_2cdbc2a8_dcb6_11ea_be15_7cb27da2bf47row0_col5\" class=\"data row0 col5\" >23.001824</td>\n",
       "                        <td id=\"T_2cdbc2a8_dcb6_11ea_be15_7cb27da2bf47row0_col6\" class=\"data row0 col6\" >4877.699733</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2cdbc2a8_dcb6_11ea_be15_7cb27da2bf47level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_2cdbc2a8_dcb6_11ea_be15_7cb27da2bf47row1_col0\" class=\"data row1 col0\" >38.962476</td>\n",
       "                        <td id=\"T_2cdbc2a8_dcb6_11ea_be15_7cb27da2bf47row1_col1\" class=\"data row1 col1\" >19.308989</td>\n",
       "                        <td id=\"T_2cdbc2a8_dcb6_11ea_be15_7cb27da2bf47row1_col2\" class=\"data row1 col2\" >61.775859</td>\n",
       "                        <td id=\"T_2cdbc2a8_dcb6_11ea_be15_7cb27da2bf47row1_col3\" class=\"data row1 col3\" >30.429582</td>\n",
       "                        <td id=\"T_2cdbc2a8_dcb6_11ea_be15_7cb27da2bf47row1_col4\" class=\"data row1 col4\" >32.758714</td>\n",
       "                        <td id=\"T_2cdbc2a8_dcb6_11ea_be15_7cb27da2bf47row1_col5\" class=\"data row1 col5\" >17.155980</td>\n",
       "                        <td id=\"T_2cdbc2a8_dcb6_11ea_be15_7cb27da2bf47row1_col6\" class=\"data row1 col6\" >3691.841471</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2cdbc2a8_dcb6_11ea_be15_7cb27da2bf47level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_2cdbc2a8_dcb6_11ea_be15_7cb27da2bf47row2_col0\" class=\"data row2 col0\" >27.244339</td>\n",
       "                        <td id=\"T_2cdbc2a8_dcb6_11ea_be15_7cb27da2bf47row2_col1\" class=\"data row2 col1\" >17.124567</td>\n",
       "                        <td id=\"T_2cdbc2a8_dcb6_11ea_be15_7cb27da2bf47row2_col2\" class=\"data row2 col2\" >33.328991</td>\n",
       "                        <td id=\"T_2cdbc2a8_dcb6_11ea_be15_7cb27da2bf47row2_col3\" class=\"data row2 col3\" >22.994592</td>\n",
       "                        <td id=\"T_2cdbc2a8_dcb6_11ea_be15_7cb27da2bf47row2_col4\" class=\"data row2 col4\" >26.135302</td>\n",
       "                        <td id=\"T_2cdbc2a8_dcb6_11ea_be15_7cb27da2bf47row2_col5\" class=\"data row2 col5\" >16.181714</td>\n",
       "                        <td id=\"T_2cdbc2a8_dcb6_11ea_be15_7cb27da2bf47row2_col6\" class=\"data row2 col6\" >3212.422024</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2cdbc2a8_dcb6_11ea_be15_7cb27da2bf47level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_2cdbc2a8_dcb6_11ea_be15_7cb27da2bf47row3_col0\" class=\"data row3 col0\" >28.740984</td>\n",
       "                        <td id=\"T_2cdbc2a8_dcb6_11ea_be15_7cb27da2bf47row3_col1\" class=\"data row3 col1\" >18.119340</td>\n",
       "                        <td id=\"T_2cdbc2a8_dcb6_11ea_be15_7cb27da2bf47row3_col2\" class=\"data row3 col2\" >32.508364</td>\n",
       "                        <td id=\"T_2cdbc2a8_dcb6_11ea_be15_7cb27da2bf47row3_col3\" class=\"data row3 col3\" >22.292849</td>\n",
       "                        <td id=\"T_2cdbc2a8_dcb6_11ea_be15_7cb27da2bf47row3_col4\" class=\"data row3 col4\" >28.118785</td>\n",
       "                        <td id=\"T_2cdbc2a8_dcb6_11ea_be15_7cb27da2bf47row3_col5\" class=\"data row3 col5\" >17.479466</td>\n",
       "                        <td id=\"T_2cdbc2a8_dcb6_11ea_be15_7cb27da2bf47row3_col6\" class=\"data row3 col6\" >2486.948337</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2cdbc2a8_dcb6_11ea_be15_7cb27da2bf47level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_2cdbc2a8_dcb6_11ea_be15_7cb27da2bf47row4_col0\" class=\"data row4 col0\" >34.610033</td>\n",
       "                        <td id=\"T_2cdbc2a8_dcb6_11ea_be15_7cb27da2bf47row4_col1\" class=\"data row4 col1\" >30.461997</td>\n",
       "                        <td id=\"T_2cdbc2a8_dcb6_11ea_be15_7cb27da2bf47row4_col2\" class=\"data row4 col2\" >40.246530</td>\n",
       "                        <td id=\"T_2cdbc2a8_dcb6_11ea_be15_7cb27da2bf47row4_col3\" class=\"data row4 col3\" >35.016924</td>\n",
       "                        <td id=\"T_2cdbc2a8_dcb6_11ea_be15_7cb27da2bf47row4_col4\" class=\"data row4 col4\" >33.654237</td>\n",
       "                        <td id=\"T_2cdbc2a8_dcb6_11ea_be15_7cb27da2bf47row4_col5\" class=\"data row4 col5\" >29.757627</td>\n",
       "                        <td id=\"T_2cdbc2a8_dcb6_11ea_be15_7cb27da2bf47row4_col6\" class=\"data row4 col6\" >2265.764607</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1950c67cc08>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def highlight_min(s):\n",
    "    '''\n",
    "    highlight the maximum in a Series yellow.\n",
    "    '''\n",
    "    is_max = s == s.min()\n",
    "    return ['background-color: yellow' if v else '' for v in is_max]\n",
    "\n",
    "results.style.apply(highlight_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rmse_general</th>\n",
       "      <th>mae_general</th>\n",
       "      <th>rmse_spike</th>\n",
       "      <th>mae_spike</th>\n",
       "      <th>rmse_normal</th>\n",
       "      <th>mae_normal</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36.753872</td>\n",
       "      <td>24.360023</td>\n",
       "      <td>49.547439</td>\n",
       "      <td>31.386293</td>\n",
       "      <td>33.725673</td>\n",
       "      <td>23.001824</td>\n",
       "      <td>4877.699733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38.962476</td>\n",
       "      <td>19.308989</td>\n",
       "      <td>61.775859</td>\n",
       "      <td>30.429582</td>\n",
       "      <td>32.758714</td>\n",
       "      <td>17.155980</td>\n",
       "      <td>3691.841471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27.244339</td>\n",
       "      <td>17.124567</td>\n",
       "      <td>33.328991</td>\n",
       "      <td>22.994592</td>\n",
       "      <td>26.135302</td>\n",
       "      <td>16.181714</td>\n",
       "      <td>3212.422024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28.740984</td>\n",
       "      <td>18.119340</td>\n",
       "      <td>32.508364</td>\n",
       "      <td>22.292849</td>\n",
       "      <td>28.118785</td>\n",
       "      <td>17.479466</td>\n",
       "      <td>2486.948337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34.610033</td>\n",
       "      <td>30.461997</td>\n",
       "      <td>40.246530</td>\n",
       "      <td>35.016924</td>\n",
       "      <td>33.654237</td>\n",
       "      <td>29.757627</td>\n",
       "      <td>2265.764607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rmse_general  mae_general  rmse_spike  mae_spike  rmse_normal  mae_normal  \\\n",
       "0     36.753872    24.360023   49.547439  31.386293    33.725673   23.001824   \n",
       "1     38.962476    19.308989   61.775859  30.429582    32.758714   17.155980   \n",
       "2     27.244339    17.124567   33.328991  22.994592    26.135302   16.181714   \n",
       "3     28.740984    18.119340   32.508364  22.292849    28.118785   17.479466   \n",
       "4     34.610033    30.461997   40.246530  35.016924    33.654237   29.757627   \n",
       "\n",
       "          time  \n",
       "0  4877.699733  \n",
       "1  3691.841471  \n",
       "2  3212.422024  \n",
       "3  2486.948337  \n",
       "4  2265.764607  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "dates_labels = ['12 ',\n",
    "                '10 ',\n",
    "                '8 ',\n",
    "                '6 ',\n",
    "                '4 ',\n",
    "                '2 ']\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.minorticks_on()\n",
    "plt.grid(which='major', linestyle='-', linewidth='0.5')\n",
    "plt.grid(which='minor', linestyle=':', linewidth='0.5')\n",
    "plt.title('LSTM: Averaged RMSE for different\\n predictive windows')\n",
    "plt.plot(rmse_gen, label = 'Overall error')\n",
    "plt.plot(rmse_spi, label = 'Spike regions')\n",
    "plt.plot(rmse_nor, label = 'Normal regions')\n",
    "plt.legend()\n",
    "plt.ylabel('RMSE (/MWh)')\n",
    "plt.xlabel('Predictive window (in months)')\n",
    "plt.xticks([0,1,2,3,4,5], dates_labels)\n",
    "plt.tight_layout()\n",
    "plt.savefig('RMSE_predictive_window.png')\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.minorticks_on()\n",
    "plt.grid(which='major', linestyle='-', linewidth='0.5')\n",
    "plt.grid(which='minor', linestyle=':', linewidth='0.5')\n",
    "plt.title('LSTM: Averaged MAE for different\\n predictive windows')\n",
    "plt.plot(mae_gen, label = 'Overall error')\n",
    "plt.plot(mae_spi, label = 'Spike regions')\n",
    "plt.plot(mae_nor, label = 'Normal regions')\n",
    "plt.legend()\n",
    "plt.ylabel('MAE (/MWh)')\n",
    "plt.xlabel('Predictive window (in months)')\n",
    "plt.xticks([0,1,2,3,4,5], dates_labels)\n",
    "plt.tight_layout()\n",
    "plt.savefig('MAE_predictive_window.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
