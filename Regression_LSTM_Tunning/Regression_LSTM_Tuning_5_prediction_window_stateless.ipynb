{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression LSTM with best parameters\n",
    "    find the best prediction window to apply w/ lr = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\maria\\anaconda3\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from pandas) (1.18.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from python-dateutil>=2.6.1->pandas) (1.14.0)\n",
      "Requirement already satisfied: sklearn in c:\\users\\maria\\anaconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\maria\\anaconda3\\lib\\site-packages (from sklearn) (0.22.1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.18.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (0.14.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\maria\\anaconda3\\lib\\site-packages (3.1.3)\n",
      "Requirement already satisfied: numpy>=1.11 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from matplotlib) (1.18.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from matplotlib) (2.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: six in c:\\users\\maria\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.14.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\maria\\anaconda3\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib) (45.2.0.post20200210)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install sklearn\n",
    "!pip install matplotlib\n",
    "\n",
    "import pandas as pd;\n",
    "import numpy as np;\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "date =  [2018010000, \n",
    "         2018030000, \n",
    "         2018050000,\n",
    "         2018070000,\n",
    "         2018090000,\n",
    "         2018110000]\n",
    "\n",
    "# parameters\n",
    "steps = 336\n",
    "n_hidden = 2\n",
    "units = 100\n",
    "batch_size = 96\n",
    "epochs = 180\n",
    "features_num = 14\n",
    "\n",
    "# lists to append results\n",
    "mae_gen = []\n",
    "mae_nor = []\n",
    "mae_spi = []\n",
    "rmse_gen = []\n",
    "rmse_nor = []\n",
    "rmse_spi = []\n",
    "y_pred_list = []\n",
    "time_count = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import keras libraries, packages and data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LeakyReLU\n",
    "from keras import initializers\n",
    "from keras import optimizers\n",
    "\n",
    "# import data\n",
    "data_full = pd.read_csv('Data_set_1_smaller_(1).csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create loop for different dates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maria\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3997: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n",
      "C:\\Users\\maria\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/180\n",
      "136/136 [==============================] - 166s 1s/step - loss: 0.1318 - mse: 0.1318 - mae: 0.2607\n",
      "Epoch 2/180\n",
      "136/136 [==============================] - 167s 1s/step - loss: 0.0096 - mse: 0.0096 - mae: 0.0720\n",
      "Epoch 3/180\n",
      "136/136 [==============================] - 169s 1s/step - loss: 0.0023 - mse: 0.0023 - mae: 0.0311\n",
      "Epoch 4/180\n",
      "136/136 [==============================] - 165s 1s/step - loss: 0.0015 - mse: 0.0015 - mae: 0.0231\n",
      "Epoch 5/180\n",
      "136/136 [==============================] - 173s 1s/step - loss: 0.0014 - mse: 0.0014 - mae: 0.0214\n",
      "Epoch 6/180\n",
      "136/136 [==============================] - 172s 1s/step - loss: 0.0014 - mse: 0.0014 - mae: 0.0206\n",
      "Epoch 7/180\n",
      "136/136 [==============================] - 171s 1s/step - loss: 0.0013 - mse: 0.0013 - mae: 0.0202\n",
      "Epoch 8/180\n",
      "136/136 [==============================] - 173s 1s/step - loss: 0.0013 - mse: 0.0013 - mae: 0.0200\n",
      "Epoch 9/180\n",
      "136/136 [==============================] - 197s 1s/step - loss: 0.0013 - mse: 0.0013 - mae: 0.0196\n",
      "Epoch 10/180\n",
      "136/136 [==============================] - 197s 1s/step - loss: 0.0013 - mse: 0.0013 - mae: 0.0193\n",
      "Epoch 11/180\n",
      "136/136 [==============================] - 184s 1s/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0190\n",
      "Epoch 12/180\n",
      "136/136 [==============================] - 186s 1s/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0189\n",
      "Epoch 13/180\n",
      "136/136 [==============================] - 193s 1s/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0183\n",
      "Epoch 14/180\n",
      "136/136 [==============================] - 198s 1s/step - loss: 0.0010 - mse: 0.0010 - mae: 0.0177\n",
      "Epoch 15/180\n",
      "136/136 [==============================] - 196s 1s/step - loss: 9.3245e-04 - mse: 9.3245e-04 - mae: 0.0175\n",
      "Epoch 16/180\n",
      "136/136 [==============================] - 184s 1s/step - loss: 0.0013 - mse: 0.0013 - mae: 0.0178\n",
      "Epoch 17/180\n",
      "136/136 [==============================] - 181s 1s/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0181\n",
      "Epoch 18/180\n",
      "136/136 [==============================] - 193s 1s/step - loss: 8.7479e-04 - mse: 8.7479e-04 - mae: 0.0171\n",
      "Epoch 19/180\n",
      "136/136 [==============================] - 185s 1s/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0175\n",
      "Epoch 20/180\n",
      "136/136 [==============================] - 184s 1s/step - loss: 8.3367e-04 - mse: 8.3367e-04 - mae: 0.0167\n",
      "Epoch 21/180\n",
      "136/136 [==============================] - 189s 1s/step - loss: 8.2388e-04 - mse: 8.2388e-04 - mae: 0.0166\n",
      "Epoch 22/180\n",
      "136/136 [==============================] - 192s 1s/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0174\n",
      "Epoch 23/180\n",
      "136/136 [==============================] - 177s 1s/step - loss: 0.0010 - mse: 0.0010 - mae: 0.0176\n",
      "Epoch 24/180\n",
      "136/136 [==============================] - 176s 1s/step - loss: 8.2383e-04 - mse: 8.2383e-04 - mae: 0.0164\n",
      "Epoch 25/180\n",
      "136/136 [==============================] - 200s 1s/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0170\n",
      "Epoch 26/180\n",
      "136/136 [==============================] - 187s 1s/step - loss: 8.0059e-04 - mse: 8.0059e-04 - mae: 0.0163\n",
      "Epoch 27/180\n",
      "136/136 [==============================] - 194s 1s/step - loss: 8.4875e-04 - mse: 8.4875e-04 - mae: 0.0166\n",
      "Epoch 28/180\n",
      "136/136 [==============================] - 189s 1s/step - loss: 7.4286e-04 - mse: 7.4286e-04 - mae: 0.0162\n",
      "Epoch 29/180\n",
      "136/136 [==============================] - 178s 1s/step - loss: 0.0013 - mse: 0.0013 - mae: 0.0174\n",
      "Epoch 30/180\n",
      "136/136 [==============================] - 186s 1s/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0174\n",
      "Epoch 31/180\n",
      "136/136 [==============================] - 182s 1s/step - loss: 8.5732e-04 - mse: 8.5732e-04 - mae: 0.0165\n",
      "Epoch 32/180\n",
      "136/136 [==============================] - 187s 1s/step - loss: 0.0013 - mse: 0.0013 - mae: 0.0173\n",
      "Epoch 33/180\n",
      "136/136 [==============================] - 184s 1s/step - loss: 7.8896e-04 - mse: 7.8896e-04 - mae: 0.0164\n",
      "Epoch 34/180\n",
      "136/136 [==============================] - 185s 1s/step - loss: 9.0050e-04 - mse: 9.0050e-04 - mae: 0.0167\n",
      "Epoch 35/180\n",
      "136/136 [==============================] - 183s 1s/step - loss: 9.1923e-04 - mse: 9.1923e-04 - mae: 0.0166\n",
      "Epoch 36/180\n",
      "136/136 [==============================] - 176s 1s/step - loss: 0.0010 - mse: 0.0010 - mae: 0.0168\n",
      "Epoch 37/180\n",
      "136/136 [==============================] - 180s 1s/step - loss: 8.2307e-04 - mse: 8.2307e-04 - mae: 0.0163\n",
      "Epoch 38/180\n",
      "136/136 [==============================] - 177s 1s/step - loss: 8.7290e-04 - mse: 8.7290e-04 - mae: 0.0165\n",
      "Epoch 39/180\n",
      "136/136 [==============================] - 178s 1s/step - loss: 0.0010 - mse: 0.0010 - mae: 0.0168\n",
      "Epoch 40/180\n",
      "136/136 [==============================] - 184s 1s/step - loss: 7.7912e-04 - mse: 7.7912e-04 - mae: 0.0160\n",
      "Epoch 41/180\n",
      "136/136 [==============================] - 182s 1s/step - loss: 7.0905e-04 - mse: 7.0905e-04 - mae: 0.0158\n",
      "Epoch 42/180\n",
      "136/136 [==============================] - 189s 1s/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0166\n",
      "Epoch 43/180\n",
      "136/136 [==============================] - 182s 1s/step - loss: 7.0556e-04 - mse: 7.0556e-04 - mae: 0.0158\n",
      "Epoch 44/180\n",
      "136/136 [==============================] - 184s 1s/step - loss: 0.0013 - mse: 0.0013 - mae: 0.0170\n",
      "Epoch 45/180\n",
      "136/136 [==============================] - 189s 1s/step - loss: 9.0993e-04 - mse: 9.0993e-04 - mae: 0.0166\n",
      "Epoch 46/180\n",
      "136/136 [==============================] - 185s 1s/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0166\n",
      "Epoch 47/180\n",
      "136/136 [==============================] - 188s 1s/step - loss: 7.3845e-04 - mse: 7.3845e-04 - mae: 0.0158\n",
      "Epoch 48/180\n",
      "136/136 [==============================] - 189s 1s/step - loss: 7.0486e-04 - mse: 7.0486e-04 - mae: 0.0157\n",
      "Epoch 49/180\n",
      "136/136 [==============================] - 192s 1s/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0168\n",
      "Epoch 50/180\n",
      "136/136 [==============================] - 187s 1s/step - loss: 8.4634e-04 - mse: 8.4634e-04 - mae: 0.0162\n",
      "Epoch 51/180\n",
      "136/136 [==============================] - 188s 1s/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0164\n",
      "Epoch 52/180\n",
      "136/136 [==============================] - 189s 1s/step - loss: 6.8926e-04 - mse: 6.8926e-04 - mae: 0.0156\n",
      "Epoch 53/180\n",
      "136/136 [==============================] - 191s 1s/step - loss: 7.3224e-04 - mse: 7.3224e-04 - mae: 0.0159\n",
      "Epoch 54/180\n",
      "136/136 [==============================] - 184s 1s/step - loss: 8.5859e-04 - mse: 8.5859e-04 - mae: 0.0161\n",
      "Epoch 55/180\n",
      "136/136 [==============================] - 174s 1s/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0166\n",
      "Epoch 56/180\n",
      "136/136 [==============================] - 174s 1s/step - loss: 7.8542e-04 - mse: 7.8542e-04 - mae: 0.0160\n",
      "Epoch 57/180\n",
      "136/136 [==============================] - 174s 1s/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0166\n",
      "Epoch 58/180\n",
      "136/136 [==============================] - 178s 1s/step - loss: 7.0388e-04 - mse: 7.0388e-04 - mae: 0.0155\n",
      "Epoch 59/180\n",
      "136/136 [==============================] - 182s 1s/step - loss: 7.7448e-04 - mse: 7.7448e-04 - mae: 0.0159\n",
      "Epoch 60/180\n",
      "136/136 [==============================] - 181s 1s/step - loss: 9.6763e-04 - mse: 9.6763e-04 - mae: 0.0163\n",
      "Epoch 61/180\n",
      "136/136 [==============================] - 184s 1s/step - loss: 8.5037e-04 - mse: 8.5037e-04 - mae: 0.0158\n",
      "Epoch 62/180\n",
      "136/136 [==============================] - 184s 1s/step - loss: 7.0457e-04 - mse: 7.0457e-04 - mae: 0.0156\n",
      "Epoch 63/180\n",
      "136/136 [==============================] - 186s 1s/step - loss: 8.0921e-04 - mse: 8.0921e-04 - mae: 0.0159\n",
      "Epoch 64/180\n",
      "136/136 [==============================] - 180s 1s/step - loss: 8.1800e-04 - mse: 8.1800e-04 - mae: 0.0159\n",
      "Epoch 65/180\n",
      "136/136 [==============================] - 174s 1s/step - loss: 6.6534e-04 - mse: 6.6534e-04 - mae: 0.0154\n",
      "Epoch 66/180\n",
      "136/136 [==============================] - 175s 1s/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0163\n",
      "Epoch 67/180\n",
      "136/136 [==============================] - 176s 1s/step - loss: 7.1388e-04 - mse: 7.1388e-04 - mae: 0.0155\n",
      "Epoch 68/180\n",
      "136/136 [==============================] - 177s 1s/step - loss: 7.8630e-04 - mse: 7.8630e-04 - mae: 0.0159\n",
      "Epoch 69/180\n",
      "136/136 [==============================] - 182s 1s/step - loss: 9.2958e-04 - mse: 9.2958e-04 - mae: 0.0163\n",
      "Epoch 70/180\n",
      "136/136 [==============================] - 183s 1s/step - loss: 7.8230e-04 - mse: 7.8230e-04 - mae: 0.0157\n",
      "Epoch 71/180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 187s 1s/step - loss: 7.6563e-04 - mse: 7.6563e-04 - mae: 0.0154\n",
      "Epoch 72/180\n",
      "136/136 [==============================] - 185s 1s/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0165\n",
      "Epoch 73/180\n",
      "136/136 [==============================] - 187s 1s/step - loss: 8.8067e-04 - mse: 8.8067e-04 - mae: 0.0158\n",
      "Epoch 74/180\n",
      "136/136 [==============================] - 195s 1s/step - loss: 8.1008e-04 - mse: 8.1008e-04 - mae: 0.0156\n",
      "Epoch 75/180\n",
      "136/136 [==============================] - 184s 1s/step - loss: 8.9149e-04 - mse: 8.9149e-04 - mae: 0.0159\n",
      "Epoch 76/180\n",
      "136/136 [==============================] - 185s 1s/step - loss: 7.1413e-04 - mse: 7.1413e-04 - mae: 0.0154\n",
      "Epoch 77/180\n",
      "136/136 [==============================] - 186s 1s/step - loss: 6.6726e-04 - mse: 6.6726e-04 - mae: 0.0152\n",
      "Epoch 78/180\n",
      "136/136 [==============================] - 182s 1s/step - loss: 9.0732e-04 - mse: 9.0732e-04 - mae: 0.0157\n",
      "Epoch 79/180\n",
      "136/136 [==============================] - 175s 1s/step - loss: 6.9005e-04 - mse: 6.9005e-04 - mae: 0.0153\n",
      "Epoch 80/180\n",
      "136/136 [==============================] - 174s 1s/step - loss: 8.7975e-04 - mse: 8.7975e-04 - mae: 0.0161\n",
      "Epoch 81/180\n",
      "136/136 [==============================] - 179s 1s/step - loss: 8.1540e-04 - mse: 8.1540e-04 - mae: 0.0156\n",
      "Epoch 82/180\n",
      "136/136 [==============================] - 175s 1s/step - loss: 7.5316e-04 - mse: 7.5316e-04 - mae: 0.0154\n",
      "Epoch 83/180\n",
      "136/136 [==============================] - 177s 1s/step - loss: 7.0827e-04 - mse: 7.0827e-04 - mae: 0.0153\n",
      "Epoch 84/180\n",
      "136/136 [==============================] - 186s 1s/step - loss: 6.4588e-04 - mse: 6.4588e-04 - mae: 0.0150\n",
      "Epoch 85/180\n",
      "136/136 [==============================] - 182s 1s/step - loss: 6.4084e-04 - mse: 6.4084e-04 - mae: 0.0150\n",
      "Epoch 86/180\n",
      "136/136 [==============================] - 182s 1s/step - loss: 7.2700e-04 - mse: 7.2700e-04 - mae: 0.0153\n",
      "Epoch 87/180\n",
      "136/136 [==============================] - 188s 1s/step - loss: 6.6520e-04 - mse: 6.6520e-04 - mae: 0.0150\n",
      "Epoch 88/180\n",
      "136/136 [==============================] - 184s 1s/step - loss: 6.5425e-04 - mse: 6.5425e-04 - mae: 0.0150\n",
      "Epoch 89/180\n",
      "136/136 [==============================] - 186s 1s/step - loss: 6.2475e-04 - mse: 6.2475e-04 - mae: 0.0148\n",
      "Epoch 90/180\n",
      "136/136 [==============================] - 187s 1s/step - loss: 8.6259e-04 - mse: 8.6259e-04 - mae: 0.0157\n",
      "Epoch 91/180\n",
      "136/136 [==============================] - 183s 1s/step - loss: 6.5393e-04 - mse: 6.5393e-04 - mae: 0.0149\n",
      "Epoch 92/180\n",
      "136/136 [==============================] - 185s 1s/step - loss: 7.8361e-04 - mse: 7.8361e-04 - mae: 0.0154\n",
      "Epoch 93/180\n",
      "136/136 [==============================] - 185s 1s/step - loss: 7.1657e-04 - mse: 7.1657e-04 - mae: 0.0153\n",
      "Epoch 94/180\n",
      "136/136 [==============================] - 192s 1s/step - loss: 6.5663e-04 - mse: 6.5663e-04 - mae: 0.0151\n",
      "Epoch 95/180\n",
      "136/136 [==============================] - 187s 1s/step - loss: 6.2390e-04 - mse: 6.2390e-04 - mae: 0.0147\n",
      "Epoch 96/180\n",
      "136/136 [==============================] - 186s 1s/step - loss: 6.4987e-04 - mse: 6.4987e-04 - mae: 0.0150\n",
      "Epoch 97/180\n",
      "136/136 [==============================] - 189s 1s/step - loss: 6.7568e-04 - mse: 6.7568e-04 - mae: 0.0150\n",
      "Epoch 98/180\n",
      "136/136 [==============================] - 189s 1s/step - loss: 7.5867e-04 - mse: 7.5867e-04 - mae: 0.0152\n",
      "Epoch 99/180\n",
      "136/136 [==============================] - 187s 1s/step - loss: 6.2372e-04 - mse: 6.2372e-04 - mae: 0.0147\n",
      "Epoch 100/180\n",
      "136/136 [==============================] - 205s 2s/step - loss: 7.8987e-04 - mse: 7.8987e-04 - mae: 0.0152\n",
      "Epoch 101/180\n",
      "136/136 [==============================] - 191s 1s/step - loss: 6.2620e-04 - mse: 6.2620e-04 - mae: 0.0146\n",
      "Epoch 102/180\n",
      "136/136 [==============================] - 174s 1s/step - loss: 8.7683e-04 - mse: 8.7683e-04 - mae: 0.0154\n",
      "Epoch 103/180\n",
      "136/136 [==============================] - 200s 1s/step - loss: 5.9949e-04 - mse: 5.9949e-04 - mae: 0.0147\n",
      "Epoch 104/180\n",
      "136/136 [==============================] - 176s 1s/step - loss: 6.1408e-04 - mse: 6.1408e-04 - mae: 0.0145\n",
      "Epoch 105/180\n",
      "136/136 [==============================] - 173s 1s/step - loss: 6.5536e-04 - mse: 6.5536e-04 - mae: 0.0149\n",
      "Epoch 106/180\n",
      "136/136 [==============================] - 176s 1s/step - loss: 6.8701e-04 - mse: 6.8701e-04 - mae: 0.0150\n",
      "Epoch 107/180\n",
      "136/136 [==============================] - 175s 1s/step - loss: 6.0239e-04 - mse: 6.0239e-04 - mae: 0.0145\n",
      "Epoch 108/180\n",
      "136/136 [==============================] - 177s 1s/step - loss: 6.6305e-04 - mse: 6.6305e-04 - mae: 0.0148\n",
      "Epoch 109/180\n",
      "136/136 [==============================] - 174s 1s/step - loss: 5.8342e-04 - mse: 5.8342e-04 - mae: 0.0144\n",
      "Epoch 110/180\n",
      "136/136 [==============================] - 177s 1s/step - loss: 6.6374e-04 - mse: 6.6374e-04 - mae: 0.0147\n",
      "Epoch 111/180\n",
      "136/136 [==============================] - 175s 1s/step - loss: 6.5430e-04 - mse: 6.5430e-04 - mae: 0.0147\n",
      "Epoch 112/180\n",
      "136/136 [==============================] - 176s 1s/step - loss: 7.0325e-04 - mse: 7.0325e-04 - mae: 0.0150\n",
      "Epoch 113/180\n",
      "136/136 [==============================] - 184s 1s/step - loss: 6.2041e-04 - mse: 6.2041e-04 - mae: 0.0147\n",
      "Epoch 114/180\n",
      "136/136 [==============================] - 175s 1s/step - loss: 5.9028e-04 - mse: 5.9028e-04 - mae: 0.0145\n",
      "Epoch 115/180\n",
      "136/136 [==============================] - 174s 1s/step - loss: 6.3444e-04 - mse: 6.3444e-04 - mae: 0.0146\n",
      "Epoch 116/180\n",
      "136/136 [==============================] - 174s 1s/step - loss: 6.1390e-04 - mse: 6.1390e-04 - mae: 0.0145\n",
      "Epoch 117/180\n",
      "136/136 [==============================] - 176s 1s/step - loss: 5.6866e-04 - mse: 5.6866e-04 - mae: 0.0143\n",
      "Epoch 118/180\n",
      "136/136 [==============================] - 176s 1s/step - loss: 6.1616e-04 - mse: 6.1616e-04 - mae: 0.0148\n",
      "Epoch 119/180\n",
      "136/136 [==============================] - 179s 1s/step - loss: 8.0466e-04 - mse: 8.0466e-04 - mae: 0.0151\n",
      "Epoch 120/180\n",
      "136/136 [==============================] - 176s 1s/step - loss: 5.7933e-04 - mse: 5.7933e-04 - mae: 0.0144\n",
      "Epoch 121/180\n",
      "136/136 [==============================] - 177s 1s/step - loss: 6.2504e-04 - mse: 6.2504e-04 - mae: 0.0147\n",
      "Epoch 122/180\n",
      "136/136 [==============================] - 176s 1s/step - loss: 9.2285e-04 - mse: 9.2285e-04 - mae: 0.0155\n",
      "Epoch 123/180\n",
      "136/136 [==============================] - 180s 1s/step - loss: 6.2315e-04 - mse: 6.2315e-04 - mae: 0.0146\n",
      "Epoch 124/180\n",
      "136/136 [==============================] - 178s 1s/step - loss: 6.5256e-04 - mse: 6.5256e-04 - mae: 0.0147\n",
      "Epoch 125/180\n",
      "136/136 [==============================] - 177s 1s/step - loss: 6.0460e-04 - mse: 6.0460e-04 - mae: 0.0145\n",
      "Epoch 126/180\n",
      "136/136 [==============================] - 177s 1s/step - loss: 6.5109e-04 - mse: 6.5109e-04 - mae: 0.0146\n",
      "Epoch 127/180\n",
      "136/136 [==============================] - 176s 1s/step - loss: 5.7912e-04 - mse: 5.7912e-04 - mae: 0.0143\n",
      "Epoch 128/180\n",
      "136/136 [==============================] - 174s 1s/step - loss: 5.9879e-04 - mse: 5.9879e-04 - mae: 0.0145\n",
      "Epoch 129/180\n",
      "136/136 [==============================] - 174s 1s/step - loss: 5.6146e-04 - mse: 5.6146e-04 - mae: 0.0143\n",
      "Epoch 130/180\n",
      "136/136 [==============================] - 176s 1s/step - loss: 6.0372e-04 - mse: 6.0372e-04 - mae: 0.0145\n",
      "Epoch 131/180\n",
      "136/136 [==============================] - 178s 1s/step - loss: 6.6812e-04 - mse: 6.6812e-04 - mae: 0.0147\n",
      "Epoch 132/180\n",
      "136/136 [==============================] - 175s 1s/step - loss: 5.9269e-04 - mse: 5.9269e-04 - mae: 0.0143\n",
      "Epoch 133/180\n",
      "136/136 [==============================] - 184s 1s/step - loss: 5.7425e-04 - mse: 5.7425e-04 - mae: 0.0144\n",
      "Epoch 134/180\n",
      "136/136 [==============================] - 175s 1s/step - loss: 6.3185e-04 - mse: 6.3185e-04 - mae: 0.0145\n",
      "Epoch 135/180\n",
      "136/136 [==============================] - 174s 1s/step - loss: 5.3120e-04 - mse: 5.3120e-04 - mae: 0.0140\n",
      "Epoch 136/180\n",
      "136/136 [==============================] - 174s 1s/step - loss: 5.9577e-04 - mse: 5.9577e-04 - mae: 0.0145\n",
      "Epoch 137/180\n",
      "136/136 [==============================] - 175s 1s/step - loss: 6.5261e-04 - mse: 6.5261e-04 - mae: 0.0146\n",
      "Epoch 138/180\n",
      "136/136 [==============================] - 176s 1s/step - loss: 5.5829e-04 - mse: 5.5829e-04 - mae: 0.0141\n",
      "Epoch 139/180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 175s 1s/step - loss: 5.9145e-04 - mse: 5.9145e-04 - mae: 0.0145\n",
      "Epoch 140/180\n",
      "136/136 [==============================] - 177s 1s/step - loss: 5.4638e-04 - mse: 5.4638e-04 - mae: 0.0142\n",
      "Epoch 141/180\n",
      "136/136 [==============================] - 175s 1s/step - loss: 6.1076e-04 - mse: 6.1076e-04 - mae: 0.0142\n",
      "Epoch 142/180\n",
      "136/136 [==============================] - 173s 1s/step - loss: 5.7466e-04 - mse: 5.7466e-04 - mae: 0.0143\n",
      "Epoch 143/180\n",
      "136/136 [==============================] - 177s 1s/step - loss: 5.4380e-04 - mse: 5.4380e-04 - mae: 0.0140\n",
      "Epoch 144/180\n",
      "136/136 [==============================] - 178s 1s/step - loss: 5.2742e-04 - mse: 5.2742e-04 - mae: 0.0140\n",
      "Epoch 145/180\n",
      "136/136 [==============================] - 177s 1s/step - loss: 5.5224e-04 - mse: 5.5224e-04 - mae: 0.0142\n",
      "Epoch 146/180\n",
      "136/136 [==============================] - 174s 1s/step - loss: 7.0930e-04 - mse: 7.0930e-04 - mae: 0.0147\n",
      "Epoch 147/180\n",
      "136/136 [==============================] - 175s 1s/step - loss: 5.3056e-04 - mse: 5.3056e-04 - mae: 0.0141\n",
      "Epoch 148/180\n",
      "136/136 [==============================] - 176s 1s/step - loss: 5.6368e-04 - mse: 5.6368e-04 - mae: 0.0141\n",
      "Epoch 149/180\n",
      "136/136 [==============================] - 174s 1s/step - loss: 5.5463e-04 - mse: 5.5463e-04 - mae: 0.0141\n",
      "Epoch 150/180\n",
      "136/136 [==============================] - 178s 1s/step - loss: 5.3961e-04 - mse: 5.3961e-04 - mae: 0.0141\n",
      "Epoch 151/180\n",
      "136/136 [==============================] - 174s 1s/step - loss: 5.4065e-04 - mse: 5.4065e-04 - mae: 0.0140\n",
      "Epoch 152/180\n",
      "136/136 [==============================] - 175s 1s/step - loss: 5.2302e-04 - mse: 5.2302e-04 - mae: 0.0141\n",
      "Epoch 153/180\n",
      "136/136 [==============================] - 181s 1s/step - loss: 6.1541e-04 - mse: 6.1541e-04 - mae: 0.0145\n",
      "Epoch 154/180\n",
      "136/136 [==============================] - 178s 1s/step - loss: 5.0995e-04 - mse: 5.0995e-04 - mae: 0.0140\n",
      "Epoch 155/180\n",
      "136/136 [==============================] - 174s 1s/step - loss: 5.6599e-04 - mse: 5.6599e-04 - mae: 0.0141\n",
      "Epoch 156/180\n",
      "136/136 [==============================] - 175s 1s/step - loss: 6.2067e-04 - mse: 6.2067e-04 - mae: 0.0145\n",
      "Epoch 157/180\n",
      "136/136 [==============================] - 176s 1s/step - loss: 5.1038e-04 - mse: 5.1038e-04 - mae: 0.0139\n",
      "Epoch 158/180\n",
      "136/136 [==============================] - 177s 1s/step - loss: 5.0640e-04 - mse: 5.0640e-04 - mae: 0.0140\n",
      "Epoch 159/180\n",
      "136/136 [==============================] - 176s 1s/step - loss: 5.6543e-04 - mse: 5.6543e-04 - mae: 0.0142\n",
      "Epoch 160/180\n",
      "136/136 [==============================] - 175s 1s/step - loss: 5.5731e-04 - mse: 5.5731e-04 - mae: 0.0142\n",
      "Epoch 161/180\n",
      "136/136 [==============================] - 177s 1s/step - loss: 4.8777e-04 - mse: 4.8777e-04 - mae: 0.0138\n",
      "Epoch 162/180\n",
      "136/136 [==============================] - 174s 1s/step - loss: 4.8561e-04 - mse: 4.8561e-04 - mae: 0.0139\n",
      "Epoch 163/180\n",
      "136/136 [==============================] - 179s 1s/step - loss: 4.8976e-04 - mse: 4.8976e-04 - mae: 0.0139\n",
      "Epoch 164/180\n",
      "136/136 [==============================] - 191s 1s/step - loss: 4.8455e-04 - mse: 4.8455e-04 - mae: 0.0138\n",
      "Epoch 165/180\n",
      "136/136 [==============================] - 179s 1s/step - loss: 5.2864e-04 - mse: 5.2864e-04 - mae: 0.0141\n",
      "Epoch 166/180\n",
      "136/136 [==============================] - 176s 1s/step - loss: 5.0218e-04 - mse: 5.0218e-04 - mae: 0.0139\n",
      "Epoch 167/180\n",
      "136/136 [==============================] - 189s 1s/step - loss: 5.3899e-04 - mse: 5.3899e-04 - mae: 0.0139\n",
      "Epoch 168/180\n",
      "136/136 [==============================] - 175s 1s/step - loss: 6.0652e-04 - mse: 6.0652e-04 - mae: 0.0142\n",
      "Epoch 169/180\n",
      "136/136 [==============================] - 175s 1s/step - loss: 5.0953e-04 - mse: 5.0953e-04 - mae: 0.0138\n",
      "Epoch 170/180\n",
      "136/136 [==============================] - 179s 1s/step - loss: 4.5398e-04 - mse: 4.5398e-04 - mae: 0.0135\n",
      "Epoch 171/180\n",
      "136/136 [==============================] - 177s 1s/step - loss: 5.3864e-04 - mse: 5.3864e-04 - mae: 0.0137\n",
      "Epoch 172/180\n",
      "136/136 [==============================] - 176s 1s/step - loss: 5.0018e-04 - mse: 5.0018e-04 - mae: 0.0137\n",
      "Epoch 173/180\n",
      "136/136 [==============================] - 180s 1s/step - loss: 4.8153e-04 - mse: 4.8153e-04 - mae: 0.0136\n",
      "Epoch 174/180\n",
      "136/136 [==============================] - 181s 1s/step - loss: 4.8881e-04 - mse: 4.8881e-04 - mae: 0.0135\n",
      "Epoch 175/180\n",
      "136/136 [==============================] - 174s 1s/step - loss: 5.2235e-04 - mse: 5.2235e-04 - mae: 0.0138\n",
      "Epoch 176/180\n",
      "136/136 [==============================] - 175s 1s/step - loss: 4.8068e-04 - mse: 4.8068e-04 - mae: 0.0136\n",
      "Epoch 177/180\n",
      "136/136 [==============================] - 176s 1s/step - loss: 4.6209e-04 - mse: 4.6209e-04 - mae: 0.0136\n",
      "Epoch 178/180\n",
      "136/136 [==============================] - 176s 1s/step - loss: 5.0775e-04 - mse: 5.0775e-04 - mae: 0.0137\n",
      "Epoch 179/180\n",
      "136/136 [==============================] - 174s 1s/step - loss: 4.6346e-04 - mse: 4.6346e-04 - mae: 0.0137\n",
      "Epoch 180/180\n",
      "136/136 [==============================] - 174s 1s/step - loss: 4.7651e-04 - mse: 4.7651e-04 - mae: 0.0135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maria\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3997: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n",
      "C:\\Users\\maria\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/180\n",
      "113/113 [==============================] - 148s 1s/step - loss: 0.1420 - mse: 0.1420 - mae: 0.2704\n",
      "Epoch 2/180\n",
      "113/113 [==============================] - 150s 1s/step - loss: 0.0153 - mse: 0.0153 - mae: 0.0952\n",
      "Epoch 3/180\n",
      "113/113 [==============================] - 149s 1s/step - loss: 0.0028 - mse: 0.0028 - mae: 0.0405\n",
      "Epoch 4/180\n",
      "113/113 [==============================] - 155s 1s/step - loss: 0.0010 - mse: 0.0010 - mae: 0.0230\n",
      "Epoch 5/180\n",
      "113/113 [==============================] - 150s 1s/step - loss: 7.8838e-04 - mse: 7.8838e-04 - mae: 0.0188\n",
      "Epoch 6/180\n",
      "113/113 [==============================] - 150s 1s/step - loss: 7.1838e-04 - mse: 7.1838e-04 - mae: 0.0175\n",
      "Epoch 7/180\n",
      "113/113 [==============================] - 151s 1s/step - loss: 6.7747e-04 - mse: 6.7747e-04 - mae: 0.0168\n",
      "Epoch 8/180\n",
      "113/113 [==============================] - 153s 1s/step - loss: 6.4622e-04 - mse: 6.4622e-04 - mae: 0.0163\n",
      "Epoch 9/180\n",
      "113/113 [==============================] - 154s 1s/step - loss: 6.2747e-04 - mse: 6.2747e-04 - mae: 0.0161\n",
      "Epoch 10/180\n",
      "113/113 [==============================] - 154s 1s/step - loss: 6.0893e-04 - mse: 6.0893e-04 - mae: 0.0157\n",
      "Epoch 11/180\n",
      "113/113 [==============================] - 154s 1s/step - loss: 5.8915e-04 - mse: 5.8915e-04 - mae: 0.0154\n",
      "Epoch 12/180\n",
      "113/113 [==============================] - 155s 1s/step - loss: 5.6816e-04 - mse: 5.6816e-04 - mae: 0.0150\n",
      "Epoch 13/180\n",
      "113/113 [==============================] - 152s 1s/step - loss: 5.5578e-04 - mse: 5.5578e-04 - mae: 0.0148\n",
      "Epoch 14/180\n",
      "113/113 [==============================] - 153s 1s/step - loss: 5.4934e-04 - mse: 5.4934e-04 - mae: 0.0146\n",
      "Epoch 15/180\n",
      "113/113 [==============================] - 155s 1s/step - loss: 5.4162e-04 - mse: 5.4162e-04 - mae: 0.0145\n",
      "Epoch 16/180\n",
      "113/113 [==============================] - 161s 1s/step - loss: 5.3662e-04 - mse: 5.3662e-04 - mae: 0.0144\n",
      "Epoch 17/180\n",
      "113/113 [==============================] - 155s 1s/step - loss: 5.3221e-04 - mse: 5.3221e-04 - mae: 0.0143\n",
      "Epoch 18/180\n",
      "113/113 [==============================] - 155s 1s/step - loss: 5.3127e-04 - mse: 5.3127e-04 - mae: 0.0142\n",
      "Epoch 19/180\n",
      "113/113 [==============================] - 154s 1s/step - loss: 5.3040e-04 - mse: 5.3040e-04 - mae: 0.0142\n",
      "Epoch 20/180\n",
      "113/113 [==============================] - 155s 1s/step - loss: 5.2845e-04 - mse: 5.2845e-04 - mae: 0.0142\n",
      "Epoch 21/180\n",
      "113/113 [==============================] - 153s 1s/step - loss: 5.2375e-04 - mse: 5.2375e-04 - mae: 0.0141\n",
      "Epoch 22/180\n",
      "113/113 [==============================] - 153s 1s/step - loss: 5.2550e-04 - mse: 5.2550e-04 - mae: 0.0141\n",
      "Epoch 23/180\n",
      "113/113 [==============================] - 155s 1s/step - loss: 5.2091e-04 - mse: 5.2091e-04 - mae: 0.0140\n",
      "Epoch 24/180\n",
      "113/113 [==============================] - 155s 1s/step - loss: 5.1989e-04 - mse: 5.1989e-04 - mae: 0.0140\n",
      "Epoch 25/180\n",
      "113/113 [==============================] - 154s 1s/step - loss: 5.2118e-04 - mse: 5.2118e-04 - mae: 0.0140\n",
      "Epoch 26/180\n",
      "113/113 [==============================] - 154s 1s/step - loss: 5.2109e-04 - mse: 5.2109e-04 - mae: 0.0140\n",
      "Epoch 27/180\n",
      "113/113 [==============================] - 154s 1s/step - loss: 5.1695e-04 - mse: 5.1695e-04 - mae: 0.0140\n",
      "Epoch 28/180\n",
      "113/113 [==============================] - 156s 1s/step - loss: 5.1867e-04 - mse: 5.1867e-04 - mae: 0.0140\n",
      "Epoch 29/180\n",
      "113/113 [==============================] - 155s 1s/step - loss: 5.1756e-04 - mse: 5.1756e-04 - mae: 0.0139\n",
      "Epoch 30/180\n",
      "113/113 [==============================] - 155s 1s/step - loss: 5.1668e-04 - mse: 5.1668e-04 - mae: 0.0140\n",
      "Epoch 31/180\n",
      "113/113 [==============================] - 154s 1s/step - loss: 5.1147e-04 - mse: 5.1147e-04 - mae: 0.0138\n",
      "Epoch 32/180\n",
      "113/113 [==============================] - 153s 1s/step - loss: 5.1303e-04 - mse: 5.1303e-04 - mae: 0.0139\n",
      "Epoch 33/180\n",
      "113/113 [==============================] - 153s 1s/step - loss: 5.1602e-04 - mse: 5.1602e-04 - mae: 0.0140\n",
      "Epoch 34/180\n",
      "113/113 [==============================] - 153s 1s/step - loss: 5.1200e-04 - mse: 5.1200e-04 - mae: 0.0139\n",
      "Epoch 35/180\n",
      "113/113 [==============================] - 155s 1s/step - loss: 5.0617e-04 - mse: 5.0617e-04 - mae: 0.0138\n",
      "Epoch 36/180\n",
      "113/113 [==============================] - 153s 1s/step - loss: 5.1090e-04 - mse: 5.1090e-04 - mae: 0.0139\n",
      "Epoch 37/180\n",
      "113/113 [==============================] - 153s 1s/step - loss: 5.1241e-04 - mse: 5.1241e-04 - mae: 0.0139\n",
      "Epoch 38/180\n",
      "113/113 [==============================] - 155s 1s/step - loss: 5.0776e-04 - mse: 5.0776e-04 - mae: 0.0138\n",
      "Epoch 39/180\n",
      "113/113 [==============================] - 165s 1s/step - loss: 5.0903e-04 - mse: 5.0903e-04 - mae: 0.0139\n",
      "Epoch 40/180\n",
      "113/113 [==============================] - 156s 1s/step - loss: 5.0901e-04 - mse: 5.0901e-04 - mae: 0.0138\n",
      "Epoch 41/180\n",
      "113/113 [==============================] - 153s 1s/step - loss: 5.0577e-04 - mse: 5.0577e-04 - mae: 0.0138\n",
      "Epoch 42/180\n",
      "113/113 [==============================] - 153s 1s/step - loss: 5.0512e-04 - mse: 5.0512e-04 - mae: 0.0138\n",
      "Epoch 43/180\n",
      "113/113 [==============================] - 154s 1s/step - loss: 5.0409e-04 - mse: 5.0409e-04 - mae: 0.0138\n",
      "Epoch 44/180\n",
      "113/113 [==============================] - 155s 1s/step - loss: 5.0469e-04 - mse: 5.0469e-04 - mae: 0.0138\n",
      "Epoch 45/180\n",
      "113/113 [==============================] - 153s 1s/step - loss: 5.0374e-04 - mse: 5.0374e-04 - mae: 0.0137\n",
      "Epoch 46/180\n",
      "113/113 [==============================] - 153s 1s/step - loss: 4.9773e-04 - mse: 4.9773e-04 - mae: 0.0136\n",
      "Epoch 47/180\n",
      "113/113 [==============================] - 155s 1s/step - loss: 4.9851e-04 - mse: 4.9851e-04 - mae: 0.0137\n",
      "Epoch 48/180\n",
      "113/113 [==============================] - 156s 1s/step - loss: 4.9934e-04 - mse: 4.9934e-04 - mae: 0.0137\n",
      "Epoch 49/180\n",
      "113/113 [==============================] - 153s 1s/step - loss: 4.9482e-04 - mse: 4.9482e-04 - mae: 0.0136\n",
      "Epoch 50/180\n",
      "113/113 [==============================] - 155s 1s/step - loss: 4.9760e-04 - mse: 4.9760e-04 - mae: 0.0136\n",
      "Epoch 51/180\n",
      "113/113 [==============================] - 160s 1s/step - loss: 4.9612e-04 - mse: 4.9612e-04 - mae: 0.0136\n",
      "Epoch 52/180\n",
      "113/113 [==============================] - 154s 1s/step - loss: 4.9700e-04 - mse: 4.9700e-04 - mae: 0.0136\n",
      "Epoch 53/180\n",
      "113/113 [==============================] - 155s 1s/step - loss: 4.9276e-04 - mse: 4.9276e-04 - mae: 0.0135\n",
      "Epoch 54/180\n",
      "113/113 [==============================] - 154s 1s/step - loss: 4.9099e-04 - mse: 4.9099e-04 - mae: 0.0135\n",
      "Epoch 55/180\n",
      "113/113 [==============================] - 156s 1s/step - loss: 4.9155e-04 - mse: 4.9155e-04 - mae: 0.0136\n",
      "Epoch 56/180\n",
      "113/113 [==============================] - 154s 1s/step - loss: 4.9313e-04 - mse: 4.9313e-04 - mae: 0.0135\n",
      "Epoch 57/180\n",
      "113/113 [==============================] - 154s 1s/step - loss: 4.8874e-04 - mse: 4.8874e-04 - mae: 0.0135\n",
      "Epoch 58/180\n",
      "113/113 [==============================] - 155s 1s/step - loss: 4.8737e-04 - mse: 4.8737e-04 - mae: 0.0134\n",
      "Epoch 59/180\n",
      "113/113 [==============================] - 153s 1s/step - loss: 4.8853e-04 - mse: 4.8853e-04 - mae: 0.0134\n",
      "Epoch 60/180\n",
      "113/113 [==============================] - 153s 1s/step - loss: 4.8785e-04 - mse: 4.8785e-04 - mae: 0.0135\n",
      "Epoch 61/180\n",
      "113/113 [==============================] - 155s 1s/step - loss: 4.8400e-04 - mse: 4.8400e-04 - mae: 0.0134\n",
      "Epoch 62/180\n",
      "113/113 [==============================] - 160s 1s/step - loss: 4.8444e-04 - mse: 4.8444e-04 - mae: 0.0133\n",
      "Epoch 63/180\n",
      "113/113 [==============================] - 157s 1s/step - loss: 4.8563e-04 - mse: 4.8563e-04 - mae: 0.0134\n",
      "Epoch 64/180\n",
      "113/113 [==============================] - 156s 1s/step - loss: 4.8115e-04 - mse: 4.8115e-04 - mae: 0.0133\n",
      "Epoch 65/180\n",
      "113/113 [==============================] - 155s 1s/step - loss: 4.8152e-04 - mse: 4.8152e-04 - mae: 0.0133\n",
      "Epoch 66/180\n",
      "113/113 [==============================] - 157s 1s/step - loss: 4.7928e-04 - mse: 4.7928e-04 - mae: 0.0133\n",
      "Epoch 67/180\n",
      "113/113 [==============================] - 154s 1s/step - loss: 4.7702e-04 - mse: 4.7702e-04 - mae: 0.0132\n",
      "Epoch 68/180\n",
      "113/113 [==============================] - 153s 1s/step - loss: 4.7914e-04 - mse: 4.7914e-04 - mae: 0.0132\n",
      "Epoch 69/180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 153s 1s/step - loss: 4.8090e-04 - mse: 4.8090e-04 - mae: 0.0133\n",
      "Epoch 70/180\n",
      "113/113 [==============================] - 155s 1s/step - loss: 4.8019e-04 - mse: 4.8019e-04 - mae: 0.0132\n",
      "Epoch 71/180\n",
      "113/113 [==============================] - 154s 1s/step - loss: 4.7339e-04 - mse: 4.7339e-04 - mae: 0.0132\n",
      "Epoch 72/180\n",
      "113/113 [==============================] - 153s 1s/step - loss: 4.7533e-04 - mse: 4.7533e-04 - mae: 0.0133\n",
      "Epoch 73/180\n",
      "113/113 [==============================] - 154s 1s/step - loss: 4.7207e-04 - mse: 4.7207e-04 - mae: 0.0132\n",
      "Epoch 74/180\n",
      "113/113 [==============================] - 158s 1s/step - loss: 4.6911e-04 - mse: 4.6911e-04 - mae: 0.0131\n",
      "Epoch 75/180\n",
      "113/113 [==============================] - 154s 1s/step - loss: 4.7680e-04 - mse: 4.7680e-04 - mae: 0.0133\n",
      "Epoch 76/180\n",
      "113/113 [==============================] - 155s 1s/step - loss: 4.7290e-04 - mse: 4.7290e-04 - mae: 0.0132\n",
      "Epoch 77/180\n",
      "113/113 [==============================] - 156s 1s/step - loss: 4.6737e-04 - mse: 4.6737e-04 - mae: 0.0131\n",
      "Epoch 78/180\n",
      "113/113 [==============================] - 157s 1s/step - loss: 4.6826e-04 - mse: 4.6826e-04 - mae: 0.0131\n",
      "Epoch 79/180\n",
      "113/113 [==============================] - 156s 1s/step - loss: 4.6893e-04 - mse: 4.6893e-04 - mae: 0.0131\n",
      "Epoch 80/180\n",
      "113/113 [==============================] - 155s 1s/step - loss: 4.6431e-04 - mse: 4.6431e-04 - mae: 0.0130\n",
      "Epoch 81/180\n",
      "113/113 [==============================] - 157s 1s/step - loss: 4.5709e-04 - mse: 4.5709e-04 - mae: 0.0129\n",
      "Epoch 82/180\n",
      "113/113 [==============================] - 153s 1s/step - loss: 4.6116e-04 - mse: 4.6116e-04 - mae: 0.0130\n",
      "Epoch 83/180\n",
      "113/113 [==============================] - 153s 1s/step - loss: 4.6659e-04 - mse: 4.6659e-04 - mae: 0.0130\n",
      "Epoch 84/180\n",
      "113/113 [==============================] - 154s 1s/step - loss: 4.5936e-04 - mse: 4.5936e-04 - mae: 0.0129\n",
      "Epoch 85/180\n",
      "113/113 [==============================] - 162s 1s/step - loss: 4.5835e-04 - mse: 4.5835e-04 - mae: 0.0130\n",
      "Epoch 86/180\n",
      "113/113 [==============================] - 156s 1s/step - loss: 4.5940e-04 - mse: 4.5940e-04 - mae: 0.0129\n",
      "Epoch 87/180\n",
      "113/113 [==============================] - 155s 1s/step - loss: 4.5734e-04 - mse: 4.5734e-04 - mae: 0.0130\n",
      "Epoch 88/180\n",
      "113/113 [==============================] - 154s 1s/step - loss: 4.5281e-04 - mse: 4.5281e-04 - mae: 0.0129\n",
      "Epoch 89/180\n",
      "113/113 [==============================] - 158s 1s/step - loss: 4.5370e-04 - mse: 4.5370e-04 - mae: 0.0129\n",
      "Epoch 90/180\n",
      "113/113 [==============================] - 158s 1s/step - loss: 4.7096e-04 - mse: 4.7096e-04 - mae: 0.0132\n",
      "Epoch 91/180\n",
      "113/113 [==============================] - 157s 1s/step - loss: 4.4892e-04 - mse: 4.4892e-04 - mae: 0.0128\n",
      "Epoch 92/180\n",
      "113/113 [==============================] - 154s 1s/step - loss: 4.5693e-04 - mse: 4.5693e-04 - mae: 0.0129\n",
      "Epoch 93/180\n",
      "113/113 [==============================] - 157s 1s/step - loss: 4.4755e-04 - mse: 4.4755e-04 - mae: 0.0128\n",
      "Epoch 94/180\n",
      "113/113 [==============================] - 154s 1s/step - loss: 4.5102e-04 - mse: 4.5102e-04 - mae: 0.0129\n",
      "Epoch 95/180\n",
      "113/113 [==============================] - 153s 1s/step - loss: 4.4648e-04 - mse: 4.4648e-04 - mae: 0.0128\n",
      "Epoch 96/180\n",
      "113/113 [==============================] - 154s 1s/step - loss: 4.4486e-04 - mse: 4.4486e-04 - mae: 0.0128\n",
      "Epoch 97/180\n",
      "113/113 [==============================] - 157s 1s/step - loss: 4.4752e-04 - mse: 4.4752e-04 - mae: 0.0128\n",
      "Epoch 98/180\n",
      "113/113 [==============================] - 155s 1s/step - loss: 4.3677e-04 - mse: 4.3677e-04 - mae: 0.0127\n",
      "Epoch 99/180\n",
      "113/113 [==============================] - 155s 1s/step - loss: 4.4124e-04 - mse: 4.4124e-04 - mae: 0.0128\n",
      "Epoch 100/180\n",
      "113/113 [==============================] - 157s 1s/step - loss: 4.5001e-04 - mse: 4.5001e-04 - mae: 0.0128\n",
      "Epoch 101/180\n",
      "113/113 [==============================] - 154s 1s/step - loss: 4.3870e-04 - mse: 4.3870e-04 - mae: 0.0127\n",
      "Epoch 102/180\n",
      "113/113 [==============================] - 153s 1s/step - loss: 4.5593e-04 - mse: 4.5593e-04 - mae: 0.0129\n",
      "Epoch 103/180\n",
      "113/113 [==============================] - 153s 1s/step - loss: 4.5705e-04 - mse: 4.5705e-04 - mae: 0.0129\n",
      "Epoch 104/180\n",
      "113/113 [==============================] - 155s 1s/step - loss: 4.4946e-04 - mse: 4.4946e-04 - mae: 0.0128\n",
      "Epoch 105/180\n",
      "113/113 [==============================] - 155s 1s/step - loss: 4.4501e-04 - mse: 4.4501e-04 - mae: 0.0128\n",
      "Epoch 106/180\n",
      "113/113 [==============================] - 154s 1s/step - loss: 4.5594e-04 - mse: 4.5594e-04 - mae: 0.0129\n",
      "Epoch 107/180\n",
      "113/113 [==============================] - 154s 1s/step - loss: 4.4459e-04 - mse: 4.4459e-04 - mae: 0.0128\n",
      "Epoch 108/180\n",
      "113/113 [==============================] - 161s 1s/step - loss: 4.5406e-04 - mse: 4.5406e-04 - mae: 0.0129\n",
      "Epoch 109/180\n",
      "113/113 [==============================] - 154s 1s/step - loss: 4.3964e-04 - mse: 4.3964e-04 - mae: 0.0127\n",
      "Epoch 110/180\n",
      "113/113 [==============================] - 153s 1s/step - loss: 4.3859e-04 - mse: 4.3859e-04 - mae: 0.0127\n",
      "Epoch 111/180\n",
      "113/113 [==============================] - 155s 1s/step - loss: 4.3516e-04 - mse: 4.3516e-04 - mae: 0.0126\n",
      "Epoch 112/180\n",
      "113/113 [==============================] - 161s 1s/step - loss: 4.4537e-04 - mse: 4.4537e-04 - mae: 0.0128\n",
      "Epoch 113/180\n",
      "113/113 [==============================] - 155s 1s/step - loss: 4.3225e-04 - mse: 4.3225e-04 - mae: 0.0127\n",
      "Epoch 114/180\n",
      "113/113 [==============================] - 155s 1s/step - loss: 4.3700e-04 - mse: 4.3700e-04 - mae: 0.0127\n",
      "Epoch 115/180\n",
      "113/113 [==============================] - 155s 1s/step - loss: 4.3552e-04 - mse: 4.3552e-04 - mae: 0.0127\n",
      "Epoch 116/180\n",
      "113/113 [==============================] - 158s 1s/step - loss: 4.1773e-04 - mse: 4.1773e-04 - mae: 0.0125\n",
      "Epoch 117/180\n",
      "113/113 [==============================] - 155s 1s/step - loss: 4.3917e-04 - mse: 4.3917e-04 - mae: 0.0126\n",
      "Epoch 118/180\n",
      "113/113 [==============================] - 154s 1s/step - loss: 4.2338e-04 - mse: 4.2338e-04 - mae: 0.0125\n",
      "Epoch 119/180\n",
      "113/113 [==============================] - 156s 1s/step - loss: 4.3566e-04 - mse: 4.3566e-04 - mae: 0.0127\n",
      "Epoch 120/180\n",
      "113/113 [==============================] - 160s 1s/step - loss: 4.2414e-04 - mse: 4.2414e-04 - mae: 0.0126\n",
      "Epoch 121/180\n",
      "113/113 [==============================] - 156s 1s/step - loss: 4.2172e-04 - mse: 4.2172e-04 - mae: 0.0126\n",
      "Epoch 122/180\n",
      "113/113 [==============================] - 160s 1s/step - loss: 4.1590e-04 - mse: 4.1590e-04 - mae: 0.0124\n",
      "Epoch 123/180\n",
      "113/113 [==============================] - 160s 1s/step - loss: 4.0755e-04 - mse: 4.0755e-04 - mae: 0.0124\n",
      "Epoch 124/180\n",
      "113/113 [==============================] - 160s 1s/step - loss: 4.2072e-04 - mse: 4.2072e-04 - mae: 0.0124\n",
      "Epoch 125/180\n",
      "113/113 [==============================] - 158s 1s/step - loss: 4.2819e-04 - mse: 4.2819e-04 - mae: 0.0127\n",
      "Epoch 126/180\n",
      "113/113 [==============================] - 160s 1s/step - loss: 4.2387e-04 - mse: 4.2387e-04 - mae: 0.0125\n",
      "Epoch 127/180\n",
      "113/113 [==============================] - 164s 1s/step - loss: 4.1256e-04 - mse: 4.1256e-04 - mae: 0.0124\n",
      "Epoch 128/180\n",
      "113/113 [==============================] - 159s 1s/step - loss: 4.0860e-04 - mse: 4.0860e-04 - mae: 0.0123\n",
      "Epoch 129/180\n",
      "113/113 [==============================] - 160s 1s/step - loss: 4.1353e-04 - mse: 4.1353e-04 - mae: 0.0124\n",
      "Epoch 130/180\n",
      "113/113 [==============================] - 160s 1s/step - loss: 4.1084e-04 - mse: 4.1084e-04 - mae: 0.0123\n",
      "Epoch 131/180\n",
      "113/113 [==============================] - 168s 1s/step - loss: 4.0029e-04 - mse: 4.0029e-04 - mae: 0.0123\n",
      "Epoch 132/180\n",
      "113/113 [==============================] - 159s 1s/step - loss: 4.0381e-04 - mse: 4.0381e-04 - mae: 0.0125\n",
      "Epoch 133/180\n",
      "113/113 [==============================] - 162s 1s/step - loss: 4.0931e-04 - mse: 4.0931e-04 - mae: 0.0123\n",
      "Epoch 134/180\n",
      "113/113 [==============================] - 162s 1s/step - loss: 4.1630e-04 - mse: 4.1630e-04 - mae: 0.0124\n",
      "Epoch 135/180\n",
      "113/113 [==============================] - 161s 1s/step - loss: 4.0904e-04 - mse: 4.0904e-04 - mae: 0.0123\n",
      "Epoch 136/180\n",
      "113/113 [==============================] - 163s 1s/step - loss: 3.9754e-04 - mse: 3.9754e-04 - mae: 0.0121\n",
      "Epoch 137/180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 161s 1s/step - loss: 3.9379e-04 - mse: 3.9379e-04 - mae: 0.0121\n",
      "Epoch 138/180\n",
      "113/113 [==============================] - 161s 1s/step - loss: 3.9983e-04 - mse: 3.9983e-04 - mae: 0.0123\n",
      "Epoch 139/180\n",
      "113/113 [==============================] - 159s 1s/step - loss: 3.9568e-04 - mse: 3.9568e-04 - mae: 0.0122\n",
      "Epoch 140/180\n",
      "113/113 [==============================] - 161s 1s/step - loss: 3.8350e-04 - mse: 3.8350e-04 - mae: 0.0120\n",
      "Epoch 141/180\n",
      "113/113 [==============================] - 159s 1s/step - loss: 3.8688e-04 - mse: 3.8688e-04 - mae: 0.0122\n",
      "Epoch 142/180\n",
      "113/113 [==============================] - 162s 1s/step - loss: 3.9538e-04 - mse: 3.9538e-04 - mae: 0.0122\n",
      "Epoch 143/180\n",
      "113/113 [==============================] - 156s 1s/step - loss: 3.8740e-04 - mse: 3.8740e-04 - mae: 0.0123\n",
      "Epoch 144/180\n",
      "113/113 [==============================] - 157s 1s/step - loss: 3.9664e-04 - mse: 3.9664e-04 - mae: 0.0122\n",
      "Epoch 145/180\n",
      "113/113 [==============================] - 160s 1s/step - loss: 3.7036e-04 - mse: 3.7036e-04 - mae: 0.0121\n",
      "Epoch 146/180\n",
      "113/113 [==============================] - 164s 1s/step - loss: 3.6329e-04 - mse: 3.6329e-04 - mae: 0.0121\n",
      "Epoch 147/180\n",
      "113/113 [==============================] - 159s 1s/step - loss: 3.7397e-04 - mse: 3.7397e-04 - mae: 0.0121\n",
      "Epoch 148/180\n",
      "113/113 [==============================] - 158s 1s/step - loss: 3.7558e-04 - mse: 3.7558e-04 - mae: 0.0120\n",
      "Epoch 149/180\n",
      "113/113 [==============================] - 160s 1s/step - loss: 3.6747e-04 - mse: 3.6747e-04 - mae: 0.0120\n",
      "Epoch 150/180\n",
      "113/113 [==============================] - 157s 1s/step - loss: 3.6627e-04 - mse: 3.6627e-04 - mae: 0.0119\n",
      "Epoch 151/180\n",
      "113/113 [==============================] - 157s 1s/step - loss: 3.5776e-04 - mse: 3.5776e-04 - mae: 0.0120\n",
      "Epoch 152/180\n",
      "113/113 [==============================] - 159s 1s/step - loss: 3.4032e-04 - mse: 3.4032e-04 - mae: 0.0119\n",
      "Epoch 153/180\n",
      "113/113 [==============================] - 163s 1s/step - loss: 3.6561e-04 - mse: 3.6561e-04 - mae: 0.0120\n",
      "Epoch 154/180\n",
      "113/113 [==============================] - 156s 1s/step - loss: 3.5743e-04 - mse: 3.5743e-04 - mae: 0.0119\n",
      "Epoch 155/180\n",
      "113/113 [==============================] - 157s 1s/step - loss: 3.6953e-04 - mse: 3.6953e-04 - mae: 0.0119\n",
      "Epoch 156/180\n",
      "113/113 [==============================] - 157s 1s/step - loss: 3.8068e-04 - mse: 3.8068e-04 - mae: 0.0119\n",
      "Epoch 157/180\n",
      "113/113 [==============================] - 159s 1s/step - loss: 3.4667e-04 - mse: 3.4667e-04 - mae: 0.0119\n",
      "Epoch 158/180\n",
      "113/113 [==============================] - 157s 1s/step - loss: 3.4958e-04 - mse: 3.4958e-04 - mae: 0.0117\n",
      "Epoch 159/180\n",
      "113/113 [==============================] - 157s 1s/step - loss: 3.5971e-04 - mse: 3.5971e-04 - mae: 0.0118\n",
      "Epoch 160/180\n",
      "113/113 [==============================] - 157s 1s/step - loss: 3.1646e-04 - mse: 3.1646e-04 - mae: 0.0117\n",
      "Epoch 161/180\n",
      "113/113 [==============================] - 159s 1s/step - loss: 3.2437e-04 - mse: 3.2437e-04 - mae: 0.0119\n",
      "Epoch 162/180\n",
      "113/113 [==============================] - 158s 1s/step - loss: 3.7051e-04 - mse: 3.7051e-04 - mae: 0.0119\n",
      "Epoch 163/180\n",
      "113/113 [==============================] - 158s 1s/step - loss: 3.4295e-04 - mse: 3.4295e-04 - mae: 0.0116\n",
      "Epoch 164/180\n",
      "113/113 [==============================] - 165s 1s/step - loss: 3.5265e-04 - mse: 3.5265e-04 - mae: 0.0116\n",
      "Epoch 165/180\n",
      "113/113 [==============================] - 158s 1s/step - loss: 3.1116e-04 - mse: 3.1116e-04 - mae: 0.0116\n",
      "Epoch 166/180\n",
      "113/113 [==============================] - 157s 1s/step - loss: 3.4239e-04 - mse: 3.4239e-04 - mae: 0.0116\n",
      "Epoch 167/180\n",
      "113/113 [==============================] - 158s 1s/step - loss: 3.1309e-04 - mse: 3.1309e-04 - mae: 0.0115\n",
      "Epoch 168/180\n",
      "113/113 [==============================] - 160s 1s/step - loss: 3.0547e-04 - mse: 3.0547e-04 - mae: 0.0115\n",
      "Epoch 169/180\n",
      "113/113 [==============================] - 159s 1s/step - loss: 3.3075e-04 - mse: 3.3075e-04 - mae: 0.0118\n",
      "Epoch 170/180\n",
      "113/113 [==============================] - 158s 1s/step - loss: 3.0540e-04 - mse: 3.0540e-04 - mae: 0.0115\n",
      "Epoch 171/180\n",
      "113/113 [==============================] - 159s 1s/step - loss: 3.1163e-04 - mse: 3.1163e-04 - mae: 0.0116\n",
      "Epoch 172/180\n",
      "113/113 [==============================] - 161s 1s/step - loss: 2.9413e-04 - mse: 2.9413e-04 - mae: 0.0115\n",
      "Epoch 173/180\n",
      "113/113 [==============================] - 157s 1s/step - loss: 2.8685e-04 - mse: 2.8685e-04 - mae: 0.0114\n",
      "Epoch 174/180\n",
      "113/113 [==============================] - 158s 1s/step - loss: 2.8669e-04 - mse: 2.8669e-04 - mae: 0.0115\n",
      "Epoch 175/180\n",
      "113/113 [==============================] - 161s 1s/step - loss: 3.4345e-04 - mse: 3.4345e-04 - mae: 0.0116\n",
      "Epoch 176/180\n",
      "113/113 [==============================] - 162s 1s/step - loss: 2.9866e-04 - mse: 2.9866e-04 - mae: 0.0114\n",
      "Epoch 177/180\n",
      "113/113 [==============================] - 158s 1s/step - loss: 2.9427e-04 - mse: 2.9427e-04 - mae: 0.0113\n",
      "Epoch 178/180\n",
      "113/113 [==============================] - 158s 1s/step - loss: 2.8972e-04 - mse: 2.8972e-04 - mae: 0.0115\n",
      "Epoch 179/180\n",
      "113/113 [==============================] - 161s 1s/step - loss: 3.1567e-04 - mse: 3.1567e-04 - mae: 0.0115\n",
      "Epoch 180/180\n",
      "113/113 [==============================] - 157s 1s/step - loss: 3.1786e-04 - mse: 3.1786e-04 - mae: 0.0113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maria\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3997: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n",
      "C:\\Users\\maria\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/180\n",
      "90/90 [==============================] - 158s 2s/step - loss: 0.1700 - mse: 0.1700 - mae: 0.3021\n",
      "Epoch 2/180\n",
      "90/90 [==============================] - 160s 2s/step - loss: 0.0300 - mse: 0.0300 - mae: 0.1348\n",
      "Epoch 3/180\n",
      "90/90 [==============================] - 167s 2s/step - loss: 0.0076 - mse: 0.0076 - mae: 0.0678\n",
      "Epoch 4/180\n",
      "90/90 [==============================] - 161s 2s/step - loss: 0.0026 - mse: 0.0026 - mae: 0.0384\n",
      "Epoch 5/180\n",
      "90/90 [==============================] - 162s 2s/step - loss: 0.0013 - mse: 0.0013 - mae: 0.0256\n",
      "Epoch 6/180\n",
      "90/90 [==============================] - 167s 2s/step - loss: 9.1526e-04 - mse: 9.1526e-04 - mae: 0.0206\n",
      "Epoch 7/180\n",
      "90/90 [==============================] - 166s 2s/step - loss: 8.2215e-04 - mse: 8.2215e-04 - mae: 0.0190\n",
      "Epoch 8/180\n",
      "90/90 [==============================] - 168s 2s/step - loss: 7.6234e-04 - mse: 7.6234e-04 - mae: 0.0180\n",
      "Epoch 9/180\n",
      "90/90 [==============================] - 163s 2s/step - loss: 7.2613e-04 - mse: 7.2613e-04 - mae: 0.0174\n",
      "Epoch 10/180\n",
      "90/90 [==============================] - 165s 2s/step - loss: 7.0425e-04 - mse: 7.0425e-04 - mae: 0.0169\n",
      "Epoch 11/180\n",
      "90/90 [==============================] - 168s 2s/step - loss: 6.7619e-04 - mse: 6.7619e-04 - mae: 0.0165\n",
      "Epoch 12/180\n",
      "90/90 [==============================] - 165s 2s/step - loss: 6.5175e-04 - mse: 6.5175e-04 - mae: 0.0160\n",
      "Epoch 13/180\n",
      "90/90 [==============================] - 168s 2s/step - loss: 6.4755e-04 - mse: 6.4755e-04 - mae: 0.0159\n",
      "Epoch 14/180\n",
      "90/90 [==============================] - 166s 2s/step - loss: 6.1428e-04 - mse: 6.1428e-04 - mae: 0.0153\n",
      "Epoch 15/180\n",
      "90/90 [==============================] - 163s 2s/step - loss: 6.0331e-04 - mse: 6.0331e-04 - mae: 0.0152\n",
      "Epoch 16/180\n",
      "90/90 [==============================] - 164s 2s/step - loss: 5.8355e-04 - mse: 5.8355e-04 - mae: 0.0149\n",
      "Epoch 17/180\n",
      "90/90 [==============================] - 176s 2s/step - loss: 5.7618e-04 - mse: 5.7618e-04 - mae: 0.0147\n",
      "Epoch 18/180\n",
      "90/90 [==============================] - 166s 2s/step - loss: 5.6224e-04 - mse: 5.6224e-04 - mae: 0.0145\n",
      "Epoch 19/180\n",
      "90/90 [==============================] - 162s 2s/step - loss: 5.6252e-04 - mse: 5.6252e-04 - mae: 0.0144\n",
      "Epoch 20/180\n",
      "90/90 [==============================] - 171s 2s/step - loss: 5.5561e-04 - mse: 5.5561e-04 - mae: 0.0143\n",
      "Epoch 21/180\n",
      "90/90 [==============================] - 169s 2s/step - loss: 5.5774e-04 - mse: 5.5774e-04 - mae: 0.0143\n",
      "Epoch 22/180\n",
      "90/90 [==============================] - 190s 2s/step - loss: 5.5071e-04 - mse: 5.5071e-04 - mae: 0.0142\n",
      "Epoch 23/180\n",
      "90/90 [==============================] - 175s 2s/step - loss: 5.4718e-04 - mse: 5.4718e-04 - mae: 0.0142\n",
      "Epoch 24/180\n",
      "90/90 [==============================] - 169s 2s/step - loss: 5.4367e-04 - mse: 5.4367e-04 - mae: 0.0140\n",
      "Epoch 25/180\n",
      "90/90 [==============================] - 162s 2s/step - loss: 5.4390e-04 - mse: 5.4390e-04 - mae: 0.0141\n",
      "Epoch 26/180\n",
      "90/90 [==============================] - 157s 2s/step - loss: 5.3918e-04 - mse: 5.3918e-04 - mae: 0.0140\n",
      "Epoch 27/180\n",
      "90/90 [==============================] - 170s 2s/step - loss: 5.3920e-04 - mse: 5.3920e-04 - mae: 0.0140\n",
      "Epoch 28/180\n",
      "90/90 [==============================] - 182s 2s/step - loss: 5.4090e-04 - mse: 5.4090e-04 - mae: 0.0139\n",
      "Epoch 29/180\n",
      "90/90 [==============================] - 179s 2s/step - loss: 5.3394e-04 - mse: 5.3394e-04 - mae: 0.0139\n",
      "Epoch 30/180\n",
      "90/90 [==============================] - 172s 2s/step - loss: 5.3760e-04 - mse: 5.3760e-04 - mae: 0.0139\n",
      "Epoch 31/180\n",
      "90/90 [==============================] - 171s 2s/step - loss: 5.3760e-04 - mse: 5.3760e-04 - mae: 0.0139\n",
      "Epoch 32/180\n",
      "90/90 [==============================] - 190s 2s/step - loss: 5.3396e-04 - mse: 5.3396e-04 - mae: 0.0139\n",
      "Epoch 33/180\n",
      "90/90 [==============================] - 179s 2s/step - loss: 5.3403e-04 - mse: 5.3403e-04 - mae: 0.0139\n",
      "Epoch 34/180\n",
      "90/90 [==============================] - 176s 2s/step - loss: 5.2965e-04 - mse: 5.2965e-04 - mae: 0.0137\n",
      "Epoch 35/180\n",
      "90/90 [==============================] - 175s 2s/step - loss: 5.2852e-04 - mse: 5.2852e-04 - mae: 0.0137\n",
      "Epoch 36/180\n",
      "90/90 [==============================] - 168s 2s/step - loss: 5.2886e-04 - mse: 5.2886e-04 - mae: 0.0138\n",
      "Epoch 37/180\n",
      "90/90 [==============================] - 182s 2s/step - loss: 5.2814e-04 - mse: 5.2814e-04 - mae: 0.0137\n",
      "Epoch 38/180\n",
      "90/90 [==============================] - 178s 2s/step - loss: 5.2250e-04 - mse: 5.2250e-04 - mae: 0.0137\n",
      "Epoch 39/180\n",
      "90/90 [==============================] - 173s 2s/step - loss: 5.2480e-04 - mse: 5.2480e-04 - mae: 0.0136\n",
      "Epoch 40/180\n",
      "90/90 [==============================] - 171s 2s/step - loss: 5.2051e-04 - mse: 5.2051e-04 - mae: 0.0136\n",
      "Epoch 41/180\n",
      "90/90 [==============================] - 163s 2s/step - loss: 5.2118e-04 - mse: 5.2118e-04 - mae: 0.0136\n",
      "Epoch 42/180\n",
      "90/90 [==============================] - 171s 2s/step - loss: 5.1696e-04 - mse: 5.1696e-04 - mae: 0.0135\n",
      "Epoch 43/180\n",
      "90/90 [==============================] - 166s 2s/step - loss: 5.1435e-04 - mse: 5.1435e-04 - mae: 0.0135\n",
      "Epoch 44/180\n",
      "90/90 [==============================] - 169s 2s/step - loss: 5.1460e-04 - mse: 5.1460e-04 - mae: 0.0135\n",
      "Epoch 45/180\n",
      "86/90 [===========================>..] - ETA: 7s - loss: 5.1423e-04 - mse: 5.1423e-04 - mae: 0.0134"
     ]
    }
   ],
   "source": [
    "# function to split data into correct shape for RNN\n",
    "def split_data(X, y, steps):\n",
    "    X_, y_ = list(), list()\n",
    "    for i in range(steps, len(y)):\n",
    "        X_.append(X[i - steps : i, :])\n",
    "        y_.append(y[i]) \n",
    "    return np.array(X_), np.array(y_)\n",
    "\n",
    "# function to cut data set so it can be divisible by the batch_size\n",
    "def cut_data(data, batch_size):\n",
    "     # see if it is divisivel\n",
    "    condition = data.shape[0] % batch_size\n",
    "    if condition == 0:\n",
    "        return data\n",
    "    else:\n",
    "        return data[: -condition]\n",
    "\n",
    "def regressor_tunning(kernel_initializer = 'he_uniform',\n",
    "                      bias_initializer = initializers.Ones()):\n",
    "    model = Sequential()\n",
    "    if n_hidden == 0:\n",
    "        model.add(LSTM(units = units,                    \n",
    "                       input_shape = (steps, features_num), \n",
    "                       kernel_initializer = kernel_initializer,\n",
    "                       bias_initializer = bias_initializer))\n",
    "        model.add(LeakyReLU(alpha = 0.2))\n",
    "        model.add(Dropout(0.2))\n",
    "    else:\n",
    "        model.add(LSTM(units = units,                    \n",
    "                       input_shape = (steps, features_num), \n",
    "                       return_sequences = True,\n",
    "                       kernel_initializer = kernel_initializer,\n",
    "                       bias_initializer = bias_initializer))\n",
    "        model.add(LeakyReLU(alpha = 0.2))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(units = units, \n",
    "                       input_shape = (steps, features_num), \n",
    "                       kernel_initializer = kernel_initializer,\n",
    "                       bias_initializer = bias_initializer))\n",
    "        model.add(LeakyReLU(alpha = 0.2))\n",
    "        model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    optimizer = optimizers.RMSprop()\n",
    "    model.compile(loss = 'mse', metrics = ['mse', 'mae'], optimizer = optimizer)\n",
    "    return model\n",
    "  \n",
    "# LOOP STARTS\n",
    "for i in date:\n",
    "    start_time = time.time()\n",
    "    # data\n",
    "    data = data_full.loc[data_full.index > i, :]\n",
    "\n",
    "    # reset index\n",
    "    data.reset_index(inplace = True)\n",
    "    data.drop('index', axis = 1, inplace = True)\n",
    "\n",
    "    # fill nan values in the whole data set\n",
    "    data.fillna(data.mean(), inplace = True)\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # divide data into train and test \n",
    "    data_train, data_test = train_test_split(\n",
    "             data, test_size = 0.15, shuffle=False)  \n",
    "    \n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    # data scaling  (including offer (y))\n",
    "    sc_X = MinMaxScaler()\n",
    "    data_train = sc_X.fit_transform(data_train)\n",
    "    data_test = sc_X.transform(data_test)\n",
    "    \n",
    "    # divide features and labels\n",
    "    X_train = data_train[:, 0:14] \n",
    "    y_train = data_train[:, -1]\n",
    "    X_test = data_test[:, 0:14] \n",
    "    y_test = data_test[:, -1] \n",
    "\n",
    "    # divide data into train and test \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "             X_train, y_train, test_size = 0.10, shuffle=False)\n",
    "\n",
    "    # put data into correct shape\n",
    "    X_train, y_train = split_data(X_train, y_train, steps)\n",
    "    X_test, y_test = split_data(X_test, y_test, steps)\n",
    "    X_val, y_val = split_data(X_val, y_val, steps)\n",
    "\n",
    "    X_train = cut_data(X_train, batch_size)\n",
    "    y_train = cut_data(y_train, batch_size)\n",
    "    X_test = cut_data(X_test, batch_size)\n",
    "    y_test = cut_data(y_test, batch_size)\n",
    "    X_val = cut_data(X_val, batch_size)\n",
    "    y_val = cut_data(y_val, batch_size)\n",
    "\n",
    "    model = regressor_tunning()\n",
    "    \n",
    "    # fitting the LSTM to the training set\n",
    "    history = model.fit(X_train,\n",
    "                        y_train, \n",
    "                        batch_size = batch_size, \n",
    "                        epochs = epochs,\n",
    "                        shuffle = False)\n",
    "                        #validation_data = (X_val, y_val))\n",
    "    \n",
    "    model.reset_states()\n",
    "    \n",
    "    # make new predicitons with test set\n",
    "    y_pred = model.predict(X_test, batch_size = batch_size)\n",
    "    \n",
    "    # prices col = 15 (inverso should not be used as scalling was made with the whole data set)\n",
    "    y_pred = (y_pred * sc_X.data_range_[14]) + (sc_X.data_min_[14])\n",
    "    y_test = (y_test * sc_X.data_range_[14]) + (sc_X.data_min_[14])\n",
    "\n",
    "    # smal adjustment\n",
    "    y_test = pd.Series(y_test)\n",
    "    y_test.replace(0, 0.0001,inplace = True)\n",
    "    \n",
    "    y_pred_list.append(y_pred)\n",
    "    \n",
    "    from sklearn.metrics import mean_squared_error as mse\n",
    "    from sklearn.metrics import mean_absolute_error as mae\n",
    "\n",
    "    rmse_error = mse(y_test, y_pred, squared = False)\n",
    "    mae_error = mae(y_test, y_pred)\n",
    "    \n",
    "    rmse_gen.append(rmse_error)\n",
    "    mae_gen.append(mae_error)\n",
    "    \n",
    "    # =============================================================================\n",
    "    # Metrics evaluation on spike regions\n",
    "    # =============================================================================\n",
    "    \n",
    "    # Need to process data with spike occurences the same way as features\n",
    "    data = pd.read_csv('Spike_binary_1std.csv', index_col = 0)\n",
    "\n",
    "    # set predictive window according with tuning best results\n",
    "    data = data.loc[data.index > i, :]\n",
    "\n",
    "    # make sure shaded area will correspond to values outputed by LSTM\n",
    "    data.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    # fill_nan is already made - so lets split data into test and train\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # divide data into train and test \n",
    "    shade_train, shade_test = train_test_split(\n",
    "             data, test_size = 0.15, shuffle = False)\n",
    "\n",
    "    # reset index of testing data\n",
    "    shade_test.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    # function to split data into correct shape for RNN\n",
    "    def split_data_shade(shade_test, steps):\n",
    "        y_spike_occ = list()\n",
    "        upper_lim = list()\n",
    "        lower_lim = list()\n",
    "        for i in range(steps, len(shade_test.index)):\n",
    "            y_spike_occ.append(shade_test['spike_occurance'][i])\n",
    "            upper_lim.append(shade_test['spike_upperlim'][i])\n",
    "            lower_lim.append(shade_test['spike_lowerlim'][i])\n",
    "        return np.array(y_spike_occ), np.array(upper_lim), np.array(lower_lim)\n",
    "\n",
    "    # function to cut data set so it can be divisible by the batch_size\n",
    "    def cut_data_shade(data, batch_size):\n",
    "         # see if it is divisivel\n",
    "        condition = data.shape[0] % batch_size\n",
    "        if condition == 0:\n",
    "            return data\n",
    "        else:\n",
    "            return data[: -condition]\n",
    "    \n",
    "    # shape y_spike_occ for the right size to compare results in normal and spike regions\n",
    "    y_spike_occ, spike_upperlim, spike_lowerlim = split_data_shade(shade_test, steps)\n",
    "    y_spike_occ = cut_data_shade(y_spike_occ, batch_size)\n",
    "\n",
    "    # continue\n",
    "    \n",
    "    # select y_pred and y_test only for regions with spikes\n",
    "    y_test_spike = (y_test.T * y_spike_occ).T\n",
    "    y_pred_spike = (y_pred.T * y_spike_occ).T\n",
    "    y_test_spike = y_test_spike[y_test_spike != 0]\n",
    "    y_pred_spike = y_pred_spike[y_pred_spike != 0]\n",
    "    \n",
    "    # calculate metric\n",
    "    rmse_spike = mse(y_test_spike, y_pred_spike, squared = False)\n",
    "    mae_spike = mae(y_test_spike, y_pred_spike)\n",
    "    \n",
    "    rmse_spi.append(rmse_spike)\n",
    "    mae_spi.append(mae_spike)\n",
    "    \n",
    "    # =============================================================================\n",
    "    # Metric evaluation on normal regions\n",
    "    # =============================================================================\n",
    "    \n",
    "    # inverse y_spike_occ so the only normal occurences are chosen\n",
    "    y_normal_occ = (y_spike_occ - 1) * (-1)\n",
    "    \n",
    "    # sanity check\n",
    "    y_normal_occ.sum() + y_spike_occ.sum() # gives the correct total \n",
    "    \n",
    "    # select y_pred and y_test only for normal regions\n",
    "    y_test_normal = (y_test.T * y_normal_occ).T\n",
    "    y_pred_normal = (y_pred.T * y_normal_occ).T\n",
    "    y_test_normal = y_test_normal[y_test_normal != 0.00]\n",
    "    y_pred_normal = y_pred_normal[y_pred_normal != 0.00]\n",
    "    \n",
    "    # calculate metric\n",
    "    rmse_normal = mse(y_test_normal, y_pred_normal, squared = False)\n",
    "    mae_normal = mae(y_test_normal, y_pred_normal)\n",
    "    \n",
    "    rmse_nor.append(rmse_normal)\n",
    "    mae_nor.append(mae_normal)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    time_count.append(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({                        \n",
    "                        'rmse_general': rmse_gen, \n",
    "                 \n",
    "                        'mae_general': mae_gen,\n",
    "                        \n",
    "                        'rmse_spike': rmse_spi,\n",
    "                 \n",
    "                        'mae_spike': mae_spi,\n",
    "                        \n",
    "                        'rmse_normal': rmse_nor,\n",
    "                    \n",
    "                        'mae_normal': mae_nor,\n",
    "    \n",
    "                        'time': time_count})\n",
    "\n",
    "#y_pred = pd.DataFrame({'dates': date,\n",
    "#                       'Predicitons': y_pred_list})\n",
    "\n",
    "#y_pred.to_csv('Pedictions_LSTM_5_prediction_window.csv')\n",
    "results.to_csv('Results_LSTM_Prediction_window.csv')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_min(s):\n",
    "    '''\n",
    "    highlight the maximum in a Series yellow.\n",
    "    '''\n",
    "    is_max = s == s.min()\n",
    "    return ['background-color: yellow' if v else '' for v in is_max]\n",
    "\n",
    "results.style.apply(highlight_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "fontsize = 13\n",
    "\n",
    "date_labels =   ['12', \n",
    "                 '10', \n",
    "                 '8',\n",
    "                 '6', \n",
    "                 '4', \n",
    "                 '2']\n",
    "\n",
    "plt.figure(figsize=(10,3.5))\n",
    "plt.minorticks_on()\n",
    "plt.grid(which='major', linestyle='-', linewidth='0.5')\n",
    "plt.grid(which='minor', linestyle=':', linewidth='0.5')\n",
    "plt.title('ANN: Averaged RMSE for different prediction windows', fontsize = fontsize + 2)\n",
    "plt.plot([0,1,2,3,4,5], rmse_gen, label = 'On the whole test set')\n",
    "plt.plot([0,1,2,3,4,5], rmse_spi, label = 'Spike regions')\n",
    "plt.plot([0,1,2,3,4,5], rmse_nor, label = 'Non-spike regions')\n",
    "plt.legend(loc = 'upper right', fontsize = fontsize - 1)\n",
    "plt.ylabel('RMSE (/MWh)', fontsize = fontsize)\n",
    "plt.xlabel('Predictive window (in months)', fontsize = fontsize)\n",
    "plt.xticks([0,1,2,3,4,5], list(np.arange(2, 14, 2))[::-1], fontsize = fontsize)\n",
    "plt.yticks(fontsize = fontsize)\n",
    "plt.xlim(0, 5)\n",
    "plt.tight_layout()\n",
    "plt.savefig('RMSE_predictive_window.png')\n",
    "\n",
    "plt.figure(figsize=(10,3.5))\n",
    "plt.minorticks_on()\n",
    "plt.grid(which='major', linestyle='-', linewidth='0.5')\n",
    "plt.grid(which='minor', linestyle=':', linewidth='0.5')\n",
    "plt.title('Random Forest: Averaged MAE for different prediction windows', fontsize = fontsize + 2)\n",
    "plt.plot([0,1,2,3,4,5], rmse_gen, label = 'On the whole test set')\n",
    "plt.plot([0,1,2,3,4,5], rmse_spi, label = 'Spike regions')\n",
    "plt.plot([0,1,2,3,4,5], rmse_nor, label = 'Non-spike regions')\n",
    "plt.legend(loc = 'upper right', fontsize = fontsize - 1)\n",
    "plt.ylabel('MAE (/MWh)', fontsize = fontsize)\n",
    "plt.xlabel('Predictive window (in months)', fontsize = fontsize)\n",
    "plt.xticks([0,1,2,3,4,5], list(np.arange(2, 14, 2))[::-1], fontsize = fontsize)\n",
    "plt.yticks(fontsize = fontsize)\n",
    "plt.xlim(0, 5)\n",
    "plt.tight_layout()\n",
    "plt.savefig('MAE_predictive_window.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "plt.plot(y_pred)\n",
    "plt.plot(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
