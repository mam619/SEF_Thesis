{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection of LSTM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\maria\\anaconda3\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from pandas) (1.18.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from python-dateutil>=2.6.1->pandas) (1.14.0)\n",
      "Requirement already satisfied: sklearn in c:\\users\\maria\\anaconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\maria\\anaconda3\\lib\\site-packages (from sklearn) (0.22.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (0.14.1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.18.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\maria\\anaconda3\\lib\\site-packages (3.1.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.11 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from matplotlib) (1.18.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from matplotlib) (2.4.6)\n",
      "Requirement already satisfied: six in c:\\users\\maria\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.14.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\maria\\anaconda3\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib) (45.2.0.post20200210)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install sklearn\n",
    "!pip install matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd;\n",
    "import numpy as np;\n",
    "import sklearn\n",
    "import time\n",
    "\n",
    "# empty list to append metric values\n",
    "mae_gen = []\n",
    "mae_nor = []\n",
    "mae_spi = []\n",
    "rmse_gen = []\n",
    "rmse_nor = []\n",
    "rmse_spi = []\n",
    "time_count = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import keras libraries, packages and data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LeakyReLU\n",
    "from keras import initializers\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# import data\n",
    "data_full = pd.read_csv('Data_set_1_smaller.csv', index_col = 0)\n",
    "\n",
    "data_full = data_full.loc[data_full.index > 2018090000, :]\n",
    "\n",
    "# reset index\n",
    "data_full.reset_index(inplace = True)\n",
    "data_full.drop('index', axis = 1, inplace = True)\n",
    "\n",
    "# fill nan values\n",
    "data_full.fillna(data.mean(), inplace = True)\n",
    "\n",
    "# parameters\n",
    "features_num = 15\n",
    "steps = 96\n",
    "n_hidden = 1\n",
    "units = 150\n",
    "batch_size = 96\n",
    "epochs = 180"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to split data into correct shape for RNN\n",
    "def split_data(X, y, steps):\n",
    "    X_, y_ = list(), list()\n",
    "    for i in range(steps, len(y)):\n",
    "        X_.append(X[i - steps : i, :])\n",
    "        y_.append(y[i]) \n",
    "    return np.array(X_), np.array(y_)\n",
    "\n",
    "# function to cut data set so it can be divisible by the batch_size\n",
    "def cut_data(data, batch_size):\n",
    "     # see if it is divisivel\n",
    "    condition = data.shape[0] % batch_size\n",
    "    if condition == 0:\n",
    "        return data\n",
    "    else:\n",
    "        return data[: -condition]\n",
    "\n",
    "# design the LSTM\n",
    "def regressor_tunning(features_num = features_num, bias_initializer = initializers.Ones() , kernel_initializer = 'he_uniform'):\n",
    "    model = Sequential()\n",
    "    if n_hidden == 0:\n",
    "        model.add(LSTM(units = units,                    \n",
    "                       batch_input_shape = (batch_size, steps, features_num), \n",
    "                       stateful = True,\n",
    "                       kernel_initializer = kernel_initializer,\n",
    "                       bias_initializer = bias_initializer))\n",
    "        model.add(LeakyReLU(alpha = 0.2))\n",
    "        model.add(Dropout(0.2))\n",
    "    else:\n",
    "        model.add(LSTM(units = units,                    \n",
    "                       batch_input_shape = (batch_size, steps, features_num), \n",
    "                       stateful = True,\n",
    "                       return_sequences = True,\n",
    "                       kernel_initializer = kernel_initializer,\n",
    "                       bias_initializer = bias_initializer))\n",
    "        model.add(LeakyReLU(alpha = 0.2))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(units = units, \n",
    "                       batch_input_shape = (batch_size, steps, features_num), \n",
    "                       stateful = True,\n",
    "                       kernel_initializer = kernel_initializer,\n",
    "                       bias_initializer = bias_initializer))\n",
    "        model.add(LeakyReLU(alpha = 0.2))\n",
    "        model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    optimizer = optimizers.RMSprop()\n",
    "    model.compile(loss = 'mse', metrics = ['mse', 'mae'], optimizer = optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First prediction with one feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/180\n",
      "65/65 [==============================] - 34s 516ms/step - loss: 0.1580 - mse: 0.1580 - mae: 0.3146 - val_loss: 0.0046 - val_mse: 0.0046 - val_mae: 0.0609\n",
      "Epoch 2/180\n",
      "65/65 [==============================] - 32s 500ms/step - loss: 0.1074 - mse: 0.1074 - mae: 0.2614 - val_loss: 0.0031 - val_mse: 0.0031 - val_mae: 0.0452\n",
      "Epoch 3/180\n",
      "65/65 [==============================] - 30s 460ms/step - loss: 0.0680 - mse: 0.0680 - mae: 0.2076 - val_loss: 0.0023 - val_mse: 0.0023 - val_mae: 0.0364\n",
      "Epoch 4/180\n",
      "65/65 [==============================] - 25s 382ms/step - loss: 0.0496 - mse: 0.0496 - mae: 0.1773 - val_loss: 0.0014 - val_mse: 0.0014 - val_mae: 0.0290\n",
      "Epoch 5/180\n",
      "65/65 [==============================] - 33s 512ms/step - loss: 0.0324 - mse: 0.0324 - mae: 0.1421 - val_loss: 0.0012 - val_mse: 0.0012 - val_mae: 0.0250\n",
      "Epoch 6/180\n",
      "65/65 [==============================] - 33s 503ms/step - loss: 0.0224 - mse: 0.0224 - mae: 0.1188 - val_loss: 0.0021 - val_mse: 0.0021 - val_mae: 0.0316\n",
      "Epoch 7/180\n",
      "65/65 [==============================] - 36s 548ms/step - loss: 0.0156 - mse: 0.0156 - mae: 0.0991 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0197\n",
      "Epoch 8/180\n",
      "65/65 [==============================] - 63s 975ms/step - loss: 0.0113 - mse: 0.0113 - mae: 0.0842 - val_loss: 0.0014 - val_mse: 0.0014 - val_mae: 0.0206\n",
      "Epoch 9/180\n",
      "65/65 [==============================] - 63s 971ms/step - loss: 0.0083 - mse: 0.0083 - mae: 0.0716 - val_loss: 0.0012 - val_mse: 0.0012 - val_mae: 0.0165\n",
      "Epoch 10/180\n",
      "65/65 [==============================] - 63s 975ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0630 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0222\n",
      "Epoch 11/180\n",
      "65/65 [==============================] - 34s 516ms/step - loss: 0.0053 - mse: 0.0053 - mae: 0.0563 - val_loss: 0.0016 - val_mse: 0.0016 - val_mae: 0.0237\n",
      "Epoch 12/180\n",
      "65/65 [==============================] - 28s 429ms/step - loss: 0.0041 - mse: 0.0041 - mae: 0.0493 - val_loss: 0.0015 - val_mse: 0.0015 - val_mae: 0.0213\n",
      "Epoch 13/180\n",
      "65/65 [==============================] - 32s 498ms/step - loss: 0.0033 - mse: 0.0033 - mae: 0.0442 - val_loss: 0.0014 - val_mse: 0.0014 - val_mae: 0.0194\n",
      "Epoch 14/180\n",
      "65/65 [==============================] - 32s 488ms/step - loss: 0.0027 - mse: 0.0027 - mae: 0.0393 - val_loss: 0.0014 - val_mse: 0.0014 - val_mae: 0.0197\n",
      "Epoch 15/180\n",
      "65/65 [==============================] - 33s 500ms/step - loss: 0.0023 - mse: 0.0023 - mae: 0.0350 - val_loss: 0.0014 - val_mse: 0.0014 - val_mae: 0.0197\n",
      "Epoch 16/180\n",
      "65/65 [==============================] - 33s 505ms/step - loss: 0.0020 - mse: 0.0020 - mae: 0.0317 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0175\n",
      "Epoch 17/180\n",
      "65/65 [==============================] - 33s 507ms/step - loss: 0.0017 - mse: 0.0017 - mae: 0.0289 - val_loss: 0.0012 - val_mse: 0.0012 - val_mae: 0.0166\n",
      "Epoch 18/180\n",
      "65/65 [==============================] - 31s 483ms/step - loss: 0.0015 - mse: 0.0015 - mae: 0.0264 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0167\n",
      "Epoch 19/180\n",
      "65/65 [==============================] - 26s 401ms/step - loss: 0.0014 - mse: 0.0014 - mae: 0.0251 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0169\n",
      "Epoch 20/180\n",
      "65/65 [==============================] - 31s 472ms/step - loss: 0.0013 - mse: 0.0013 - mae: 0.0235 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0173\n",
      "Epoch 21/180\n",
      "65/65 [==============================] - 34s 519ms/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0226 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0191\n",
      "Epoch 22/180\n",
      "65/65 [==============================] - 34s 526ms/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0214 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0181\n",
      "Epoch 23/180\n",
      "65/65 [==============================] - 33s 513ms/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0207 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0184\n",
      "Epoch 24/180\n",
      "65/65 [==============================] - 31s 481ms/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0203 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0190\n",
      "Epoch 25/180\n",
      "65/65 [==============================] - 32s 498ms/step - loss: 0.0010 - mse: 0.0010 - mae: 0.0200 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0190\n",
      "Epoch 26/180\n",
      "65/65 [==============================] - 32s 495ms/step - loss: 0.0010 - mse: 0.0010 - mae: 0.0194 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0198\n",
      "Epoch 27/180\n",
      "65/65 [==============================] - 32s 498ms/step - loss: 0.0010 - mse: 0.0010 - mae: 0.0192 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0195\n",
      "Epoch 28/180\n",
      "65/65 [==============================] - 33s 515ms/step - loss: 0.0010 - mse: 0.0010 - mae: 0.0192 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0194\n",
      "Epoch 29/180\n",
      "65/65 [==============================] - 30s 467ms/step - loss: 9.9420e-04 - mse: 9.9420e-04 - mae: 0.0190 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0195\n",
      "Epoch 30/180\n",
      "65/65 [==============================] - 29s 448ms/step - loss: 9.7545e-04 - mse: 9.7545e-04 - mae: 0.0187 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0193\n",
      "Epoch 31/180\n",
      "65/65 [==============================] - 33s 507ms/step - loss: 9.8266e-04 - mse: 9.8266e-04 - mae: 0.0186 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0191\n",
      "Epoch 32/180\n",
      "65/65 [==============================] - 35s 533ms/step - loss: 9.7730e-04 - mse: 9.7730e-04 - mae: 0.0186 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0202\n",
      "Epoch 33/180\n",
      "65/65 [==============================] - 64s 977ms/step - loss: 9.6119e-04 - mse: 9.6119e-04 - mae: 0.0185 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0198\n",
      "Epoch 34/180\n",
      "65/65 [==============================] - 56s 856ms/step - loss: 9.5803e-04 - mse: 9.5803e-04 - mae: 0.0184 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0200\n",
      "Epoch 35/180\n",
      "65/65 [==============================] - 28s 432ms/step - loss: 9.6415e-04 - mse: 9.6415e-04 - mae: 0.0185 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0200\n",
      "Epoch 36/180\n",
      "65/65 [==============================] - 27s 408ms/step - loss: 9.6557e-04 - mse: 9.6557e-04 - mae: 0.0184 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0199\n",
      "Epoch 37/180\n",
      "65/65 [==============================] - 29s 446ms/step - loss: 9.7079e-04 - mse: 9.7079e-04 - mae: 0.0186 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0194\n",
      "Epoch 38/180\n",
      "65/65 [==============================] - 33s 515ms/step - loss: 9.6640e-04 - mse: 9.6640e-04 - mae: 0.0183 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0191\n",
      "Epoch 39/180\n",
      "65/65 [==============================] - 34s 516ms/step - loss: 9.5310e-04 - mse: 9.5310e-04 - mae: 0.0182 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0192\n",
      "Epoch 40/180\n",
      "65/65 [==============================] - 34s 521ms/step - loss: 9.6168e-04 - mse: 9.6168e-04 - mae: 0.0183 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0192\n",
      "Epoch 41/180\n",
      "65/65 [==============================] - 34s 528ms/step - loss: 9.5414e-04 - mse: 9.5414e-04 - mae: 0.0183 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0194\n",
      "Epoch 42/180\n",
      "65/65 [==============================] - 32s 488ms/step - loss: 9.6548e-04 - mse: 9.6548e-04 - mae: 0.0185 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0193\n",
      "Epoch 43/180\n",
      "65/65 [==============================] - 30s 468ms/step - loss: 9.5392e-04 - mse: 9.5392e-04 - mae: 0.0182 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0194\n",
      "Epoch 44/180\n",
      "65/65 [==============================] - 32s 490ms/step - loss: 9.5838e-04 - mse: 9.5838e-04 - mae: 0.0183 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0194\n",
      "Epoch 45/180\n",
      "65/65 [==============================] - 34s 516ms/step - loss: 9.4762e-04 - mse: 9.4762e-04 - mae: 0.0181 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0195\n",
      "Epoch 46/180\n",
      "65/65 [==============================] - 31s 478ms/step - loss: 9.4729e-04 - mse: 9.4729e-04 - mae: 0.0182 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0195\n",
      "Epoch 47/180\n",
      "65/65 [==============================] - 31s 481ms/step - loss: 9.6375e-04 - mse: 9.6375e-04 - mae: 0.0183 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0190\n",
      "Epoch 48/180\n",
      "65/65 [==============================] - 32s 495ms/step - loss: 9.4685e-04 - mse: 9.4685e-04 - mae: 0.0181 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0190\n",
      "Epoch 49/180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 26s 405ms/step - loss: 9.4180e-04 - mse: 9.4180e-04 - mae: 0.0182 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0194\n",
      "Epoch 50/180\n",
      "65/65 [==============================] - 31s 479ms/step - loss: 9.4066e-04 - mse: 9.4066e-04 - mae: 0.0181 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0191\n",
      "Epoch 51/180\n",
      "65/65 [==============================] - 34s 522ms/step - loss: 9.3970e-04 - mse: 9.3970e-04 - mae: 0.0181 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0191\n",
      "Epoch 52/180\n",
      "65/65 [==============================] - 36s 553ms/step - loss: 9.4099e-04 - mse: 9.4099e-04 - mae: 0.0181 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0192\n",
      "Epoch 53/180\n",
      "65/65 [==============================] - 36s 561ms/step - loss: 9.4265e-04 - mse: 9.4265e-04 - mae: 0.0181 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0196\n",
      "Epoch 54/180\n",
      "65/65 [==============================] - 34s 522ms/step - loss: 9.4907e-04 - mse: 9.4907e-04 - mae: 0.0182 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0194\n",
      "Epoch 55/180\n",
      "65/65 [==============================] - 38s 584ms/step - loss: 9.3506e-04 - mse: 9.3506e-04 - mae: 0.0181 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0193\n",
      "Epoch 56/180\n",
      "65/65 [==============================] - 28s 436ms/step - loss: 9.3964e-04 - mse: 9.3964e-04 - mae: 0.0181 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0196\n",
      "Epoch 57/180\n",
      "65/65 [==============================] - 35s 534ms/step - loss: 9.3641e-04 - mse: 9.3641e-04 - mae: 0.0180 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0194\n",
      "Epoch 58/180\n",
      "65/65 [==============================] - 35s 532ms/step - loss: 9.3236e-04 - mse: 9.3236e-04 - mae: 0.0181 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0195\n",
      "Epoch 59/180\n",
      "65/65 [==============================] - 38s 582ms/step - loss: 9.4026e-04 - mse: 9.4026e-04 - mae: 0.0181 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0192\n",
      "Epoch 60/180\n",
      "65/65 [==============================] - 38s 585ms/step - loss: 9.3896e-04 - mse: 9.3896e-04 - mae: 0.0180 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0191\n",
      "Epoch 61/180\n",
      "65/65 [==============================] - 38s 583ms/step - loss: 9.3607e-04 - mse: 9.3607e-04 - mae: 0.0179 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0190\n",
      "Epoch 62/180\n",
      "65/65 [==============================] - 36s 550ms/step - loss: 9.3455e-04 - mse: 9.3455e-04 - mae: 0.0179 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0192\n",
      "Epoch 63/180\n",
      "65/65 [==============================] - 34s 523ms/step - loss: 9.3537e-04 - mse: 9.3537e-04 - mae: 0.0179 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0192\n",
      "Epoch 64/180\n",
      "65/65 [==============================] - 34s 516ms/step - loss: 9.3287e-04 - mse: 9.3287e-04 - mae: 0.0179 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0190\n",
      "Epoch 65/180\n",
      "65/65 [==============================] - 34s 527ms/step - loss: 9.3825e-04 - mse: 9.3825e-04 - mae: 0.0179 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0191\n",
      "Epoch 66/180\n",
      "65/65 [==============================] - 32s 498ms/step - loss: 9.3323e-04 - mse: 9.3323e-04 - mae: 0.0179 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0195\n",
      "Epoch 67/180\n",
      "65/65 [==============================] - 34s 526ms/step - loss: 9.3095e-04 - mse: 9.3095e-04 - mae: 0.0179 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0193\n",
      "Epoch 68/180\n",
      "65/65 [==============================] - 32s 488ms/step - loss: 9.3274e-04 - mse: 9.3274e-04 - mae: 0.0179 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0193\n",
      "Epoch 69/180\n",
      "65/65 [==============================] - 33s 509ms/step - loss: 9.3328e-04 - mse: 9.3328e-04 - mae: 0.0179 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0193\n",
      "Epoch 70/180\n",
      "65/65 [==============================] - 35s 531ms/step - loss: 9.3189e-04 - mse: 9.3189e-04 - mae: 0.0179 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0193\n",
      "Epoch 71/180\n",
      "65/65 [==============================] - 34s 520ms/step - loss: 9.4031e-04 - mse: 9.4031e-04 - mae: 0.0179 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0190\n",
      "Epoch 72/180\n",
      "65/65 [==============================] - 34s 520ms/step - loss: 9.2905e-04 - mse: 9.2905e-04 - mae: 0.0180 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0193\n",
      "Epoch 73/180\n",
      "65/65 [==============================] - 33s 515ms/step - loss: 9.3485e-04 - mse: 9.3485e-04 - mae: 0.0179 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0190\n",
      "Epoch 74/180\n",
      "65/65 [==============================] - 33s 505ms/step - loss: 9.3163e-04 - mse: 9.3163e-04 - mae: 0.0179 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0189\n",
      "Epoch 75/180\n",
      "65/65 [==============================] - 33s 503ms/step - loss: 9.2644e-04 - mse: 9.2644e-04 - mae: 0.0179 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0190\n",
      "Epoch 76/180\n",
      "65/65 [==============================] - 31s 483ms/step - loss: 9.2252e-04 - mse: 9.2252e-04 - mae: 0.0178 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0186\n",
      "Epoch 77/180\n",
      "65/65 [==============================] - 29s 451ms/step - loss: 9.3505e-04 - mse: 9.3505e-04 - mae: 0.0178 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0190\n",
      "Epoch 78/180\n",
      "65/65 [==============================] - 32s 496ms/step - loss: 9.3171e-04 - mse: 9.3171e-04 - mae: 0.0179 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0191\n",
      "Epoch 79/180\n",
      "65/65 [==============================] - 32s 496ms/step - loss: 9.3084e-04 - mse: 9.3084e-04 - mae: 0.0178 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0190\n",
      "Epoch 80/180\n",
      "65/65 [==============================] - 34s 527ms/step - loss: 9.3304e-04 - mse: 9.3304e-04 - mae: 0.0178 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0190\n",
      "Epoch 81/180\n",
      "65/65 [==============================] - 31s 483ms/step - loss: 9.3433e-04 - mse: 9.3433e-04 - mae: 0.0179 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0192\n",
      "Epoch 82/180\n",
      "65/65 [==============================] - 34s 527ms/step - loss: 9.3316e-04 - mse: 9.3316e-04 - mae: 0.0179 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0194\n",
      "Epoch 83/180\n",
      "65/65 [==============================] - 37s 564ms/step - loss: 9.2377e-04 - mse: 9.2377e-04 - mae: 0.0178 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0193\n",
      "Epoch 84/180\n",
      "65/65 [==============================] - 29s 453ms/step - loss: 9.2493e-04 - mse: 9.2493e-04 - mae: 0.0178 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0190\n",
      "Epoch 85/180\n",
      "65/65 [==============================] - 29s 438ms/step - loss: 9.3118e-04 - mse: 9.3118e-04 - mae: 0.0179 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0193\n",
      "Epoch 86/180\n",
      "65/65 [==============================] - 26s 397ms/step - loss: 9.3577e-04 - mse: 9.3577e-04 - mae: 0.0179 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0189\n",
      "Epoch 87/180\n",
      "65/65 [==============================] - 31s 477ms/step - loss: 9.2673e-04 - mse: 9.2673e-04 - mae: 0.0177 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0191\n",
      "Epoch 88/180\n",
      "65/65 [==============================] - 33s 502ms/step - loss: 9.2665e-04 - mse: 9.2665e-04 - mae: 0.0178 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0189\n",
      "Epoch 89/180\n",
      "65/65 [==============================] - 30s 465ms/step - loss: 9.2108e-04 - mse: 9.2108e-04 - mae: 0.0177 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0191\n",
      "Epoch 90/180\n",
      "65/65 [==============================] - 24s 365ms/step - loss: 9.2822e-04 - mse: 9.2822e-04 - mae: 0.0178 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0189\n",
      "Epoch 91/180\n",
      "65/65 [==============================] - 29s 440ms/step - loss: 9.2632e-04 - mse: 9.2632e-04 - mae: 0.0178 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0190\n",
      "Epoch 92/180\n",
      "65/65 [==============================] - 32s 496ms/step - loss: 9.3276e-04 - mse: 9.3276e-04 - mae: 0.0179 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0190\n",
      "Epoch 93/180\n",
      "65/65 [==============================] - 26s 405ms/step - loss: 9.3040e-04 - mse: 9.3040e-04 - mae: 0.0178 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0191\n",
      "Epoch 94/180\n",
      "65/65 [==============================] - 29s 454ms/step - loss: 9.2280e-04 - mse: 9.2280e-04 - mae: 0.0178 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0193\n",
      "Epoch 95/180\n",
      "65/65 [==============================] - 24s 373ms/step - loss: 9.2222e-04 - mse: 9.2222e-04 - mae: 0.0178 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/180\n",
      "65/65 [==============================] - 26s 397ms/step - loss: 9.2828e-04 - mse: 9.2828e-04 - mae: 0.0178 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0194\n",
      "Epoch 97/180\n",
      "65/65 [==============================] - 26s 394ms/step - loss: 9.2008e-04 - mse: 9.2008e-04 - mae: 0.0178 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0196\n",
      "Epoch 98/180\n",
      "65/65 [==============================] - 29s 443ms/step - loss: 9.1958e-04 - mse: 9.1958e-04 - mae: 0.0178 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0194\n",
      "Epoch 99/180\n",
      "65/65 [==============================] - 33s 514ms/step - loss: 9.2210e-04 - mse: 9.2210e-04 - mae: 0.0177 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0191\n",
      "Epoch 100/180\n",
      "65/65 [==============================] - 30s 460ms/step - loss: 9.2580e-04 - mse: 9.2580e-04 - mae: 0.0178 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0193\n",
      "Epoch 101/180\n",
      "65/65 [==============================] - 29s 450ms/step - loss: 9.2535e-04 - mse: 9.2535e-04 - mae: 0.0178 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0192\n",
      "Epoch 102/180\n",
      "65/65 [==============================] - 32s 493ms/step - loss: 9.2058e-04 - mse: 9.2058e-04 - mae: 0.0177 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0191\n",
      "Epoch 103/180\n",
      "65/65 [==============================] - 32s 494ms/step - loss: 9.2457e-04 - mse: 9.2457e-04 - mae: 0.0178 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0192\n",
      "Epoch 104/180\n",
      "65/65 [==============================] - 31s 483ms/step - loss: 9.2645e-04 - mse: 9.2645e-04 - mae: 0.0178 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0191\n",
      "Epoch 105/180\n",
      "65/65 [==============================] - 32s 494ms/step - loss: 9.2572e-04 - mse: 9.2572e-04 - mae: 0.0179 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0189\n",
      "Epoch 106/180\n",
      "65/65 [==============================] - 32s 489ms/step - loss: 9.2794e-04 - mse: 9.2794e-04 - mae: 0.0178 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0193\n",
      "Epoch 107/180\n",
      "65/65 [==============================] - 32s 487ms/step - loss: 9.1857e-04 - mse: 9.1857e-04 - mae: 0.0178 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0193\n",
      "Epoch 108/180\n",
      "65/65 [==============================] - 30s 463ms/step - loss: 9.1881e-04 - mse: 9.1881e-04 - mae: 0.0178 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0192\n",
      "Epoch 109/180\n",
      "65/65 [==============================] - 34s 526ms/step - loss: 9.2218e-04 - mse: 9.2218e-04 - mae: 0.0177 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0191\n",
      "Epoch 110/180\n",
      "65/65 [==============================] - 34s 520ms/step - loss: 9.2585e-04 - mse: 9.2585e-04 - mae: 0.0178 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0189\n",
      "Epoch 111/180\n",
      "65/65 [==============================] - 30s 466ms/step - loss: 9.1979e-04 - mse: 9.1979e-04 - mae: 0.0177 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0192\n",
      "Epoch 112/180\n",
      "65/65 [==============================] - 32s 487ms/step - loss: 9.2237e-04 - mse: 9.2237e-04 - mae: 0.0177 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0193\n",
      "Epoch 113/180\n",
      "65/65 [==============================] - 32s 496ms/step - loss: 9.2096e-04 - mse: 9.2096e-04 - mae: 0.0178 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0193\n",
      "Epoch 114/180\n",
      "65/65 [==============================] - 33s 512ms/step - loss: 9.1690e-04 - mse: 9.1690e-04 - mae: 0.0177 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0193\n",
      "Epoch 115/180\n",
      "65/65 [==============================] - 31s 481ms/step - loss: 9.2193e-04 - mse: 9.2193e-04 - mae: 0.0177 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0192\n",
      "Epoch 116/180\n",
      "65/65 [==============================] - 31s 477ms/step - loss: 9.1985e-04 - mse: 9.1985e-04 - mae: 0.0178 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0191\n",
      "Epoch 117/180\n",
      "65/65 [==============================] - 32s 496ms/step - loss: 9.1500e-04 - mse: 9.1500e-04 - mae: 0.0176 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0191\n",
      "Epoch 118/180\n",
      "65/65 [==============================] - 33s 508ms/step - loss: 9.1463e-04 - mse: 9.1463e-04 - mae: 0.0177 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0195\n",
      "Epoch 119/180\n",
      "65/65 [==============================] - 32s 487ms/step - loss: 9.1660e-04 - mse: 9.1660e-04 - mae: 0.0177 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0194\n",
      "Epoch 120/180\n",
      "65/65 [==============================] - 31s 480ms/step - loss: 9.2536e-04 - mse: 9.2536e-04 - mae: 0.0177 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0193\n",
      "Epoch 121/180\n",
      "65/65 [==============================] - 32s 490ms/step - loss: 9.2292e-04 - mse: 9.2292e-04 - mae: 0.0178 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0191\n",
      "Epoch 122/180\n",
      "65/65 [==============================] - 33s 512ms/step - loss: 9.2017e-04 - mse: 9.2017e-04 - mae: 0.0177 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0192\n",
      "Epoch 123/180\n",
      "65/65 [==============================] - 32s 489ms/step - loss: 9.2396e-04 - mse: 9.2396e-04 - mae: 0.0178 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0191\n",
      "Epoch 124/180\n",
      "65/65 [==============================] - 29s 446ms/step - loss: 9.1913e-04 - mse: 9.1913e-04 - mae: 0.0177 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0194\n",
      "Epoch 125/180\n",
      "65/65 [==============================] - 26s 404ms/step - loss: 9.2133e-04 - mse: 9.2133e-04 - mae: 0.0177 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0191\n",
      "Epoch 126/180\n",
      "65/65 [==============================] - 31s 471ms/step - loss: 9.1892e-04 - mse: 9.1892e-04 - mae: 0.0177 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0191\n",
      "Epoch 127/180\n",
      "65/65 [==============================] - 30s 469ms/step - loss: 9.1558e-04 - mse: 9.1558e-04 - mae: 0.0177 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0190\n",
      "Epoch 128/180\n",
      "65/65 [==============================] - 30s 467ms/step - loss: 9.1510e-04 - mse: 9.1510e-04 - mae: 0.0177 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0189\n",
      "Epoch 129/180\n",
      "65/65 [==============================] - 31s 475ms/step - loss: 9.1393e-04 - mse: 9.1393e-04 - mae: 0.0176 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0192\n",
      "Epoch 130/180\n",
      "65/65 [==============================] - 35s 536ms/step - loss: 9.1564e-04 - mse: 9.1564e-04 - mae: 0.0177 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0192\n",
      "Epoch 131/180\n",
      "65/65 [==============================] - 33s 511ms/step - loss: 9.1814e-04 - mse: 9.1814e-04 - mae: 0.0177 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0192\n",
      "Epoch 132/180\n",
      "65/65 [==============================] - 34s 522ms/step - loss: 9.2059e-04 - mse: 9.2059e-04 - mae: 0.0177 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0191\n",
      "Epoch 133/180\n",
      "65/65 [==============================] - 34s 521ms/step - loss: 9.2206e-04 - mse: 9.2206e-04 - mae: 0.0177 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0192\n",
      "Epoch 134/180\n",
      "65/65 [==============================] - 33s 514ms/step - loss: 9.1451e-04 - mse: 9.1451e-04 - mae: 0.0176 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0191\n",
      "Epoch 135/180\n",
      "65/65 [==============================] - 34s 519ms/step - loss: 9.1888e-04 - mse: 9.1888e-04 - mae: 0.0177 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0190\n",
      "Epoch 136/180\n",
      "65/65 [==============================] - 33s 508ms/step - loss: 9.1089e-04 - mse: 9.1089e-04 - mae: 0.0177 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0192\n",
      "Epoch 137/180\n",
      "65/65 [==============================] - 31s 482ms/step - loss: 9.1536e-04 - mse: 9.1536e-04 - mae: 0.0177 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0193\n",
      "Epoch 138/180\n",
      "65/65 [==============================] - 34s 520ms/step - loss: 9.1252e-04 - mse: 9.1252e-04 - mae: 0.0176 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0190\n",
      "Epoch 139/180\n",
      "65/65 [==============================] - 33s 502ms/step - loss: 9.2315e-04 - mse: 9.2315e-04 - mae: 0.0177 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0190\n",
      "Epoch 140/180\n",
      "65/65 [==============================] - 32s 493ms/step - loss: 9.1538e-04 - mse: 9.1538e-04 - mae: 0.0176 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0191\n",
      "Epoch 141/180\n",
      "65/65 [==============================] - 34s 520ms/step - loss: 9.1720e-04 - mse: 9.1720e-04 - mae: 0.0177 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0190\n",
      "Epoch 142/180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 31s 482ms/step - loss: 9.1959e-04 - mse: 9.1959e-04 - mae: 0.0177 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0188\n",
      "Epoch 143/180\n",
      "65/65 [==============================] - 32s 488ms/step - loss: 9.1059e-04 - mse: 9.1059e-04 - mae: 0.0176 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0188\n",
      "Epoch 144/180\n",
      "65/65 [==============================] - 32s 486ms/step - loss: 9.1178e-04 - mse: 9.1178e-04 - mae: 0.0177 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0191\n",
      "Epoch 145/180\n",
      "65/65 [==============================] - 32s 498ms/step - loss: 9.1413e-04 - mse: 9.1413e-04 - mae: 0.0177 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0189\n",
      "Epoch 146/180\n",
      "65/65 [==============================] - 30s 458ms/step - loss: 9.1563e-04 - mse: 9.1563e-04 - mae: 0.0176 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0189\n",
      "Epoch 147/180\n",
      "65/65 [==============================] - 33s 501ms/step - loss: 9.1945e-04 - mse: 9.1945e-04 - mae: 0.0176 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0189\n",
      "Epoch 148/180\n",
      "65/65 [==============================] - 35s 546ms/step - loss: 9.1081e-04 - mse: 9.1081e-04 - mae: 0.0176 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0191\n",
      "Epoch 149/180\n",
      "65/65 [==============================] - 35s 542ms/step - loss: 9.1669e-04 - mse: 9.1669e-04 - mae: 0.0176 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0190\n",
      "Epoch 150/180\n",
      "65/65 [==============================] - 35s 543ms/step - loss: 9.1571e-04 - mse: 9.1571e-04 - mae: 0.0177 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0193\n",
      "Epoch 151/180\n",
      "65/65 [==============================] - 35s 535ms/step - loss: 9.1206e-04 - mse: 9.1206e-04 - mae: 0.0176 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0194\n",
      "Epoch 152/180\n",
      "65/65 [==============================] - 35s 543ms/step - loss: 9.1152e-04 - mse: 9.1152e-04 - mae: 0.0176 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0193\n",
      "Epoch 153/180\n",
      "65/65 [==============================] - 35s 541ms/step - loss: 9.1042e-04 - mse: 9.1042e-04 - mae: 0.0177 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0190\n",
      "Epoch 154/180\n",
      "65/65 [==============================] - 34s 524ms/step - loss: 9.0914e-04 - mse: 9.0914e-04 - mae: 0.0176 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0192\n",
      "Epoch 155/180\n",
      "65/65 [==============================] - 35s 539ms/step - loss: 9.1145e-04 - mse: 9.1145e-04 - mae: 0.0176 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0191\n",
      "Epoch 156/180\n",
      "65/65 [==============================] - 34s 530ms/step - loss: 9.1271e-04 - mse: 9.1271e-04 - mae: 0.0176 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0190\n",
      "Epoch 157/180\n",
      "65/65 [==============================] - 34s 527ms/step - loss: 9.1353e-04 - mse: 9.1353e-04 - mae: 0.0176 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0193\n",
      "Epoch 158/180\n",
      "65/65 [==============================] - 35s 533ms/step - loss: 9.1434e-04 - mse: 9.1434e-04 - mae: 0.0176 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0189\n",
      "Epoch 159/180\n",
      "65/65 [==============================] - 35s 532ms/step - loss: 9.1383e-04 - mse: 9.1383e-04 - mae: 0.0176 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0190\n",
      "Epoch 160/180\n",
      "65/65 [==============================] - 34s 528ms/step - loss: 9.1060e-04 - mse: 9.1060e-04 - mae: 0.0176 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0188\n",
      "Epoch 161/180\n",
      "65/65 [==============================] - 34s 524ms/step - loss: 9.1196e-04 - mse: 9.1196e-04 - mae: 0.0176 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0188\n",
      "Epoch 162/180\n",
      "65/65 [==============================] - 25s 387ms/step - loss: 9.1439e-04 - mse: 9.1439e-04 - mae: 0.0176 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0189\n",
      "Epoch 163/180\n",
      "65/65 [==============================] - 31s 478ms/step - loss: 9.0951e-04 - mse: 9.0951e-04 - mae: 0.0175 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0188\n",
      "Epoch 164/180\n",
      "65/65 [==============================] - 31s 480ms/step - loss: 9.1391e-04 - mse: 9.1391e-04 - mae: 0.0176 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0193\n",
      "Epoch 165/180\n",
      "65/65 [==============================] - 32s 487ms/step - loss: 9.1025e-04 - mse: 9.1025e-04 - mae: 0.0177 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0193\n",
      "Epoch 166/180\n",
      "65/65 [==============================] - 33s 502ms/step - loss: 9.0992e-04 - mse: 9.0992e-04 - mae: 0.0176 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0191\n",
      "Epoch 167/180\n",
      "65/65 [==============================] - 30s 466ms/step - loss: 9.1644e-04 - mse: 9.1644e-04 - mae: 0.0177 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0190\n",
      "Epoch 168/180\n",
      "65/65 [==============================] - 33s 512ms/step - loss: 9.1725e-04 - mse: 9.1725e-04 - mae: 0.0177 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0192\n",
      "Epoch 169/180\n",
      "65/65 [==============================] - 32s 492ms/step - loss: 9.1336e-04 - mse: 9.1336e-04 - mae: 0.0177 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0192\n",
      "Epoch 170/180\n",
      "65/65 [==============================] - 34s 519ms/step - loss: 9.0690e-04 - mse: 9.0690e-04 - mae: 0.0176 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0192\n",
      "Epoch 171/180\n",
      "65/65 [==============================] - 31s 479ms/step - loss: 9.1459e-04 - mse: 9.1459e-04 - mae: 0.0176 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0188\n",
      "Epoch 172/180\n",
      "65/65 [==============================] - 32s 497ms/step - loss: 9.1244e-04 - mse: 9.1244e-04 - mae: 0.0176 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0190\n",
      "Epoch 173/180\n",
      "65/65 [==============================] - 30s 459ms/step - loss: 9.0802e-04 - mse: 9.0802e-04 - mae: 0.0175 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0190\n",
      "Epoch 174/180\n",
      "65/65 [==============================] - 32s 496ms/step - loss: 9.1113e-04 - mse: 9.1113e-04 - mae: 0.0176 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0192\n",
      "Epoch 175/180\n",
      "65/65 [==============================] - 35s 531ms/step - loss: 9.1183e-04 - mse: 9.1183e-04 - mae: 0.0176 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0192\n",
      "Epoch 176/180\n",
      "65/65 [==============================] - 34s 527ms/step - loss: 9.1166e-04 - mse: 9.1166e-04 - mae: 0.0176 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0192\n",
      "Epoch 177/180\n",
      "65/65 [==============================] - 33s 507ms/step - loss: 9.0870e-04 - mse: 9.0870e-04 - mae: 0.0176 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0193\n",
      "Epoch 178/180\n",
      "65/65 [==============================] - 33s 501ms/step - loss: 9.1455e-04 - mse: 9.1455e-04 - mae: 0.0176 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0191\n",
      "Epoch 179/180\n",
      "65/65 [==============================] - 31s 480ms/step - loss: 9.1347e-04 - mse: 9.1347e-04 - mae: 0.0176 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0194\n",
      "Epoch 180/180\n",
      "65/65 [==============================] - 29s 453ms/step - loss: 9.1111e-04 - mse: 9.1111e-04 - mae: 0.0176 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0194\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# get data ready with one feature\n",
    "data = data_full.loc[:,['PrevDay', 'Offers']]\n",
    "\n",
    "# divide data into train and test \n",
    "data_train, data_test = train_test_split(\n",
    "             data, test_size = 0.15, shuffle=False) \n",
    "\n",
    "sc_X = MinMaxScaler()\n",
    "data_train = sc_X.fit_transform(data_train)\n",
    "data_test = sc_X.transform(data_test)\n",
    "\n",
    "# divide features and labels\n",
    "X_train = data_train[:, 0] \n",
    "y_train = data_train[:, -1]\n",
    "X_test = data_test[:, 0] \n",
    "y_test = data_test[:, -1]\n",
    "\n",
    "# divide data into train and test \n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "             X_train, y_train, test_size = 0.15, shuffle=False)\n",
    "\n",
    "# function to split data into correct shape for RNN for 1 feature only\n",
    "def split_data_1(X, y, steps):\n",
    "    X_, y_ = list(), list()\n",
    "    for i in range(steps, len(y)):\n",
    "        X_.append(X[i - steps : i])\n",
    "        y_.append(y[i]) \n",
    "    return np.array(X_), np.array(y_)\n",
    "\n",
    "# put data into correct shape\n",
    "X_train, y_train = split_data_1(X_train, y_train, steps)\n",
    "X_test, y_test = split_data_1(X_test, y_test, steps)\n",
    "X_val, y_val = split_data_1(X_val, y_val, steps)\n",
    "\n",
    "X_train = cut_data(X_train, batch_size)\n",
    "y_train = cut_data(y_train, batch_size)\n",
    "X_test = cut_data(X_test, batch_size)\n",
    "y_test = cut_data(y_test, batch_size)\n",
    "X_val = cut_data(X_val, batch_size)\n",
    "y_val = cut_data(y_val, batch_size)\n",
    "\n",
    "# Reshaping\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_val = np.reshape(X_val, (X_val.shape[0], X_val.shape[1], 1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "features_num = 1\n",
    "\n",
    "model = regressor_tunning(features_num = features_num)\n",
    "\n",
    "# fitting the LSTM to the training set\n",
    "history = model.fit(X_train,\n",
    "                    y_train, \n",
    "                    batch_size = batch_size, \n",
    "                    epochs = epochs,\n",
    "                    shuffle = False, \n",
    "                    validation_data = (X_val, y_val))\n",
    "\n",
    "# make new predicitons with test set\n",
    "y_pred = model.predict(X_test, batch_size = batch_size)\n",
    "    \n",
    "# prices col = 15 (inverso should not be used as scalling was made with the whole data set)\n",
    "y_pred = (y_pred * sc_X.data_range_[-1]) + (sc_X.data_min_[-1])\n",
    "y_test = (y_test * sc_X.data_range_[-1]) + (sc_X.data_min_[-1])\n",
    "\n",
    "# smal adjustment\n",
    "y_test = pd.Series(y_test)\n",
    "y_test.replace(0, 0.0001,inplace = True)\n",
    " \n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "\n",
    "rmse_error = mse(y_test, y_pred, squared = False)\n",
    "mae_error = mae(y_test, y_pred)\n",
    "  \n",
    "rmse_gen.append(rmse_error)\n",
    "mae_gen.append(mae_error)\n",
    "    \n",
    "# =============================================================================\n",
    "# Metrics evaluation on spike regions\n",
    "# =============================================================================\n",
    "    \n",
    "y_spike_occ = pd.read_csv('Spike_binary_1std.csv', usecols = [6])\n",
    "    \n",
    "# create array same size as y_test\n",
    "y_spike_occ = y_spike_occ.iloc[- len(y_test):]\n",
    "y_spike_occ = pd.Series(y_spike_occ.iloc[:,0]).values\n",
    "\n",
    "# select y_pred and y_test only for regions with spikes\n",
    "y_test_spike = (y_test.T * y_spike_occ).T\n",
    "y_pred_spike = (y_pred.T * y_spike_occ).T\n",
    "y_test_spike = y_test_spike[y_test_spike != 0]\n",
    "y_pred_spike = y_pred_spike[y_pred_spike != 0]\n",
    "    \n",
    "# calculate metric\n",
    "rmse_spike = mse(y_test_spike, y_pred_spike, squared = False)\n",
    "mae_spike = mae(y_test_spike, y_pred_spike)\n",
    "    \n",
    "rmse_spi.append(rmse_spike)\n",
    "mae_spi.append(mae_spike)\n",
    "    \n",
    "# =============================================================================\n",
    "# Metric evaluation on normal regions\n",
    "# =============================================================================\n",
    "    \n",
    "# inverse y_spike_occ so the only normal occurences are chosen\n",
    "y_normal_occ = (y_spike_occ - 1) * (-1)\n",
    " \n",
    "# sanity check\n",
    "y_normal_occ.sum() + y_spike_occ.sum() # gives the correct total \n",
    "  \n",
    "# select y_pred and y_test only for normal regions\n",
    "y_test_normal = (y_test.T * y_normal_occ).T\n",
    "y_pred_normal = (y_pred.T * y_normal_occ).T\n",
    "y_test_normal = y_test_normal[y_test_normal != 0.00]\n",
    "y_pred_normal = y_pred_normal[y_pred_normal != 0.00]\n",
    "    \n",
    "# calculate metric\n",
    "rmse_normal = mse(y_test_normal, y_pred_normal, squared = False)\n",
    "mae_normal = mae(y_test_normal, y_pred_normal)\n",
    "    \n",
    "rmse_nor.append(rmse_normal)\n",
    "mae_nor.append(mae_normal)\n",
    "    \n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "time_count.append(elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop to continue testing other features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/180\n",
      "65/65 [==============================] - 28s 426ms/step - loss: 0.1749 - mse: 0.1749 - mae: 0.3249 - val_loss: 0.0104 - val_mse: 0.0104 - val_mae: 0.0794\n",
      "Epoch 2/180\n",
      "65/65 [==============================] - 27s 412ms/step - loss: 0.0977 - mse: 0.0977 - mae: 0.2493 - val_loss: 0.0107 - val_mse: 0.0107 - val_mae: 0.0822\n",
      "Epoch 3/180\n",
      "65/65 [==============================] - 25s 377ms/step - loss: 0.0617 - mse: 0.0617 - mae: 0.1985 - val_loss: 0.0102 - val_mse: 0.0102 - val_mae: 0.0611\n",
      "Epoch 4/180\n",
      "65/65 [==============================] - 27s 410ms/step - loss: 0.0397 - mse: 0.0397 - mae: 0.1592 - val_loss: 0.0117 - val_mse: 0.0117 - val_mae: 0.0663\n",
      "Epoch 5/180\n",
      "65/65 [==============================] - 26s 400ms/step - loss: 0.0283 - mse: 0.0283 - mae: 0.1335 - val_loss: 0.0091 - val_mse: 0.0091 - val_mae: 0.0624\n",
      "Epoch 6/180\n",
      "65/65 [==============================] - 27s 415ms/step - loss: 0.0204 - mse: 0.0204 - mae: 0.1138 - val_loss: 0.0093 - val_mse: 0.0093 - val_mae: 0.0599\n",
      "Epoch 7/180\n",
      "65/65 [==============================] - 29s 441ms/step - loss: 0.0166 - mse: 0.0166 - mae: 0.1029 - val_loss: 0.0105 - val_mse: 0.0105 - val_mae: 0.0615\n",
      "Epoch 8/180\n",
      "65/65 [==============================] - 29s 446ms/step - loss: 0.0135 - mse: 0.0135 - mae: 0.0920 - val_loss: 0.0096 - val_mse: 0.0096 - val_mae: 0.0592\n",
      "Epoch 9/180\n",
      "65/65 [==============================] - 29s 439ms/step - loss: 0.0113 - mse: 0.0113 - mae: 0.0846 - val_loss: 0.0090 - val_mse: 0.0090 - val_mae: 0.0635\n",
      "Epoch 10/180\n",
      "65/65 [==============================] - 26s 404ms/step - loss: 0.0101 - mse: 0.0101 - mae: 0.0802 - val_loss: 0.0093 - val_mse: 0.0093 - val_mae: 0.0589\n",
      "Epoch 11/180\n",
      "65/65 [==============================] - 33s 512ms/step - loss: 0.0090 - mse: 0.0090 - mae: 0.0753 - val_loss: 0.0091 - val_mse: 0.0091 - val_mae: 0.0597\n",
      "Epoch 12/180\n",
      "65/65 [==============================] - 35s 543ms/step - loss: 0.0084 - mse: 0.0084 - mae: 0.0723 - val_loss: 0.0091 - val_mse: 0.0091 - val_mae: 0.0598\n",
      "Epoch 13/180\n",
      "65/65 [==============================] - 30s 458ms/step - loss: 0.0080 - mse: 0.0080 - mae: 0.0705 - val_loss: 0.0093 - val_mse: 0.0093 - val_mae: 0.0590\n",
      "Epoch 14/180\n",
      "65/65 [==============================] - 24s 372ms/step - loss: 0.0074 - mse: 0.0074 - mae: 0.0683 - val_loss: 0.0092 - val_mse: 0.0092 - val_mae: 0.0589\n",
      "Epoch 15/180\n",
      "65/65 [==============================] - 24s 370ms/step - loss: 0.0072 - mse: 0.0072 - mae: 0.0664 - val_loss: 0.0090 - val_mse: 0.0090 - val_mae: 0.0599\n",
      "Epoch 16/180\n",
      "65/65 [==============================] - 25s 379ms/step - loss: 0.0068 - mse: 0.0068 - mae: 0.0654 - val_loss: 0.0092 - val_mse: 0.0092 - val_mae: 0.0589\n",
      "Epoch 17/180\n",
      "65/65 [==============================] - 34s 520ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0648 - val_loss: 0.0091 - val_mse: 0.0091 - val_mae: 0.0590\n",
      "Epoch 18/180\n",
      "65/65 [==============================] - 32s 500ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0638 - val_loss: 0.0092 - val_mse: 0.0092 - val_mae: 0.0587\n",
      "Epoch 19/180\n",
      "65/65 [==============================] - 35s 532ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0628 - val_loss: 0.0094 - val_mse: 0.0094 - val_mae: 0.0587\n",
      "Epoch 20/180\n",
      "65/65 [==============================] - 34s 528ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0633 - val_loss: 0.0093 - val_mse: 0.0093 - val_mae: 0.0587\n",
      "Epoch 21/180\n",
      "65/65 [==============================] - 31s 475ms/step - loss: 0.0064 - mse: 0.0064 - mae: 0.0631 - val_loss: 0.0091 - val_mse: 0.0091 - val_mae: 0.0587\n",
      "Epoch 22/180\n",
      "65/65 [==============================] - 33s 501ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0620 - val_loss: 0.0092 - val_mse: 0.0092 - val_mae: 0.0585\n",
      "Epoch 23/180\n",
      "65/65 [==============================] - 28s 427ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0619 - val_loss: 0.0091 - val_mse: 0.0091 - val_mae: 0.0585\n",
      "Epoch 24/180\n",
      "65/65 [==============================] - 28s 429ms/step - loss: 0.0062 - mse: 0.0062 - mae: 0.0619 - val_loss: 0.0092 - val_mse: 0.0092 - val_mae: 0.0584\n",
      "Epoch 25/180\n",
      "65/65 [==============================] - 31s 483ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0618 - val_loss: 0.0092 - val_mse: 0.0092 - val_mae: 0.0583\n",
      "Epoch 26/180\n",
      "65/65 [==============================] - 28s 429ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0615 - val_loss: 0.0091 - val_mse: 0.0091 - val_mae: 0.0583\n",
      "Epoch 27/180\n",
      "65/65 [==============================] - 32s 499ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0612 - val_loss: 0.0092 - val_mse: 0.0092 - val_mae: 0.0582\n",
      "Epoch 28/180\n",
      "65/65 [==============================] - 33s 510ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0616 - val_loss: 0.0091 - val_mse: 0.0091 - val_mae: 0.0582\n",
      "Epoch 29/180\n",
      "65/65 [==============================] - 32s 493ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0616 - val_loss: 0.0092 - val_mse: 0.0092 - val_mae: 0.0582\n",
      "Epoch 30/180\n",
      "65/65 [==============================] - 28s 427ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0607 - val_loss: 0.0090 - val_mse: 0.0090 - val_mae: 0.0581\n",
      "Epoch 31/180\n",
      "65/65 [==============================] - 26s 399ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0610 - val_loss: 0.0091 - val_mse: 0.0091 - val_mae: 0.0580\n",
      "Epoch 32/180\n",
      "65/65 [==============================] - 32s 499ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0606 - val_loss: 0.0090 - val_mse: 0.0090 - val_mae: 0.0579\n",
      "Epoch 33/180\n",
      "65/65 [==============================] - 32s 491ms/step - loss: 0.0060 - mse: 0.0060 - mae: 0.0607 - val_loss: 0.0091 - val_mse: 0.0091 - val_mae: 0.0579\n",
      "Epoch 34/180\n",
      "65/65 [==============================] - 33s 507ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0608 - val_loss: 0.0091 - val_mse: 0.0091 - val_mae: 0.0578\n",
      "Epoch 35/180\n",
      "65/65 [==============================] - 34s 521ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0605 - val_loss: 0.0090 - val_mse: 0.0090 - val_mae: 0.0578\n",
      "Epoch 36/180\n",
      "65/65 [==============================] - 33s 515ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0604 - val_loss: 0.0091 - val_mse: 0.0091 - val_mae: 0.0578\n",
      "Epoch 37/180\n",
      "65/65 [==============================] - 32s 497ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0601 - val_loss: 0.0090 - val_mse: 0.0090 - val_mae: 0.0578\n",
      "Epoch 38/180\n",
      "65/65 [==============================] - 33s 500ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0605 - val_loss: 0.0090 - val_mse: 0.0090 - val_mae: 0.0577\n",
      "Epoch 39/180\n",
      "65/65 [==============================] - 32s 489ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0602 - val_loss: 0.0090 - val_mse: 0.0090 - val_mae: 0.0576\n",
      "Epoch 40/180\n",
      "65/65 [==============================] - 30s 469ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0609 - val_loss: 0.0089 - val_mse: 0.0089 - val_mae: 0.0577\n",
      "Epoch 41/180\n",
      "65/65 [==============================] - 34s 528ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0601 - val_loss: 0.0089 - val_mse: 0.0089 - val_mae: 0.0576\n",
      "Epoch 42/180\n",
      "65/65 [==============================] - 36s 560ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0606 - val_loss: 0.0090 - val_mse: 0.0090 - val_mae: 0.0575\n",
      "Epoch 43/180\n",
      "65/65 [==============================] - 36s 560ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0601 - val_loss: 0.0089 - val_mse: 0.0089 - val_mae: 0.0575\n",
      "Epoch 44/180\n",
      "65/65 [==============================] - 35s 538ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0601 - val_loss: 0.0089 - val_mse: 0.0089 - val_mae: 0.0575\n",
      "Epoch 45/180\n",
      "65/65 [==============================] - 37s 565ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0604 - val_loss: 0.0089 - val_mse: 0.0089 - val_mae: 0.0574\n",
      "Epoch 46/180\n",
      "65/65 [==============================] - 37s 565ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0601 - val_loss: 0.0088 - val_mse: 0.0088 - val_mae: 0.0574\n",
      "Epoch 47/180\n",
      "65/65 [==============================] - 37s 569ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0603 - val_loss: 0.0089 - val_mse: 0.0089 - val_mae: 0.0573\n",
      "Epoch 48/180\n",
      "65/65 [==============================] - 30s 463ms/step - loss: 0.0059 - mse: 0.0059 - mae: 0.0604 - val_loss: 0.0089 - val_mse: 0.0089 - val_mae: 0.0573\n",
      "Epoch 49/180\n",
      "65/65 [==============================] - 30s 469ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0602 - val_loss: 0.0089 - val_mse: 0.0089 - val_mae: 0.0572\n",
      "Epoch 50/180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 40s 621ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0598 - val_loss: 0.0089 - val_mse: 0.0089 - val_mae: 0.0572\n",
      "Epoch 51/180\n",
      "65/65 [==============================] - 38s 585ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0600 - val_loss: 0.0089 - val_mse: 0.0089 - val_mae: 0.0572\n",
      "Epoch 52/180\n",
      "65/65 [==============================] - 35s 545ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0596 - val_loss: 0.0088 - val_mse: 0.0088 - val_mae: 0.0571\n",
      "Epoch 53/180\n",
      "65/65 [==============================] - 34s 520ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0600 - val_loss: 0.0088 - val_mse: 0.0088 - val_mae: 0.0571\n",
      "Epoch 54/180\n",
      "65/65 [==============================] - 33s 514ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0598 - val_loss: 0.0089 - val_mse: 0.0089 - val_mae: 0.0569\n",
      "Epoch 55/180\n",
      "65/65 [==============================] - 33s 510ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0595 - val_loss: 0.0088 - val_mse: 0.0088 - val_mae: 0.0568\n",
      "Epoch 56/180\n",
      "65/65 [==============================] - 34s 516ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0594 - val_loss: 0.0088 - val_mse: 0.0088 - val_mae: 0.0566\n",
      "Epoch 57/180\n",
      "65/65 [==============================] - 34s 516ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0594 - val_loss: 0.0087 - val_mse: 0.0087 - val_mae: 0.0564\n",
      "Epoch 58/180\n",
      "65/65 [==============================] - 35s 541ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0593 - val_loss: 0.0087 - val_mse: 0.0087 - val_mae: 0.0563\n",
      "Epoch 59/180\n",
      "65/65 [==============================] - 38s 590ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0590 - val_loss: 0.0088 - val_mse: 0.0088 - val_mae: 0.0563\n",
      "Epoch 60/180\n",
      "65/65 [==============================] - 40s 610ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0580 - val_loss: 0.0088 - val_mse: 0.0088 - val_mae: 0.0559\n",
      "Epoch 61/180\n",
      "65/65 [==============================] - 40s 621ms/step - loss: 0.0052 - mse: 0.0052 - mae: 0.0568 - val_loss: 0.0088 - val_mse: 0.0088 - val_mae: 0.0554\n",
      "Epoch 62/180\n",
      "65/65 [==============================] - 41s 626ms/step - loss: 0.0051 - mse: 0.0051 - mae: 0.0557 - val_loss: 0.0088 - val_mse: 0.0088 - val_mae: 0.0553\n",
      "Epoch 63/180\n",
      "65/65 [==============================] - 36s 548ms/step - loss: 0.0050 - mse: 0.0050 - mae: 0.0548 - val_loss: 0.0089 - val_mse: 0.0089 - val_mae: 0.0554\n",
      "Epoch 64/180\n",
      "65/65 [==============================] - 33s 504ms/step - loss: 0.0049 - mse: 0.0049 - mae: 0.0545 - val_loss: 0.0089 - val_mse: 0.0089 - val_mae: 0.0559\n",
      "Epoch 65/180\n",
      "65/65 [==============================] - 33s 508ms/step - loss: 0.0050 - mse: 0.0050 - mae: 0.0550 - val_loss: 0.0090 - val_mse: 0.0090 - val_mae: 0.0561\n",
      "Epoch 66/180\n",
      "65/65 [==============================] - 31s 483ms/step - loss: 0.0049 - mse: 0.0049 - mae: 0.0546 - val_loss: 0.0090 - val_mse: 0.0090 - val_mae: 0.0572\n",
      "Epoch 67/180\n",
      "65/65 [==============================] - 35s 540ms/step - loss: 0.0049 - mse: 0.0049 - mae: 0.0540 - val_loss: 0.0090 - val_mse: 0.0090 - val_mae: 0.0591\n",
      "Epoch 68/180\n",
      "65/65 [==============================] - 35s 543ms/step - loss: 0.0048 - mse: 0.0048 - mae: 0.0538 - val_loss: 0.0090 - val_mse: 0.0090 - val_mae: 0.0591\n",
      "Epoch 69/180\n",
      "65/65 [==============================] - 42s 645ms/step - loss: 0.0049 - mse: 0.0049 - mae: 0.0544 - val_loss: 0.0090 - val_mse: 0.0090 - val_mae: 0.0594\n",
      "Epoch 70/180\n",
      "65/65 [==============================] - 39s 597ms/step - loss: 0.0049 - mse: 0.0049 - mae: 0.0539 - val_loss: 0.0091 - val_mse: 0.0091 - val_mae: 0.0598\n",
      "Epoch 71/180\n",
      "65/65 [==============================] - 38s 580ms/step - loss: 0.0048 - mse: 0.0048 - mae: 0.0536 - val_loss: 0.0092 - val_mse: 0.0092 - val_mae: 0.0607\n",
      "Epoch 72/180\n",
      "65/65 [==============================] - 37s 570ms/step - loss: 0.0048 - mse: 0.0048 - mae: 0.0537 - val_loss: 0.0092 - val_mse: 0.0092 - val_mae: 0.0605\n",
      "Epoch 73/180\n",
      "65/65 [==============================] - 37s 575ms/step - loss: 0.0049 - mse: 0.0049 - mae: 0.0548 - val_loss: 0.0095 - val_mse: 0.0095 - val_mae: 0.0628\n",
      "Epoch 74/180\n",
      "65/65 [==============================] - 40s 611ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0530 - val_loss: 0.0093 - val_mse: 0.0093 - val_mae: 0.0609\n",
      "Epoch 75/180\n",
      "65/65 [==============================] - 39s 598ms/step - loss: 0.0049 - mse: 0.0049 - mae: 0.0548 - val_loss: 0.0097 - val_mse: 0.0097 - val_mae: 0.0643\n",
      "Epoch 76/180\n",
      "65/65 [==============================] - 39s 606ms/step - loss: 0.0048 - mse: 0.0048 - mae: 0.0539 - val_loss: 0.0092 - val_mse: 0.0092 - val_mae: 0.0612\n",
      "Epoch 77/180\n",
      "65/65 [==============================] - 40s 621ms/step - loss: 0.0047 - mse: 0.0047 - mae: 0.0529 - val_loss: 0.0099 - val_mse: 0.0099 - val_mae: 0.0662\n",
      "Epoch 78/180\n",
      "65/65 [==============================] - 39s 597ms/step - loss: 0.0046 - mse: 0.0046 - mae: 0.0528 - val_loss: 0.0094 - val_mse: 0.0094 - val_mae: 0.0630\n",
      "Epoch 79/180\n",
      "65/65 [==============================] - 38s 585ms/step - loss: 0.0046 - mse: 0.0046 - mae: 0.0523 - val_loss: 0.0094 - val_mse: 0.0094 - val_mae: 0.0626\n",
      "Epoch 80/180\n",
      "65/65 [==============================] - 40s 621ms/step - loss: 0.0045 - mse: 0.0045 - mae: 0.0523 - val_loss: 0.0093 - val_mse: 0.0093 - val_mae: 0.0622\n",
      "Epoch 81/180\n",
      "65/65 [==============================] - 44s 670ms/step - loss: 0.0046 - mse: 0.0046 - mae: 0.0525 - val_loss: 0.0096 - val_mse: 0.0096 - val_mae: 0.0644\n",
      "Epoch 82/180\n",
      "65/65 [==============================] - 39s 604ms/step - loss: 0.0046 - mse: 0.0046 - mae: 0.0527 - val_loss: 0.0096 - val_mse: 0.0096 - val_mae: 0.0639\n",
      "Epoch 83/180\n",
      "65/65 [==============================] - 36s 556ms/step - loss: 0.0046 - mse: 0.0046 - mae: 0.0523 - val_loss: 0.0095 - val_mse: 0.0095 - val_mae: 0.0634\n",
      "Epoch 84/180\n",
      "65/65 [==============================] - 40s 610ms/step - loss: 0.0046 - mse: 0.0046 - mae: 0.0523 - val_loss: 0.0090 - val_mse: 0.0090 - val_mae: 0.0600\n",
      "Epoch 85/180\n",
      "65/65 [==============================] - 42s 649ms/step - loss: 0.0045 - mse: 0.0045 - mae: 0.0517 - val_loss: 0.0092 - val_mse: 0.0092 - val_mae: 0.0614\n",
      "Epoch 86/180\n",
      "65/65 [==============================] - 42s 649ms/step - loss: 0.0045 - mse: 0.0045 - mae: 0.0519 - val_loss: 0.0095 - val_mse: 0.0095 - val_mae: 0.0638\n",
      "Epoch 87/180\n",
      "65/65 [==============================] - 33s 502ms/step - loss: 0.0045 - mse: 0.0045 - mae: 0.0519 - val_loss: 0.0097 - val_mse: 0.0097 - val_mae: 0.0649\n",
      "Epoch 88/180\n",
      "65/65 [==============================] - 36s 560ms/step - loss: 0.0044 - mse: 0.0044 - mae: 0.0514 - val_loss: 0.0094 - val_mse: 0.0094 - val_mae: 0.0628\n",
      "Epoch 89/180\n",
      "65/65 [==============================] - 39s 593ms/step - loss: 0.0045 - mse: 0.0045 - mae: 0.0517 - val_loss: 0.0100 - val_mse: 0.0100 - val_mae: 0.0675\n",
      "Epoch 90/180\n",
      "65/65 [==============================] - 32s 485ms/step - loss: 0.0045 - mse: 0.0045 - mae: 0.0518 - val_loss: 0.0093 - val_mse: 0.0093 - val_mae: 0.0623\n",
      "Epoch 91/180\n",
      "65/65 [==============================] - 30s 465ms/step - loss: 0.0044 - mse: 0.0044 - mae: 0.0513 - val_loss: 0.0098 - val_mse: 0.0098 - val_mae: 0.0660\n",
      "Epoch 92/180\n",
      "65/65 [==============================] - 37s 571ms/step - loss: 0.0045 - mse: 0.0045 - mae: 0.0517 - val_loss: 0.0096 - val_mse: 0.0096 - val_mae: 0.0645\n",
      "Epoch 93/180\n",
      "65/65 [==============================] - 32s 499ms/step - loss: 0.0044 - mse: 0.0044 - mae: 0.0512 - val_loss: 0.0095 - val_mse: 0.0095 - val_mae: 0.0636\n",
      "Epoch 94/180\n",
      "65/65 [==============================] - 32s 491ms/step - loss: 0.0044 - mse: 0.0044 - mae: 0.0509 - val_loss: 0.0093 - val_mse: 0.0093 - val_mae: 0.0623\n",
      "Epoch 95/180\n",
      "65/65 [==============================] - 31s 471ms/step - loss: 0.0044 - mse: 0.0044 - mae: 0.0512 - val_loss: 0.0098 - val_mse: 0.0098 - val_mae: 0.0655\n",
      "Epoch 96/180\n",
      "65/65 [==============================] - 36s 561ms/step - loss: 0.0044 - mse: 0.0044 - mae: 0.0513 - val_loss: 0.0098 - val_mse: 0.0098 - val_mae: 0.0658\n",
      "Epoch 97/180\n",
      "65/65 [==============================] - 36s 547ms/step - loss: 0.0044 - mse: 0.0044 - mae: 0.0513 - val_loss: 0.0097 - val_mse: 0.0097 - val_mae: 0.0656\n",
      "Epoch 98/180\n",
      "65/65 [==============================] - 38s 582ms/step - loss: 0.0044 - mse: 0.0044 - mae: 0.0513 - val_loss: 0.0098 - val_mse: 0.0098 - val_mae: 0.0662\n",
      "Epoch 99/180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 34s 528ms/step - loss: 0.0044 - mse: 0.0044 - mae: 0.0512 - val_loss: 0.0097 - val_mse: 0.0097 - val_mae: 0.0651\n",
      "Epoch 100/180\n",
      "65/65 [==============================] - 31s 478ms/step - loss: 0.0044 - mse: 0.0044 - mae: 0.0511 - val_loss: 0.0099 - val_mse: 0.0099 - val_mae: 0.0665\n",
      "Epoch 101/180\n",
      "65/65 [==============================] - 32s 491ms/step - loss: 0.0044 - mse: 0.0044 - mae: 0.0512 - val_loss: 0.0095 - val_mse: 0.0095 - val_mae: 0.0638\n",
      "Epoch 102/180\n",
      "65/65 [==============================] - 31s 483ms/step - loss: 0.0043 - mse: 0.0043 - mae: 0.0508 - val_loss: 0.0097 - val_mse: 0.0097 - val_mae: 0.0650\n",
      "Epoch 103/180\n",
      "65/65 [==============================] - 36s 561ms/step - loss: 0.0044 - mse: 0.0044 - mae: 0.0510 - val_loss: 0.0097 - val_mse: 0.0097 - val_mae: 0.0654\n",
      "Epoch 104/180\n",
      "65/65 [==============================] - 37s 576ms/step - loss: 0.0044 - mse: 0.0044 - mae: 0.0511 - val_loss: 0.0103 - val_mse: 0.0103 - val_mae: 0.0697\n",
      "Epoch 105/180\n",
      "65/65 [==============================] - 37s 564ms/step - loss: 0.0044 - mse: 0.0044 - mae: 0.0512 - val_loss: 0.0093 - val_mse: 0.0093 - val_mae: 0.0625\n",
      "Epoch 106/180\n",
      "65/65 [==============================] - 39s 600ms/step - loss: 0.0044 - mse: 0.0044 - mae: 0.0508 - val_loss: 0.0098 - val_mse: 0.0098 - val_mae: 0.0661\n",
      "Epoch 107/180\n",
      "65/65 [==============================] - 40s 623ms/step - loss: 0.0043 - mse: 0.0043 - mae: 0.0509 - val_loss: 0.0098 - val_mse: 0.0098 - val_mae: 0.0666\n",
      "Epoch 108/180\n",
      "65/65 [==============================] - 36s 557ms/step - loss: 0.0044 - mse: 0.0044 - mae: 0.0508 - val_loss: 0.0100 - val_mse: 0.0100 - val_mae: 0.0679\n",
      "Epoch 109/180\n",
      "65/65 [==============================] - 35s 536ms/step - loss: 0.0043 - mse: 0.0043 - mae: 0.0507 - val_loss: 0.0098 - val_mse: 0.0098 - val_mae: 0.0660\n",
      "Epoch 110/180\n",
      "65/65 [==============================] - 35s 536ms/step - loss: 0.0043 - mse: 0.0043 - mae: 0.0508 - val_loss: 0.0101 - val_mse: 0.0101 - val_mae: 0.0686\n",
      "Epoch 111/180\n",
      "65/65 [==============================] - 35s 542ms/step - loss: 0.0043 - mse: 0.0043 - mae: 0.0511 - val_loss: 0.0100 - val_mse: 0.0100 - val_mae: 0.0678\n",
      "Epoch 112/180\n",
      "65/65 [==============================] - 34s 525ms/step - loss: 0.0043 - mse: 0.0043 - mae: 0.0508 - val_loss: 0.0094 - val_mse: 0.0094 - val_mae: 0.0626\n",
      "Epoch 113/180\n",
      "65/65 [==============================] - 37s 573ms/step - loss: 0.0042 - mse: 0.0042 - mae: 0.0501 - val_loss: 0.0099 - val_mse: 0.0099 - val_mae: 0.0673\n",
      "Epoch 114/180\n",
      "65/65 [==============================] - 31s 481ms/step - loss: 0.0043 - mse: 0.0043 - mae: 0.0508 - val_loss: 0.0101 - val_mse: 0.0101 - val_mae: 0.0683\n",
      "Epoch 115/180\n",
      "65/65 [==============================] - 34s 526ms/step - loss: 0.0043 - mse: 0.0043 - mae: 0.0509 - val_loss: 0.0093 - val_mse: 0.0093 - val_mae: 0.0626\n",
      "Epoch 116/180\n",
      "65/65 [==============================] - 34s 528ms/step - loss: 0.0042 - mse: 0.0042 - mae: 0.0503 - val_loss: 0.0101 - val_mse: 0.0101 - val_mae: 0.0684\n",
      "Epoch 117/180\n",
      "65/65 [==============================] - 32s 499ms/step - loss: 0.0043 - mse: 0.0043 - mae: 0.0504 - val_loss: 0.0103 - val_mse: 0.0103 - val_mae: 0.0698\n",
      "Epoch 118/180\n",
      "65/65 [==============================] - 30s 454ms/step - loss: 0.0042 - mse: 0.0042 - mae: 0.0505 - val_loss: 0.0101 - val_mse: 0.0101 - val_mae: 0.0684\n",
      "Epoch 119/180\n",
      "65/65 [==============================] - 31s 472ms/step - loss: 0.0041 - mse: 0.0041 - mae: 0.0498 - val_loss: 0.0101 - val_mse: 0.0101 - val_mae: 0.0684\n",
      "Epoch 120/180\n",
      "65/65 [==============================] - 35s 533ms/step - loss: 0.0043 - mse: 0.0043 - mae: 0.0507 - val_loss: 0.0099 - val_mse: 0.0099 - val_mae: 0.0663\n",
      "Epoch 121/180\n",
      "65/65 [==============================] - 32s 497ms/step - loss: 0.0043 - mse: 0.0043 - mae: 0.0504 - val_loss: 0.0100 - val_mse: 0.0100 - val_mae: 0.0678\n",
      "Epoch 122/180\n",
      "65/65 [==============================] - 32s 496ms/step - loss: 0.0042 - mse: 0.0042 - mae: 0.0502 - val_loss: 0.0100 - val_mse: 0.0100 - val_mae: 0.0677\n",
      "Epoch 123/180\n",
      "65/65 [==============================] - 32s 498ms/step - loss: 0.0043 - mse: 0.0043 - mae: 0.0507 - val_loss: 0.0102 - val_mse: 0.0102 - val_mae: 0.0689\n",
      "Epoch 124/180\n",
      "65/65 [==============================] - 35s 544ms/step - loss: 0.0042 - mse: 0.0042 - mae: 0.0502 - val_loss: 0.0100 - val_mse: 0.0100 - val_mae: 0.0676\n",
      "Epoch 125/180\n",
      "65/65 [==============================] - 34s 523ms/step - loss: 0.0042 - mse: 0.0042 - mae: 0.0503 - val_loss: 0.0101 - val_mse: 0.0101 - val_mae: 0.0681\n",
      "Epoch 126/180\n",
      "65/65 [==============================] - 32s 489ms/step - loss: 0.0043 - mse: 0.0043 - mae: 0.0507 - val_loss: 0.0097 - val_mse: 0.0097 - val_mae: 0.0650\n",
      "Epoch 127/180\n",
      "65/65 [==============================] - 36s 556ms/step - loss: 0.0042 - mse: 0.0042 - mae: 0.0504 - val_loss: 0.0103 - val_mse: 0.0103 - val_mae: 0.0696\n",
      "Epoch 128/180\n",
      "65/65 [==============================] - 37s 569ms/step - loss: 0.0044 - mse: 0.0044 - mae: 0.0512 - val_loss: 0.0095 - val_mse: 0.0095 - val_mae: 0.0635\n",
      "Epoch 129/180\n",
      "65/65 [==============================] - 37s 566ms/step - loss: 0.0042 - mse: 0.0042 - mae: 0.0502 - val_loss: 0.0099 - val_mse: 0.0099 - val_mae: 0.0667\n",
      "Epoch 130/180\n",
      "65/65 [==============================] - 31s 483ms/step - loss: 0.0042 - mse: 0.0042 - mae: 0.0503 - val_loss: 0.0103 - val_mse: 0.0103 - val_mae: 0.0696\n",
      "Epoch 131/180\n",
      "65/65 [==============================] - 28s 425ms/step - loss: 0.0042 - mse: 0.0042 - mae: 0.0504 - val_loss: 0.0100 - val_mse: 0.0100 - val_mae: 0.0679\n",
      "Epoch 132/180\n",
      "65/65 [==============================] - 27s 422ms/step - loss: 0.0043 - mse: 0.0043 - mae: 0.0506 - val_loss: 0.0099 - val_mse: 0.0099 - val_mae: 0.0671\n",
      "Epoch 133/180\n",
      "65/65 [==============================] - 27s 415ms/step - loss: 0.0042 - mse: 0.0042 - mae: 0.0500 - val_loss: 0.0102 - val_mse: 0.0102 - val_mae: 0.0691\n",
      "Epoch 134/180\n",
      "65/65 [==============================] - 30s 468ms/step - loss: 0.0043 - mse: 0.0043 - mae: 0.0506 - val_loss: 0.0094 - val_mse: 0.0094 - val_mae: 0.0630\n",
      "Epoch 135/180\n",
      "65/65 [==============================] - 28s 433ms/step - loss: 0.0042 - mse: 0.0042 - mae: 0.0498 - val_loss: 0.0101 - val_mse: 0.0101 - val_mae: 0.0683\n",
      "Epoch 136/180\n",
      "65/65 [==============================] - 29s 448ms/step - loss: 0.0043 - mse: 0.0043 - mae: 0.0506 - val_loss: 0.0099 - val_mse: 0.0099 - val_mae: 0.0669\n",
      "Epoch 137/180\n",
      "65/65 [==============================] - 29s 439ms/step - loss: 0.0041 - mse: 0.0041 - mae: 0.0499 - val_loss: 0.0100 - val_mse: 0.0100 - val_mae: 0.0680\n",
      "Epoch 138/180\n",
      "65/65 [==============================] - 30s 461ms/step - loss: 0.0042 - mse: 0.0042 - mae: 0.0500 - val_loss: 0.0100 - val_mse: 0.0100 - val_mae: 0.0677\n",
      "Epoch 139/180\n",
      "65/65 [==============================] - 29s 441ms/step - loss: 0.0042 - mse: 0.0042 - mae: 0.0504 - val_loss: 0.0096 - val_mse: 0.0096 - val_mae: 0.0645\n",
      "Epoch 140/180\n",
      "65/65 [==============================] - 31s 474ms/step - loss: 0.0042 - mse: 0.0042 - mae: 0.0499 - val_loss: 0.0098 - val_mse: 0.0098 - val_mae: 0.0657\n",
      "Epoch 141/180\n",
      "65/65 [==============================] - 31s 471ms/step - loss: 0.0042 - mse: 0.0042 - mae: 0.0502 - val_loss: 0.0102 - val_mse: 0.0102 - val_mae: 0.0689\n",
      "Epoch 142/180\n",
      "65/65 [==============================] - 30s 468ms/step - loss: 0.0042 - mse: 0.0042 - mae: 0.0502 - val_loss: 0.0104 - val_mse: 0.0104 - val_mae: 0.0701\n",
      "Epoch 143/180\n",
      "65/65 [==============================] - 30s 468ms/step - loss: 0.0042 - mse: 0.0042 - mae: 0.0501 - val_loss: 0.0097 - val_mse: 0.0097 - val_mae: 0.0655\n",
      "Epoch 144/180\n",
      "65/65 [==============================] - 29s 439ms/step - loss: 0.0042 - mse: 0.0042 - mae: 0.0499 - val_loss: 0.0098 - val_mse: 0.0098 - val_mae: 0.0656\n",
      "Epoch 145/180\n",
      "52/65 [=======================>......] - ETA: 4s - loss: 0.0040 - mse: 0.0040 - mae: 0.0494"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-5080a788c788>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     69\u001b[0m                         \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m                         \u001b[0mshuffle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m                         validation_data = (X_val, y_val))\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 848\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    849\u001b[0m               \u001b[1;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m               \u001b[1;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    609\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 611\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    612\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2418\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2420\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2422\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1665\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1666\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1667\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1746\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    599\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# features list; order made according to Linear Regression FS\n",
    "features_list = ['APXP', \n",
    "                 'LOLP',  \n",
    "                 'In_gen',\n",
    "                 'Ren_R',\n",
    "                 'DA_imb_France', \n",
    "                 'Rene',\n",
    "                 'ratio_offers_vol',\n",
    "                 'DA_price_france',\n",
    "                 'TSDF',\n",
    "                 'dino_bin',\n",
    "                 'DA_margin',\n",
    "                 'Im_Pr']\n",
    "\n",
    "best_score = rmse_error\n",
    "\n",
    "# LOOP STARTS\n",
    "for i in features_list:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # data recovery for in case there is no improvement\n",
    "    data_recovery = data\n",
    "    \n",
    "    # update feature number\n",
    "    features_num = features_num + 1\n",
    "    \n",
    "    # add new feature\n",
    "    data = pd.concat([data, data_full.loc[:,i]], axis = 1)    \n",
    "\n",
    "    # divide data into train and test \n",
    "    data_train, data_test = train_test_split(\n",
    "             data, test_size = 0.15, shuffle=False)    \n",
    "    \n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "    # data scaling  (including offer (y))\n",
    "    sc_X = MinMaxScaler()\n",
    "    data_train = sc_X.fit_transform(data_train)\n",
    "    data_test = sc_X.transform(data_test)\n",
    "    \n",
    "    # divide features and labels\n",
    "    X_train = data_train[:, 0: features_num] \n",
    "    y_train = data_train[:, -1]\n",
    "    X_test = data_test[:, 0:features_num] \n",
    "    y_test = data_test[:, -1] \n",
    "\n",
    "    # divide data into train and test \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "             X_train, y_train, test_size = 0.15, shuffle=False)\n",
    "\n",
    "    # put data into correct shape\n",
    "    X_train, y_train = split_data(X_train, y_train, steps)\n",
    "    X_test, y_test = split_data(X_test, y_test, steps)\n",
    "    X_val, y_val = split_data(X_val, y_val, steps)\n",
    "\n",
    "    X_train = cut_data(X_train, batch_size)\n",
    "    y_train = cut_data(y_train, batch_size)\n",
    "    X_test = cut_data(X_test, batch_size)\n",
    "    y_test = cut_data(y_test, batch_size)\n",
    "    X_val = cut_data(X_val, batch_size)\n",
    "    y_val = cut_data(y_val, batch_size)\n",
    "\n",
    "    model = regressor_tunning(features_num = features_num)\n",
    "    \n",
    "    # fitting the LSTM to the training set\n",
    "    history = model.fit(X_train,\n",
    "                        y_train, \n",
    "                        batch_size = batch_size, \n",
    "                        epochs = epochs,\n",
    "                        shuffle = False, \n",
    "                        validation_data = (X_val, y_val))\n",
    "    \n",
    "    model.reset_states()\n",
    "    \n",
    "    # make new predicitons with test set\n",
    "    y_pred = model.predict(X_test, batch_size = batch_size)\n",
    "    \n",
    "    # prices col = 15 (inverso should not be used as scalling was made with the whole data set)\n",
    "    y_pred = (y_pred * sc_X.data_range_[-1]) + (sc_X.data_min_[-1])\n",
    "    y_test = (y_test * sc_X.data_range_[-1]) + (sc_X.data_min_[-1])\n",
    "\n",
    "    # smal adjustment\n",
    "    y_test = pd.Series(y_test)\n",
    "    y_test.replace(0, 0.0001,inplace = True)\n",
    "    \n",
    "    from sklearn.metrics import mean_squared_error as mse\n",
    "    from sklearn.metrics import mean_absolute_error as mae\n",
    "\n",
    "    rmse_error = mse(y_test, y_pred, squared = False)\n",
    "    mae_error = mae(y_test, y_pred)\n",
    "    \n",
    "    rmse_gen.append(rmse_error)\n",
    "    mae_gen.append(mae_error)\n",
    "    \n",
    "    # =============================================================================\n",
    "    # Metrics evaluation on spike regions\n",
    "    # =============================================================================\n",
    "    \n",
    "    y_spike_occ = pd.read_csv('Spike_binary_1std.csv', usecols = [6])\n",
    "    \n",
    "    # create array same size as y_test\n",
    "    y_spike_occ = y_spike_occ.iloc[- len(y_test):]\n",
    "    y_spike_occ = pd.Series(y_spike_occ.iloc[:,0]).values\n",
    "\n",
    "    # select y_pred and y_test only for regions with spikes\n",
    "    y_test_spike = (y_test.T * y_spike_occ).T\n",
    "    y_pred_spike = (y_pred.T * y_spike_occ).T\n",
    "    y_test_spike = y_test_spike[y_test_spike != 0]\n",
    "    y_pred_spike = y_pred_spike[y_pred_spike != 0]\n",
    "    \n",
    "    # calculate metric\n",
    "    rmse_spike = mse(y_test_spike, y_pred_spike, squared = False)\n",
    "    mae_spike = mae(y_test_spike, y_pred_spike)\n",
    "    \n",
    "    rmse_spi.append(rmse_spike)\n",
    "    mae_spi.append(mae_spike)\n",
    "    \n",
    "    # =============================================================================\n",
    "    # Metric evaluation on normal regions\n",
    "    # =============================================================================\n",
    "    \n",
    "    # inverse y_spike_occ so the only normal occurences are chosen\n",
    "    y_normal_occ = (y_spike_occ - 1) * (-1)\n",
    "    \n",
    "    # sanity check\n",
    "    y_normal_occ.sum() + y_spike_occ.sum() # gives the correct total \n",
    "    \n",
    "    # select y_pred and y_test only for normal regions\n",
    "    y_test_normal = (y_test.T * y_normal_occ).T\n",
    "    y_pred_normal = (y_pred.T * y_normal_occ).T\n",
    "    y_test_normal = y_test_normal[y_test_normal != 0.00]\n",
    "    y_pred_normal = y_pred_normal[y_pred_normal != 0.00]\n",
    "    \n",
    "    # calculate metric\n",
    "    rmse_normal = mse(y_test_normal, y_pred_normal, squared = False)\n",
    "    mae_normal = mae(y_test_normal, y_pred_normal)\n",
    "    \n",
    "    rmse_nor.append(rmse_normal)\n",
    "    mae_nor.append(mae_normal)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    time_count.append(elapsed_time)\n",
    "    \n",
    "    # condition of improvement for FS\n",
    "    if best_score < rmse_gen[-1]:\n",
    "        data = data_recovery\n",
    "        features_num = features_num - 1\n",
    "    else:\n",
    "        data = data\n",
    "        best_score = rmse_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33.54381425533274"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape\n",
    "rmse_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({                        \n",
    "                        'rmse_general': rmse_gen, \n",
    "                 \n",
    "                        'mae_general': mae_gen,\n",
    "                        \n",
    "                        'rmse_spike': rmse_spi,\n",
    "                 \n",
    "                        'mae_spike': mae_spi,\n",
    "                        \n",
    "                        'rmse_normal': rmse_nor,\n",
    "                    \n",
    "                        'mae_normal': mae_nor,\n",
    "    \n",
    "                        'time': time_count})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_d3e14a22_d71c_11ea_9975_7cb27da2bf47row0_col0 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_d3e14a22_d71c_11ea_9975_7cb27da2bf47row0_col1 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_d3e14a22_d71c_11ea_9975_7cb27da2bf47row0_col2 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_d3e14a22_d71c_11ea_9975_7cb27da2bf47row0_col3 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_d3e14a22_d71c_11ea_9975_7cb27da2bf47row0_col4 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_d3e14a22_d71c_11ea_9975_7cb27da2bf47row0_col5 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_d3e14a22_d71c_11ea_9975_7cb27da2bf47row0_col6 {\n",
       "            background-color:  yellow;\n",
       "        }</style><table id=\"T_d3e14a22_d71c_11ea_9975_7cb27da2bf47\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >rmse_general</th>        <th class=\"col_heading level0 col1\" >mae_general</th>        <th class=\"col_heading level0 col2\" >rmse_spike</th>        <th class=\"col_heading level0 col3\" >mae_spike</th>        <th class=\"col_heading level0 col4\" >rmse_normal</th>        <th class=\"col_heading level0 col5\" >mae_normal</th>        <th class=\"col_heading level0 col6\" >time</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_d3e14a22_d71c_11ea_9975_7cb27da2bf47level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_d3e14a22_d71c_11ea_9975_7cb27da2bf47row0_col0\" class=\"data row0 col0\" >33.543814</td>\n",
       "                        <td id=\"T_d3e14a22_d71c_11ea_9975_7cb27da2bf47row0_col1\" class=\"data row0 col1\" >24.595927</td>\n",
       "                        <td id=\"T_d3e14a22_d71c_11ea_9975_7cb27da2bf47row0_col2\" class=\"data row0 col2\" >31.079131</td>\n",
       "                        <td id=\"T_d3e14a22_d71c_11ea_9975_7cb27da2bf47row0_col3\" class=\"data row0 col3\" >24.491377</td>\n",
       "                        <td id=\"T_d3e14a22_d71c_11ea_9975_7cb27da2bf47row0_col4\" class=\"data row0 col4\" >33.894667</td>\n",
       "                        <td id=\"T_d3e14a22_d71c_11ea_9975_7cb27da2bf47row0_col5\" class=\"data row0 col5\" >24.611458</td>\n",
       "                        <td id=\"T_d3e14a22_d71c_11ea_9975_7cb27da2bf47row0_col6\" class=\"data row0 col6\" >6030.956796</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1c36431ef08>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def highlight_min(s):\n",
    "    '''\n",
    "    highlight the maximum in a Series yellow.\n",
    "    '''\n",
    "    is_max = s == s.min()\n",
    "    return ['background-color: yellow' if v else '' for v in is_max]\n",
    "\n",
    "results.style.apply(highlight_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "dates_labels = ['12 ',\n",
    "                '10 ',\n",
    "                '8 ',\n",
    "                '6 ',\n",
    "                '4 ',\n",
    "                '2 ']\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.minorticks_on()\n",
    "plt.grid(which='major', linestyle='-', linewidth='0.5')\n",
    "plt.grid(which='minor', linestyle=':', linewidth='0.5')\n",
    "plt.title('LSTM: Averaged RMSE for different\\n predictive windows')\n",
    "plt.plot(rmse_gen, label = 'Overall error')\n",
    "plt.plot(rmse_spi, label = 'Spike regions')\n",
    "plt.plot(rmse_nor, label = 'Normal regions')\n",
    "plt.legend()\n",
    "plt.ylabel('RMSE (/MWh)')\n",
    "plt.xlabel('Predictive window (in months)')\n",
    "plt.xticks([0,1,2,3,4,5], dates_labels)\n",
    "plt.tight_layout()\n",
    "plt.savefig('RMSE_predictive_window.png')\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.minorticks_on()\n",
    "plt.grid(which='major', linestyle='-', linewidth='0.5')\n",
    "plt.grid(which='minor', linestyle=':', linewidth='0.5')\n",
    "plt.title('LSTM: Averaged MAE for different\\n predictive windows')\n",
    "plt.plot(mae_gen, label = 'Overall error')\n",
    "plt.plot(mae_spi, label = 'Spike regions')\n",
    "plt.plot(mae_nor, label = 'Normal regions')\n",
    "plt.legend()\n",
    "plt.ylabel('MAE (/MWh)')\n",
    "plt.xlabel('Predictive window (in months)')\n",
    "plt.xticks([0,1,2,3,4,5], dates_labels)\n",
    "plt.tight_layout()\n",
    "plt.savefig('MAE_predictive_window.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
