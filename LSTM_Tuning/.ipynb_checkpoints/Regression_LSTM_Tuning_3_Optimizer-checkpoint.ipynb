{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Parameter Tuning\n",
    "    \n",
    "    Look for better optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\maria\\anaconda3\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from pandas) (1.18.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from python-dateutil>=2.6.1->pandas) (1.14.0)\n",
      "Requirement already satisfied: sklearn in c:\\users\\maria\\anaconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\maria\\anaconda3\\lib\\site-packages (from sklearn) (0.22.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (0.14.1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.18.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\maria\\anaconda3\\lib\\site-packages (3.1.3)\n",
      "Requirement already satisfied: numpy>=1.11 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from matplotlib) (1.18.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from matplotlib) (2.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from python-dateutil>=2.1->matplotlib) (1.14.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\maria\\anaconda3\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib) (45.2.0.post20200210)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install sklearn\n",
    "!pip install matplotlib\n",
    "\n",
    "import pandas as pd;\n",
    "import numpy as np;\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# parameters\n",
    "steps = 48\n",
    "n_hidden = 2\n",
    "units = 100\n",
    "batch_size = 48\n",
    "\n",
    "# months to evaluate model on\n",
    "date = 2018110000\n",
    "\n",
    "# for later use\n",
    "features_num = 14\n",
    "\n",
    "# lists to append results\n",
    "mae_gen = []\n",
    "mae_nor = []\n",
    "mae_spi = []\n",
    "rmse_gen = []\n",
    "rmse_nor = []\n",
    "rmse_spi = []\n",
    "y_pred_list = []\n",
    "time_count = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data; set X and y; fill nan values and split in test and training  data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "data = pd.read_csv('Data_set_1_smaller_(1).csv', index_col = 0)\n",
    "\n",
    "# 2018 data\n",
    "data = data.loc[data.index > date, :]\n",
    "\n",
    "# reset index\n",
    "data.reset_index(inplace = True)\n",
    "data.drop('index', axis = 1, inplace = True)\n",
    "\n",
    "# fill nan values\n",
    "data.fillna(data.mean(), inplace = True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# divide data into train and test \n",
    "data_train, data_test = train_test_split(\n",
    "         data, test_size = 0.15, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply feature scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# data scaling  (including offer (y))\n",
    "sc_X = MinMaxScaler()\n",
    "data_train = sc_X.fit_transform(data_train)\n",
    "data_test = sc_X.transform(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Keras libraries and packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LeakyReLU\n",
    "from keras import initializers\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need to process data with spike occurences the same way as features and offers:\n",
    "(Required to evaluate predictions in both normal regions and spike regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data for shaded area\n",
    "data = pd.read_csv('Spike_binary_1std.csv', index_col = 0)\n",
    "\n",
    "# set predictive window according with tuning best results\n",
    "data = data.loc[data.index > date, :]\n",
    "\n",
    "# make sure shaded area will correspond to values outputed by LSTM\n",
    "data.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# fill_nan is already made - so lets split data into test and train\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# divide data into train and test \n",
    "shade_train, shade_test = train_test_split(\n",
    "         data, test_size = 0.15, shuffle = False)\n",
    "\n",
    "# reset index of testing data\n",
    "shade_test.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# function to split data into correct shape for RNN\n",
    "def split_data(shade_test, steps):\n",
    "    y_spike_occ = list()\n",
    "    upper_lim = list()\n",
    "    lower_lim = list()\n",
    "    for i in range(steps, len(shade_test.index)):\n",
    "        y_spike_occ.append(shade_test['spike_occurance'][i])\n",
    "        upper_lim.append(shade_test['spike_upperlim'][i])\n",
    "        lower_lim.append(shade_test['spike_lowerlim'][i])\n",
    "    return np.array(y_spike_occ), np.array(upper_lim), np.array(lower_lim)\n",
    "\n",
    "# function to cut data set so it can be divisible by the batch_size\n",
    "def cut_data(data, batch_size):\n",
    "     # see if it is divisivel\n",
    "    condition = data.shape[0] % batch_size\n",
    "    if condition == 0:\n",
    "        return data\n",
    "    else:\n",
    "        return data[: -condition]\n",
    "\n",
    "# shape y_spike_occ for the right size to compare results in normal and spike regions\n",
    "y_spike_occ, spike_upperlim, spike_lowerlim = split_data(shade_test, steps)\n",
    "y_spike_occ = cut_data(y_spike_occ, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare of data according to LSTM needs,  create regressor & tune:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "44/44 [==============================] - 9s 213ms/step - loss: 0.2358 - mse: 0.2358 - mae: 0.3599 - val_loss: 0.1951 - val_mse: 0.1951 - val_mae: 0.4405\n",
      "Epoch 2/100\n",
      "44/44 [==============================] - 7s 152ms/step - loss: 0.1135 - mse: 0.1135 - mae: 0.2685 - val_loss: 0.0032 - val_mse: 0.0032 - val_mae: 0.0549\n",
      "Epoch 3/100\n",
      "44/44 [==============================] - 7s 148ms/step - loss: 0.0552 - mse: 0.0552 - mae: 0.1870 - val_loss: 0.0198 - val_mse: 0.0198 - val_mae: 0.1394\n",
      "Epoch 4/100\n",
      "44/44 [==============================] - 6s 143ms/step - loss: 0.0286 - mse: 0.0286 - mae: 0.1341 - val_loss: 0.0311 - val_mse: 0.0311 - val_mae: 0.1754\n",
      "Epoch 5/100\n",
      "44/44 [==============================] - 6s 143ms/step - loss: 0.0150 - mse: 0.0150 - mae: 0.0960 - val_loss: 0.0037 - val_mse: 0.0037 - val_mae: 0.0579\n",
      "Epoch 6/100\n",
      "44/44 [==============================] - 6s 145ms/step - loss: 0.0078 - mse: 0.0078 - mae: 0.0685 - val_loss: 0.0063 - val_mse: 0.0063 - val_mae: 0.0767\n",
      "Epoch 7/100\n",
      "44/44 [==============================] - 6s 147ms/step - loss: 0.0048 - mse: 0.0048 - mae: 0.0529 - val_loss: 5.9912e-04 - val_mse: 5.9912e-04 - val_mae: 0.0219\n",
      "Epoch 8/100\n",
      "44/44 [==============================] - 6s 143ms/step - loss: 0.0031 - mse: 0.0031 - mae: 0.0399 - val_loss: 0.0020 - val_mse: 0.0020 - val_mae: 0.0433\n",
      "Epoch 9/100\n",
      "44/44 [==============================] - 7s 151ms/step - loss: 0.0021 - mse: 0.0021 - mae: 0.0305 - val_loss: 0.0012 - val_mse: 0.0012 - val_mae: 0.0292\n",
      "Epoch 10/100\n",
      "44/44 [==============================] - 6s 147ms/step - loss: 0.0017 - mse: 0.0017 - mae: 0.0256 - val_loss: 6.6969e-04 - val_mse: 6.6969e-04 - val_mae: 0.0169\n",
      "Epoch 11/100\n",
      "44/44 [==============================] - 6s 137ms/step - loss: 0.0014 - mse: 0.0014 - mae: 0.0224 - val_loss: 5.3872e-04 - val_mse: 5.3872e-04 - val_mae: 0.0200\n",
      "Epoch 12/100\n",
      "44/44 [==============================] - 7s 149ms/step - loss: 0.0013 - mse: 0.0013 - mae: 0.0205 - val_loss: 9.8596e-04 - val_mse: 9.8596e-04 - val_mae: 0.0244\n",
      "Epoch 13/100\n",
      "44/44 [==============================] - 6s 147ms/step - loss: 0.0013 - mse: 0.0013 - mae: 0.0197 - val_loss: 5.9373e-04 - val_mse: 5.9373e-04 - val_mae: 0.0146\n",
      "Epoch 14/100\n",
      "44/44 [==============================] - 7s 153ms/step - loss: 0.0013 - mse: 0.0013 - mae: 0.0192 - val_loss: 4.3631e-04 - val_mse: 4.3631e-04 - val_mae: 0.0090\n",
      "Epoch 15/100\n",
      "44/44 [==============================] - 6s 145ms/step - loss: 0.0013 - mse: 0.0013 - mae: 0.0186 - val_loss: 6.3995e-04 - val_mse: 6.3995e-04 - val_mae: 0.0160\n",
      "Epoch 16/100\n",
      "44/44 [==============================] - 7s 153ms/step - loss: 0.0013 - mse: 0.0013 - mae: 0.0185 - val_loss: 3.8838e-04 - val_mse: 3.8838e-04 - val_mae: 0.0108\n",
      "Epoch 17/100\n",
      "44/44 [==============================] - 6s 144ms/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0181 - val_loss: 4.0915e-04 - val_mse: 4.0915e-04 - val_mae: 0.0086\n",
      "Epoch 18/100\n",
      "44/44 [==============================] - 5s 119ms/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0179 - val_loss: 5.0553e-04 - val_mse: 5.0553e-04 - val_mae: 0.0116\n",
      "Epoch 19/100\n",
      "44/44 [==============================] - 6s 128ms/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0178 - val_loss: 4.4386e-04 - val_mse: 4.4386e-04 - val_mae: 0.0094\n",
      "Epoch 20/100\n",
      "44/44 [==============================] - 6s 135ms/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0173 - val_loss: 3.9607e-04 - val_mse: 3.9607e-04 - val_mae: 0.0092\n",
      "Epoch 21/100\n",
      "44/44 [==============================] - 5s 125ms/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0175 - val_loss: 6.1361e-04 - val_mse: 6.1361e-04 - val_mae: 0.0152\n",
      "Epoch 22/100\n",
      "44/44 [==============================] - 6s 126ms/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0172 - val_loss: 3.8799e-04 - val_mse: 3.8799e-04 - val_mae: 0.0096\n",
      "Epoch 23/100\n",
      "44/44 [==============================] - 6s 130ms/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0170 - val_loss: 4.0357e-04 - val_mse: 4.0357e-04 - val_mae: 0.0090\n",
      "Epoch 24/100\n",
      "44/44 [==============================] - 6s 144ms/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0167 - val_loss: 3.8340e-04 - val_mse: 3.8340e-04 - val_mae: 0.0097\n",
      "Epoch 25/100\n",
      "44/44 [==============================] - 7s 149ms/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0166 - val_loss: 3.7374e-04 - val_mse: 3.7374e-04 - val_mae: 0.0110\n",
      "Epoch 26/100\n",
      "17/44 [==========>...................] - ETA: 3s - loss: 0.0015 - mse: 0.0015 - mae: 0.0155"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-a907b58dee62>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     76\u001b[0m                         \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m                         \u001b[0mshuffle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m                         validation_data = (X_val, y_val))\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 848\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    849\u001b[0m               \u001b[1;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m               \u001b[1;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    609\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 611\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    612\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2418\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2420\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2422\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1665\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1666\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1667\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1746\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    599\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "optimizers = [optimizers.RMSprop(), \n",
    "              optimizers.Adam(), \n",
    "              optimizers.Nadam(),\n",
    "              optimizers.Adamax()]\n",
    "\n",
    "\n",
    "# function to split data into correct shape for RNN\n",
    "def split_data(X, y, steps):\n",
    "    X_, y_ = list(), list()\n",
    "    for i in range(steps, len(y)):\n",
    "        X_.append(X[i - steps : i, :])\n",
    "        y_.append(y[i]) \n",
    "    return np.array(X_), np.array(y_)\n",
    "\n",
    "# function to cut data set so it can be divisible by the batch_size\n",
    "def cut_data(data, batch_size):\n",
    "     # see if it is divisivel\n",
    "    condition = data.shape[0] % batch_size\n",
    "    if condition == 0:\n",
    "        return data\n",
    "    else:\n",
    "        return data[: -condition]\n",
    "\n",
    "# divide features and labels\n",
    "X_train = data_train[:, 0:14] \n",
    "y_train = data_train[:, -1]\n",
    "X_test = data_test[:, 0:14] \n",
    "y_test = data_test[:, -1] \n",
    "\n",
    "# divide data into train and test \n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "         X_train, y_train, test_size = 0.15, shuffle=False)\n",
    "\n",
    "# put data into correct shape\n",
    "X_train, y_train = split_data(X_train, y_train, steps)\n",
    "X_test, y_test = split_data(X_test, y_test, steps)\n",
    "X_val, y_val = split_data(X_val, y_val, steps)\n",
    "\n",
    "X_train = cut_data(X_train, batch_size)\n",
    "y_train = cut_data(y_train, batch_size)\n",
    "X_test = cut_data(X_test, batch_size)\n",
    "y_test = cut_data(y_test, batch_size)\n",
    "X_val = cut_data(X_val, batch_size)\n",
    "y_val = cut_data(y_val, batch_size)\n",
    "\n",
    "# inverse of test set should not be inside the loop \n",
    "y_test = (y_test * sc_X.data_range_[14]) + (sc_X.data_min_[14])\n",
    "\n",
    "# smal adjustment\n",
    "y_test = pd.Series(y_test)\n",
    "y_test.replace(0, 0.0001,inplace = True)\n",
    "\n",
    "for i in optimizers:  \n",
    "    # design the LSTM\n",
    "    def regressor_tunning(kernel_initializer = 'he_uniform',\n",
    "                          bias_initializer = initializers.Ones()):\n",
    "        model = Sequential()\n",
    "        if n_hidden == 0:\n",
    "            model.add(LSTM(units = units,                    \n",
    "                           input_shape = (steps, features_num), \n",
    "                           kernel_initializer = kernel_initializer,\n",
    "                           bias_initializer = bias_initializer))\n",
    "            model.add(LeakyReLU(alpha = 0.2))\n",
    "            model.add(Dropout(0.2))\n",
    "        else:\n",
    "            model.add(LSTM(units = units,                    \n",
    "                           input_shape = (steps, features_num), \n",
    "                           return_sequences = True,\n",
    "                           kernel_initializer = kernel_initializer,\n",
    "                           bias_initializer = bias_initializer))\n",
    "            model.add(LeakyReLU(alpha = 0.2))\n",
    "            model.add(Dropout(0.2))\n",
    "            model.add(LSTM(units = units, \n",
    "                           input_shape = (steps, features_num), \n",
    "                           kernel_initializer = kernel_initializer,\n",
    "                           bias_initializer = bias_initializer))\n",
    "            model.add(LeakyReLU(alpha = 0.2))\n",
    "            model.add(Dropout(0.2))\n",
    "        model.add(Dense(1, activation='linear'))\n",
    "        optimizer = i\n",
    "        model.compile(loss = 'mse', metrics = ['mse', 'mae'], optimizer = optimizer)\n",
    "        return model\n",
    "\n",
    "    model = regressor_tunning()\n",
    "\n",
    "    # fitting the LSTM to the training set\n",
    "    history = model.fit(X_train,\n",
    "                        y_train, \n",
    "                        batch_size = batch_size, \n",
    "                        epochs = 100,\n",
    "                        shuffle = False, \n",
    "                        validation_data = (X_val, y_val))\n",
    "    \n",
    "    model.reset_states()\n",
    "    \n",
    "    # make new predicitons with test set\n",
    "    y_pred = model.predict(X_test, batch_size = batch_size)\n",
    "    \n",
    "    # prices col = 15 (inverso should not be used as scalling was made with the whole data set)\n",
    "    y_pred = (y_pred * sc_X.data_range_[14]) + (sc_X.data_min_[14])\n",
    "    \n",
    "    y_pred_list.append(y_pred)\n",
    "    \n",
    "    from sklearn.metrics import mean_squared_error as mse\n",
    "    from sklearn.metrics import mean_absolute_error as mae\n",
    "\n",
    "    rmse_error = mse(y_test, y_pred, squared = False)\n",
    "    mae_error = mae(y_test, y_pred)\n",
    "    \n",
    "    rmse_gen.append(rmse_error)\n",
    "    mae_gen.append(mae_error)\n",
    "    \n",
    "    # =============================================================================\n",
    "    # Metrics evaluation on spike regions\n",
    "    # =============================================================================\n",
    "    \n",
    "    # Need to process data with spike occurences the same way as features\n",
    "    data = pd.read_csv('Spike_binary_1std.csv', index_col = 0)\n",
    "\n",
    "    # set predictive window according with tuning best results\n",
    "    data = data.loc[data.index > date, :]\n",
    "\n",
    "    # make sure shaded area will correspond to values outputed by LSTM\n",
    "    data.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    # fill_nan is already made - so lets split data into test and train\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # divide data into train and test \n",
    "    shade_train, shade_test = train_test_split(\n",
    "             data, test_size = 0.15, shuffle = False)\n",
    "\n",
    "    # reset index of testing data\n",
    "    shade_test.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    # function to split data into correct shape for RNN\n",
    "    def split_data_shade(shade_test, steps):\n",
    "        y_spike_occ = list()\n",
    "        upper_lim = list()\n",
    "        lower_lim = list()\n",
    "        for i in range(steps, len(shade_test.index)):\n",
    "            y_spike_occ.append(shade_test['spike_occurance'][i])\n",
    "            upper_lim.append(shade_test['spike_upperlim'][i])\n",
    "            lower_lim.append(shade_test['spike_lowerlim'][i])\n",
    "        return np.array(y_spike_occ), np.array(upper_lim), np.array(lower_lim)\n",
    "    \n",
    "    # function to cut data set so it can be divisible by the batch_size\n",
    "    def cut_data_shade(data, batch_size):\n",
    "         # see if it is divisivel\n",
    "        condition = data.shape[0] % batch_size\n",
    "        if condition == 0:\n",
    "            return data\n",
    "        else:\n",
    "            return data[: -condition]\n",
    "    \n",
    "    # shape y_spike_occ for the right size to compare results in normal and spike regions\n",
    "    y_spike_occ, spike_upperlim, spike_lowerlim = split_data_shade(shade_test, steps)\n",
    "    y_spike_occ = cut_data_shade(y_spike_occ, batch_size)\n",
    "    \n",
    "    # continue\n",
    "    \n",
    "    # select y_pred and y_test only for regions with spikes\n",
    "    y_test_spike = (y_test.T * y_spike_occ).T\n",
    "    y_pred_spike = (y_pred.T * y_spike_occ).T\n",
    "    y_test_spike = y_test_spike[y_test_spike != 0]\n",
    "    y_pred_spike = y_pred_spike[y_pred_spike != 0]\n",
    "    \n",
    "    # calculate metric\n",
    "    rmse_spike = mse(y_test_spike, y_pred_spike, squared = False)\n",
    "    mae_spike = mae(y_test_spike, y_pred_spike)\n",
    "    \n",
    "    rmse_spi.append(rmse_spike)\n",
    "    mae_spi.append(mae_spike)\n",
    "    \n",
    "    # =============================================================================\n",
    "    # Metric evaluation on normal regions\n",
    "    # =============================================================================\n",
    "    \n",
    "    # inverse y_spike_occ so the only normal occurences are chosen\n",
    "    y_normal_occ = (y_spike_occ - 1) * (-1)\n",
    "    \n",
    "    # sanity check\n",
    "    y_normal_occ.sum() + y_spike_occ.sum() # gives the correct total \n",
    "    \n",
    "    # select y_pred and y_test only for normal regions\n",
    "    y_test_normal = (y_test.T * y_normal_occ).T\n",
    "    y_pred_normal = (y_pred.T * y_normal_occ).T\n",
    "    y_test_normal = y_test_normal[y_test_normal != 0.00]\n",
    "    y_pred_normal = y_pred_normal[y_pred_normal != 0.00]\n",
    "    \n",
    "    # calculate metric\n",
    "    rmse_normal = mse(y_test_normal, y_pred_normal, squared = False)\n",
    "    mae_normal = mae(y_test_normal, y_pred_normal)\n",
    "    \n",
    "    rmse_nor.append(rmse_normal)\n",
    "    mae_nor.append(mae_normal)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    time_count.append(elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({'optimizers':optimizers,\n",
    "                        \n",
    "                        'rmse_general': rmse_gen, \n",
    "                 \n",
    "                        'mae_general': mae_gen,\n",
    "                        \n",
    "                        'rmse_spike': rmse_spi,\n",
    "                 \n",
    "                        'mae_spike': mae_spi,\n",
    "                        \n",
    "                        'rmse_normal': rmse_nor,\n",
    "                    \n",
    "                        'mae_normal': mae_nor})\n",
    "\n",
    "results.to_csv('Results_LSTM_3_optimizer.csv')\n",
    "\n",
    "y_pred = pd.DataFrame({'optimizers':optimizers,\n",
    "                       'Predicitons': y_pred_list})\n",
    "\n",
    "y_pred.to_csv('Pedictions_LSTM_3_optimizers.csv')\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({'rmse_general': rmse_gen, \n",
    "                 \n",
    "                        'mae_general': mae_gen,\n",
    "                        \n",
    "                        'rmse_spike': rmse_spi,\n",
    "                 \n",
    "                        'mae_spike': mae_spi,\n",
    "                        \n",
    "                        'rmse_normal': rmse_nor,\n",
    "                    \n",
    "                        'mae_normal': mae_nor})\n",
    "\n",
    "def highlight_min(s):\n",
    "    '''\n",
    "    highlight the maximum in a Series yellow.\n",
    "    '''\n",
    "    is_max = s == s.min()\n",
    "    return ['background-color: yellow' if v else '' for v in is_max]\n",
    "\n",
    "results.style.apply(highlight_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "w_plot = 200 \n",
    "fontsize = 13\n",
    "\n",
    "y_pred = y_pred_list[3]\n",
    "\n",
    "y_pred = y_pred.reshape(len(y_pred))\n",
    "\n",
    "Residual = list(y_test) - y_pred\n",
    "\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.plot(np.arange(0, (w_plot)), y_test[-w_plot:], label = 'Real values', linewidth = 1.5, color = 'steelblue')\n",
    "plt.plot(np.arange(0, (w_plot)), y_pred[-w_plot:], label = 'Predicted values', linewidth = 1.2, color= 'deepskyblue')\n",
    "plt.plot(np.arange(0, (w_plot)), Residual[-w_plot:], label = 'Residual error', linewidth = 0.8, color = 'slategrey')\n",
    "plt.fill_between(np.arange(0, (w_plot)),  data['spike_lowerlim'][-w_plot:],data['spike_upperlim'][-w_plot:], facecolor='skyblue', alpha=0.5, label = 'Not spike regions')\n",
    "plt.xlim(0, w_plot - 1)\n",
    "plt.ylim(-100, 260)\n",
    "plt.minorticks_on()\n",
    "plt.grid(which='major', linestyle='-', linewidth='0.5')\n",
    "plt.grid(which='minor', linestyle=':', linewidth='0.5')\n",
    "plt.xlabel('Accumulated SP', fontsize = fontsize)\n",
    "plt.ylabel('RMSE (Â£/MWh)', fontsize = fontsize)\n",
    "plt.xticks(fontsize = fontsize)\n",
    "plt.yticks([-100, -50, 0, 50,100, 150, 200, 250],[-100, -50, 0, 50, 100, 150, 200, 250],  fontsize = fontsize)\n",
    "plt.title('LSTM prediction with the last optimizer: checking', fontsize = fontsize + 2)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
